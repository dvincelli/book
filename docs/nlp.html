<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Natural Language Processing - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="natural-language-processing">
<h1>Natural Language Processing</h1>
<p>Text is a dominant media type on the Internet along with images, videos, and audios. Many of our day-to-day tasks involve text analysis. Natural language processing (NLP) is a powerful tool to extract insights from text corpora.</p>
<p>NLP is a large topic that covers many different advanced problems, such as speech tagging, named entity recognition, machine translation, speech recognition, etc. We surely cannot cover all of them in this one single chapter, perhaps not even a whole book. To this end, in this chapter we mainly focus on the vector space models and topic modelling.</p>
<p>TODO: Explain Topic modelling briefly</p>
<p>TODO: this chapter now mainly lacks general text introduction of NLP.</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Survey the literature, give a high-level picture of NLP. Talk about classic NLP … structured and unstructured text …</p>
<p>In this chapter, we will use a <a href="https://github.com/ryanrhymes/owl_dataset/raw/master/news.txt.gz">news dataset</a> crawled from the Internet. It contains 130000 pieces of news from various sources, each line in the file represents one entry. For example we the first line/document is:</p>
<div class="highlight">
<pre><code class="language-text">a brazilian man who earns a living by selling space for tattoo adverts on his body is now looking for a customer for his forehead , it appears . edson aparecido borim already has 49 adverts tattooed on his chest , back and arms , the g1 news portal reports. he says it all started eight years ago with " a dare in a bar " , but now the ads are his main source of income. " my goal now is to get a big company to tattoo my forehead , but it would have to be a good contract , " he says . he walks around bare-chested in the small town of tabani in the state of sao paulo , but says he 's not obliged to do so all the time. the brazilian charges between 50 and 400 reals ( $ 14- $ 110 ) a month for a tattoo , depending on its size and location on his body , and on the client. borim says when clients do n't pay or cancel an ad , he crosses them out . " skinvertising " caused a stir in the mid-2000s , when many dot.com companies experimented with it. the practice left behind a trail of ads for companies that do n't exist any more , buzzfeed reports . use # newsfromelsewhere to stay up-to-date with our reports via twitter .</code></pre>
</div>
</section>
<section class="level2" id="text-corpus">
<h2>Text Corpus</h2>
<p>Normally we call a collection of documents a <em>text corpus</em>, which contains a large and structured set of texts. For example, for the English language there are the <a href="https://www.english-corpora.org/coca/">Corpus of Contemporary American English</a>, <a href="https://corpling.uis.georgetown.edu/gum">Georgetown University Multilayer Corpus</a>, etc. Our news collection is also one such example. To perform NLP tasks such as topic modelling, the first and perhaps the most important thing is to represent a text corpus as format that the models can process, instead of directly using natural language.</p>
<p>TODO: A survey of annotation methods.</p>
<p>For the task of topic modelling, we perform the tokenisation on the input English text. The target is to represent each word as an integer index so that we can further process the numbers instead of words. This is called the <em>tokenisation</em> of the text. Of course we also need to have a mapping from index to word.</p>
<section class="level3" id="step-by-step-operation">
<h3>Step-by-step Operation</h3>
<p>The NLP module in Owl supports building a proper text corpus from given text dataset. In this section we will show how we can build a corpus from a collection of documents, in a step step way.</p>
<p>In the first step, remove the special characters. We define a regular expression <code>regexp_split</code> for special characters such as <code>,</code>, <code>?</code>, <code>\t</code> etc. First remove them, and then convert all the text into lower-case. The code below define such a process function, and the <code>Nlp.Corpus.preprocess</code> apply it to all the text. Note this function will not change the number of lines in a corpus.</p>
<div class="highlight">
<pre><code class="language-ocaml">let simple_process s =
  Str.split Owl_nlp_utils.regexp_split s
  |&gt; List.filter (fun x -&gt; String.length x &gt; 1)
  |&gt; String.concat " "
  |&gt; String.lowercase_ascii
  |&gt; Bytes.of_string

let preprocess input_file =
  let output_file = input_file ^ ".output" in
  Nlp.Corpus.preprocess simple_process input_file output_file</code></pre>
</div>
<p>Based on the processed text corpus, we can build the <em>vocabulary</em>. Each word is assigned a number id, or index, and we have the dictionary to map word to index, and index to word. This is achieved by using the <code>Nlp.Vocabulary.build</code> function.</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_vocabulary input_file =
  let vocab = Nlp.Vocabulary.build input_file in
  let output_file = input_file ^ ".vocab" in
  Nlp.Vocabulary.save vocab output_file</code></pre>
</div>
<p>The <code>build</code> function returns a vocabulary. It contains three <code>Hasthtbl</code>s. The first maps a word to an index, and the second index to word. The last hash table is a map between index and its frequency, i.e.&nbsp;number of occurrence in the whole text body. We can check out the words of highest frequency with:</p>
<div class="highlight">
<pre><code class="language-ocaml">let print_freq vocab =
  Nlp.Vocabulary.top vocab 10 |&gt;
  Owl.Utils.Array.to_string ~sep:", " fst</code></pre>
</div>
<p>Unsurprisingly, the “the”’s and “a”’s are most frequently used:</p>
<div class="highlight">
<pre><code class="language-text">- : string =
"the, to, of, a, and, in, \", s, that, on"</code></pre>
</div>
<p>Change <code>Nlp.Vocabulary.top</code> to <code>Nlp.Vocabulary.bottom</code> can shows the words of lowest frequency:</p>
<div class="highlight">
<pre><code class="language-text">"eichorst, gcs, freeross, depoliticisation, humping, shopable, appurify, intersperse, vyaecheslav, raphaelle"</code></pre>
</div>
<p>However, in a topic modelling task, we don’t want these too frequent but meaningless words and perhaps also the least frequent words that are not about the topic of this document. Now let’s trim off some most and least frequency words. You can trim either by absolute number or by percent. We use percent here, namely trimming off top and bottom 1% of the words.</p>
<div class="highlight">
<pre><code class="language-ocaml">let trim_vocabulary vocab =
  Nlp.Vocabulary.trim_percent ~lo:0.01 ~hi:0.01 vocab</code></pre>
</div>
<p>With a proper vocabulary at hands, now we are ready to tokenise a piece of text.</p>
<div class="highlight">
<pre><code class="language-ocaml">let tokenise vocab text =
  String.split_on_char ' ' text |&gt;
  List.map (Nlp.Vocabulary.word2index vocab)</code></pre>
</div>
<p>For example, if we tokenise “this is owl book”, you will get the following output.</p>
<div class="highlight">
<pre><code class="language-text">tokenise vocab "this is an owl book";;
- : int list = [55756; 18322; 109456; 90661; 22362]</code></pre>
</div>
<p>Furthermore, we can now tokenise the whole news collection.</p>
<div class="highlight">
<pre><code class="language-ocaml">let tokenise_all vocab input_file =
  let doc_s = Owl_utils.Stack.make () in
  Owl_io.iteri_lines_of_file
    (fun i s -&gt;
      let t =
        Str.split Owl_nlp_utils.regexp_split s
        |&gt; List.filter (Owl_nlp_vocabulary.exits_w vocab)
        |&gt; List.map (Owl_nlp_vocabulary.word2index vocab)
        |&gt; Array.of_list
      in
      Owl_utils.Stack.push doc_s i)
    input_file;
  doc_s</code></pre>
</div>
<p>The process is simple: in the text corpus each line is a document and we iterate through the text line by line. For each line/document, we remove the special characters, filter out the words that exist in the vocabulary, and map each word to an integer index accordingly. Even though this is a simplified case, it well illustrates the typical starting point of text analysis before delving into any topic modelling.</p>
</section>
<section class="level3" id="use-the-corpus-module">
<h3>Use the Corpus Module</h3>
<p>But we don’t have to build a text corpus step by step. We provide the <code>NLP.Corpus</code> module for convenience. By using the <code>Nlp.Corpus.build</code> we perform both tasks we have introduced: building vocabulary, and tokenising the text corpus. With this function we can also specify how to trim off the high-frequency and low-frequency words. Here is an example:</p>
<div class="highlight">
<pre><code class="language-ocaml">let main () =
  let ids = Nlp.Corpus.unique "news.txt" "clean.txt" in
  Printf.printf "removed %i duplicates." (Array.length ids);
  let corpus = Nlp.Corpus.build ~lo:0.01 ~hi:0.01 "clean.txt" in
  Nlp.Corpus.print corpus</code></pre>
</div>
<p>The <code>Nlp.Corpus.unique</code> function is just one more layer of pre-processing. It removes the possible duplicated lines/documents. The output prints out the processing progress, and then a summary of the corpus is printed out.</p>
<div class="highlight">
<pre><code class="language-text">2020-01-28 19:07:05.461 INFO : build up vocabulary ...
2020-01-28 19:07:10.461 INFO : processed 13587, avg. 2717 docs/s
2020-01-28 19:07:15.463 INFO : processed 26447, avg. 2644 docs/s
...
2020-01-28 19:08:09.125 INFO : convert to binary and tokenise ...
2020-01-28 19:08:34.130 INFO : processed 52628, avg. 2104 docs/s
2020-01-28 19:08:39.132 INFO : processed 55727, avg. 1857 docs/s
...
corpus info
  file path  : news.txt
  # of docs  : 129968
  doc minlen : 10
- : unit = ()</code></pre>
</div>
<p>The corpus contains xxx parts: the vocabulary, token, and text string. By calling the <code>build</code> function, we also save them for later use. It creates several files in the current directory. First, there is the vocabulary file <code>news.txt.voc</code> and <code>news.txt.voc.txt</code>. They are the same; only that the latter is in a human-readable format that has each line a word and the corresponding index number. We can get the vocabulary with <code>Corpus.get_vocab</code>.</p>
<p>The tokenised text corpus is marshalled to the <code>news.txt.tok</code> file, and the string format content is saved as binary file to <code>news.txt.bin</code>. We choose to save the content as binary format to save file size. To get the i-th document, we can use <code>Corpus.get corpus i</code> to get the text string, or <code>Corpus.get_tok corpus i</code> to get an integer array that is tokenised version of this document.</p>
<p>To efficiently access different documents by the document index (line number), we keep track of the accumulated length of text corpus and token array after processing each document. These two type of indexes are saved in the <code>news.txt.mdl</code> file. This file also contains the document id. We have seen the <code>minlen</code> value in the output of corpus information. Each document with less than 10 words will not be included in the corpus. The document id is an int array that shows the index (line number) of each document in the original text corpus so that it can be traced back. The document id can be retrieved by <code>Corpus.get_docid corpus</code></p>
<p>In the <code>Corpus</code> module, we provide three mechanisms to iterate through the text corpus: <code>next</code>, <code>iteri</code>, <code>mapi</code>. The <code>next</code> function is a generator that yields the next line of text document string in the text corpus until it hits the end of file. The <code>iteri</code> and <code>mapi</code> functions work exactly like in the normal Array module. The first function iterates all the documents one by one in the corpus, and the second maps all the documents in a corpus into another array. The <code>iteri_tok</code> and <code>mapi_tok</code> work the same, except that the function should work on integer array instead of string. Their signatures is shown below:</p>
<div class="highlight">
<pre><code class="language-text">val iteri : (int -&gt; string -&gt; unit) -&gt; t -&gt; unit

val iteri_tok : (int -&gt; int array -&gt; unit) -&gt; t -&gt; unit

val mapi : (int -&gt; string -&gt; 'a) -&gt; t -&gt; 'a array

val mapi_tok : (int -&gt; 'a -&gt; 'b) -&gt; t -&gt; 'b array</code></pre>
</div>
<p>The <code>Corpus</code> module is designed to support a large number of text corpus. With this tool in hand, we can further proceed with the discussion of topic modelling.</p>
</section>
</section>
<section class="level2" id="vector-space-models">
<h2>Vector Space Models</h2>
<p>Based on the tokenised text corpus, the next thing we need is a mathematical model to express abstract ideas such as “this sentence makes sense and that one does not”, “these two documents are similar”, or “the key word in that paragraph is such and such”. To perform NLP tasks such as text retrieval and topic modelling, we use the <em>Vector Space Model</em> (VSM) to do that.</p>
<p>According to the wikipedia, a VSM is “an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers”. It may sounds tricky but the basic idea is actually very simple. For example, let’s assume we only care about three topics in any news: covid19, economics, and election. Then we can represent any news article with a three-element vector, each representing the weight of this topic in it. For the BBC news <a href="https://www.bbc.co.uk/news/uk-52462928">“Coronavirus: Millions more to be eligible for testing”</a>, we can represent it with vector <code>(100, 2.5, 0)</code>. The specific value does not actually matter here. The point is that now instead of a large chunks of text corpus, we only need to deal with this vector for further processing.</p>
<p>The vector space model proposes a framework that maps a document to a vector <span class="math inline">\(d = (x_1, x_1, \ldots, x_N)\)</span>. This N-dimensional vector space is defined by <span class="math inline">\(N\)</span> basic terms. Under this framework, we mainly have to decide on three factors. The first is to choose the meaning of each dimension, or the <span class="math inline">\(N\)</span> basic concepts in the vector space. The second is to specify the weight of each dimension for a document. In our simple example, why do we assign the first weight to <code>100</code> instead of <code>50</code>? There should be rules about it. Finally, after learning the vector representation, how should we measure their similarity? The similarity of document is a basic idea in text processing. For topic modelling, we can cluster the documents based on their similarity. (TODO: Extend this point)</p>
<p>In this chapter we focusing on mapping a document to a vector space. However, VSM is not limited to only documents. We can also map a word into a vector that represents a point in a certain vector space. This vector is also called <em>word embedding</em>. In a proper representation, the similar words should be cluster together, and can even be used for calculation such as:</p>
<p><span class="math display">\[V_\textrm{king} - V_\textrm{man} + V_\textrm{women} \approx V_\textrm{queen}.\]</span></p>
<p>One of the most widely used method for word embedding is the <code>word2vec</code> proposed in <span data-cites="mikolov2013exploiting" class="citation">(Mikolov, Le, and Sutskever 2013)</span>. It includes different algorithms such as the skip-gram for computing the vector representation of words. For general purpose use, Google has already published a <a href="https://code.google.com/archive/p/word2vec/">pre-trained</a> word2vec-based word embedding vector set based on part of the GoogleNews dataset. This vector set contains 300-dimensional vectors for 3 million words and phrases.</p>
<p>Back to the theme of mapping documents to vector space. In the next chapter, we will start with a simple method that instantiate the VSM: the Bag of Words.</p>
</section>
<section class="level2" id="bag-of-words-bow">
<h2>Bag of Words (BOW)</h2>
<p>The Bag of Words is a simple way to map docs into a vector space. This space uses all the vocabulary as the dimensions. Suppose there are totally <span class="math inline">\(N\)</span> different words in the vocabulary, then the vector space is of <span class="math inline">\(N\)</span> dimension. The mapping function is simply counting how many times each word in the vocabulary appears in a document.</p>
<p>For example, let’s use the five words “news”, “about”, “coronavirus”, “test”, “cases” as the five dimensions in the vector space. Then if a document is “…we heard news a new coronavirus vaccine is being developed which is expected to be tested about September…” will be represented as <code>[1, 1, 1, 1, 0]</code> and the document “…number of positive coronavirus cases is 100 and cumulative cases are 1000…” will be projected to vector <code>[0, 0, 1, 0, 2]</code>.</p>
<p>This Bag of Words method is easy to implement based on the text corpus. We first define a function that count the term occurrence in a document and return a hash table:</p>
<div class="highlight">
<pre><code class="language-ocaml">let term_count htbl doc =
  Array.iter
    (fun w -&gt;
      match Hashtbl.mem htbl w with
      | true  -&gt;
        let a = Hashtbl.find htbl w in
        Hashtbl.replace htbl w (a +. 1.)
      | false -&gt; Hashtbl.add htbl w 1.)
    doc</code></pre>
</div>
<p>The hash table contains all the counts of words in this document. Of course, we can also represent the returned results as an array of integers, though the array would likely be sparse. Then we can apply this function to all the documents in the corpus using the map function:</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_bow corpus =
  Nlp.Corpus.mapi_tok
    (fun i doc -&gt;
      let htbl = Hashtbl.create 128 in
      term_count htbl doc;
      htbl)
    corpus</code></pre>
</div>
<p>Based on this bag of words, the similarity between two vectors can be measured using different methods, e.g.&nbsp;with a simple dot product.</p>
<p>This method is easy to implement and the computation is inexpensive. It maybe simple, but for some tasks, especially those that has no strict requirement for context or position of words, this method proves to work well. For example, to cluster spam email, we only need to specify proper keywords as dimensions, such as “income”, “bonus”, “extra”, “cash”, “free”, “refund”, “promise” etc. We can expect that the spam email texts will be clustered closely and easy to recognise in this vector space using the bag of words.</p>
<p>On the other hand, this simple approach does have its own problems. Back to the previous example, if we want to get how close the a document is to “news about coronavirus test cases”, then the doc “…number of positive coronavirus cases is 100 and cumulative cases are 1000…” is scored the same as “hey, I got some good news about your math test result…”. This is not what we expected. Intuitively, words like “coronavirus” should matter more than the more normal words like “test” and “about”. That’s why we are going to introduce an improved method in the next section.</p>
</section>
<section class="level2" id="term-frequencyinverse-document-frequency-tfidf">
<h2>Term Frequency–Inverse Document Frequency (TFIDF)</h2>
<p>Explain what is TFIDF, mention Cambridge Wolfson fellow. The corpus we have built in the previous section is used as input to the following function.</p>
<p>Explain why TFIDF is better than BOW, what is the motivation. Give an example to illustrate.</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_tfidf corpus =
  (* configure and build the model *)
  let tf = Nlp.Tfidf.Count in
  let df = Nlp.Tfidf.Idf in
  let model = Nlp.Tfidf.build ~tf ~df corpus in

  (* print and save model *)
  Nlp.Tfidf.print model;
  Nlp.Tfidf.save model "news.tfidf";

  model</code></pre>
</div>
<p>After the model is build, illustrate how to find k similar documents. The following exmaple uses consine similarity, then convert a document into vector using previously trained TFIDF model. Note do NOT teach how to index and how the similarity is calculted here, teach in Indexing and Searching section.</p>
<div class="highlight">
<pre><code class="language-ocaml">let query model doc k =
  (* TODO: change api *)
  let typ = Owl_nlp_similarity.Cosine in
  let vec = Nlp.Tfidf.apply model doc in
  let knn = Nlp.Tfidf.nearest ~typ model vec k in
  knn</code></pre>
</div>
</section>
<section class="level2" id="latent-dirichlet-allocation-lda">
<h2>Latent Dirichlet Allocation (LDA)</h2>
<p>Explain what is LDA. <code>topics</code> is the number of topics. Owl supports the following types of LDA algorithms.</p>
<div class="highlight">
<pre><code class="language-ocaml">type lda_typ =
  | SimpleLDA
  | FTreeLDA
  | LightLDA
  | SparseLDA</code></pre>
</div>
<p>How to train an LDA model.</p>
<div class="highlight">
<pre><code class="language-text">(* change to ocaml when image progation finished *)
let build_lda corpus topics =
  let model = Nlp.Lda.init ~iter:1000 topics in
  let lda_typ = Nlp.Lda.SparseLDA in
  Nlp.Lda.train lda_typ model;
  Owl.Log.info "LDA training finished."</code></pre>
</div>
</section>
<section class="level2" id="latent-semantic-analysis-lsa">
<h2>Latent Semantic Analysis (LSA)</h2>
<p>Explain what is LSA, and how it differs from LDA w.r.t. derived topics.</p>
<p>Read * https://en.wikipedia.org/wiki/Latent_semantic_analysis * https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/</p>
</section>
<section class="level2" id="indexing-and-searching">
<h2>Indexing and Searching</h2>
<p>Topic models are effective tools for clustering documents based on their similarity or relevance. We can further use this tool to query relevant document given an input one. In this section, we will go through some techniques on how to index and query model built using the previous topic modelling method.</p>
<section class="level3" id="euclidean-and-cosine-similarity">
<h3>Euclidean and Cosine Similarity</h3>
<p>Define what is euclidean and cosine similarity. Emphasise both are correlated on a high-dimensional ball model.</p>
<p>TODO: use an image to illustrate.</p>
</section>
<section class="level3" id="linear-searching">
<h3>Linear Searching</h3>
<p>First implement linear search, in this case, we do not need index at all, but it is very slow. In the following, the corpus is an array of arrays (each of which is an document).</p>
<div class="highlight">
<pre><code class="language-ocaml">(* calculate pairwise distance for the whole model, format (id,dist) *)
let all_pairwise_distance typ corpus x =
  let dist_fun = Owl_nlp_similarity.distance typ in
  let l = Array.mapi (fun i y -&gt; i, dist_fun x y) corpus in
  Array.sort (fun a b -&gt; Stdlib.compare (snd a) (snd b)) l;
  l

(* return the k most relevant documents *)
let query corpus doc k =
  let typ = Owl_nlp_similarity.Cosine in
  let l = all_pairwise_distance typ corpus doc in
  Array.sub l 0 k</code></pre>
</div>
</section>
<section class="level3" id="use-matrix-multiplication">
<h3>Use Matrix Multiplication</h3>
<p>Show that pairwise distance can be done in a matrix multiplication, which is often highly-optimised GEMM operation. We assume the corpus has been converted into a dense matrix, wherein each row vector represents a document.</p>
<div class="highlight">
<pre><code class="language-ocaml">let query corpus doc k =
  let vec = Mat.transpose doc in
  let l = Mat.(corpus *@ vec) in
  Mat.bottom l k</code></pre>
</div>
</section>
<section class="level3" id="random-projection">
<h3>Random Projection</h3>
<p>NOTE: give an image illustration on what is random project, but no need to implement. We will reserve in-depth discussion in Recommender System Chapter.</p>
<p>Only give a teaser here which goes like: “Linear Search does not rely on data structure, but is slow; matrix multiplication is paralleled version LS, faster, but still compute all the pairwise. If you want to simplify computation, you need to use index, and Random Projection is a widely used index method in industry. It is a big topic, and we will cover that in detail in the case chapter.”</p>
<figure>
<img alt="" style="width:90.0%" id="fig:nlp:plot_01" title="plot_01" src="images/nlp/plot_01.png"><figcaption>Figure 1: Random projection on 2D plane</figcaption>
</figure>
</section>
</section>
<section class="level2" id="summary">
<h2>Summary</h2>
<p>TBD</p>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-mikolov2013exploiting">
<p>Mikolov, Tomas, Quoc V Le, and Ilya Sutskever. 2013. “Exploiting Similarities Among Languages for Machine Translation.” <em>arXiv Preprint arXiv:1309.4168</em>.</p>
</div>
</div>
</section>
</section>
</article></div><a href="dataframe.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 16</small>Dataframe for Tabular Data</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>