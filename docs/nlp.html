<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Natural Language Processing - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="natural-language-processing">
<h1>Natural Language Processing</h1>
<p>Text is a dominant media type on the Internet along with images, videos, and audios. Many of our day-to-day tasks involve text analysis. Natural language processing is a powerful tool to extract insights from text copora.</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Survey the literature, give a high-level picture of NLP. Talk about classic NLP … structured and unstructured text …</p>
<p>In this chapter, we will use a <a href="https://github.com/ryanrhymes/owl_dataset/raw/master/news.txt.gz">news dataset</a> crawled from the Internet. It contains 130000 pieces of news from various sources, each line in the file represents one entry.</p>
</section>
<section class="level2" id="text-corpus">
<h2>Text Corpus</h2>
<p>We call a collection of documents a text corpus.</p>
<section class="level3" id="step-by-step-operation">
<h3>Step-by-step Operation</h3>
<p>Show how to build a corpus from a collection of documents, in a step step way.</p>
<p>Preprocess the text, convert all the text into lowercase.</p>
<div class="highlight">
<pre><code class="language-ocaml">let simple_process s =
  Str.split Owl_nlp_utils.regexp_split s
  |&gt; List.filter (fun x -&gt; String.length x &gt; 1)
  |&gt; String.concat " "
  |&gt; String.lowercase_ascii
  |&gt; Bytes.of_string

let preprocess input_file =
  let output_file = input_file ^ ".output" in
  Nlp.Corpus.preprocess simple_process input_file output_file</code></pre>
</div>
<p>Then let’s build vocabulary out of the text corpus.</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_vocabulary input_file =
  let vocab = Nlp.Vocabulary.build input_file in

  (* print out the words of highest frequency *)
  Nlp.Vocabulary.top vocab 10 |&gt;
  Owl.Utils.Array.to_string ~sep:", " fst;

  (* print out the words of lowest frequency*)
  Nlp.Vocabulary.bottom vocab 10 |&gt;
  Owl.Utils.Array.to_string ~sep:", " fst;

  (* save the vocabulary *)
  let output_file = input_file ^ ".vocab" in 
  Nlp.Vocabulary.save vocab output_file</code></pre>
</div>
<p>The progress of building the vocabulary is printed out. After the vocabular is built, the token of the highest frequency is printed out.</p>
<div class="highlight">
<pre><code class="language-text">2020-01-28 20:29:41.592 INFO : processed 13485, avg. 2696 docs/s
2020-01-28 20:29:46.593 INFO : processed 24975, avg. 2497 docs/s
2020-01-28 20:29:51.593 INFO : processed 39728, avg. 2648 docs/s
2020-01-28 20:29:56.595 INFO : processed 53277, avg. 2663 docs/s
2020-01-28 20:30:01.595 INFO : processed 62908, avg. 2516 docs/s
2020-01-28 20:30:06.595 INFO : processed 69924, avg. 2330 docs/s
...
- : string =
"the, to, of, a, and, in, \", s, that, on"
"eichorst, gcs, freeross, depoliticisation, humping, shopable, appurify, intersperse, vyaecheslav, raphaelle"</code></pre>
</div>
<p>Now let’s trim off some most and least frequency words. You can trim either by absolute number or by percent. We use percent here, namely triming off top and bottom 1% of the words.</p>
<div class="highlight">
<pre><code class="language-ocaml">let trim_vocabulary vocab =
  Nlp.Vocabulary.trim_percent ~lo:0.01 ~hi:0.01 vocab</code></pre>
</div>
<p>With a vocabulary at hands, now we are ready to tokenise a piece of text.</p>
<div class="highlight">
<pre><code class="language-ocaml">let tokenise vocab text =
  String.split_on_char ' ' text |&gt; 
  List.map (Nlp.Vocabulary.word2index vocab)</code></pre>
</div>
<p>For example, if we tokenise “this is owl book”, you will get the following output.</p>
<div class="highlight">
<pre><code class="language-text">tokenise vocab "this is an owl book";;
- : int list = [55756; 18322; 109456; 90661; 22362]</code></pre>
</div>
<p>Furthermore, we can now tokenise the whole news collection.</p>
<div class="highlight">
<pre><code class="language-ocaml">(* TODO *)</code></pre>
</div>
</section>
<section class="level3" id="use-convenient-function">
<h3>Use Convenient Function</h3>
<p>Show how to build corpus using a convenient function in <code>Nlp.Corpus</code></p>
<div class="highlight">
<pre><code class="language-ocaml">let main () =
  let corpus = Nlp.Corpus.build "news.txt" in
  Nlp.Corpus.save corpus "news.corpus";
  Nlp.Corpus.print corpus</code></pre>
</div>
<p>The output is like this …</p>
<div class="highlight">
<pre><code class="language-text">2020-01-28 19:07:05.461 INFO : build up vocabulary ...
2020-01-28 19:07:10.461 INFO : processed 13587, avg. 2717 docs/s
2020-01-28 19:07:15.463 INFO : processed 26447, avg. 2644 docs/s
2020-01-28 19:07:20.463 INFO : processed 43713, avg. 2913 docs/s
2020-01-28 19:07:25.463 INFO : processed 57699, avg. 2884 docs/s
2020-01-28 19:07:30.463 INFO : processed 65537, avg. 2621 docs/s
2020-01-28 19:07:35.463 INFO : processed 76199, avg. 2539 docs/s
...
2020-01-28 19:08:09.125 INFO : convert to binary and tokenise ...
2020-01-28 19:08:14.126 INFO : processed 16756, avg. 3350 docs/s
2020-01-28 19:08:19.126 INFO : processed 32888, avg. 3288 docs/s
2020-01-28 19:08:24.127 INFO : processed 43110, avg. 2873 docs/s
2020-01-28 19:08:29.127 INFO : processed 48362, avg. 2417 docs/s
2020-01-28 19:08:34.130 INFO : processed 52628, avg. 2104 docs/s
2020-01-28 19:08:39.132 INFO : processed 55727, avg. 1857 docs/s
...
corpus info
  file path  : news.txt
  # of docs  : 129968
  doc minlen : 10
- : unit = ()</code></pre>
</div>
<p>Show how to further process vocabulary by trim out top and bottom frequency words.</p>
</section>
</section>
<section class="level2" id="vector-space-models">
<h2>Vector Space Models</h2>
<p>Survey, explain what is VSM, documents become vectors, i.e.&nbsp;a point in high-dimensional space. With VSM, we can cluster the documents based on their proximity, i.e.&nbsp;similarity.</p>
</section>
<section class="level2" id="bag-of-words-bow">
<h2>Bag of Words (BOW)</h2>
<p>Explain what is BOW</p>
</section>
<section class="level2" id="term-frequencyinverse-document-frequency-tfidf">
<h2>Term Frequency–Inverse Document Frequency (TFIDF)</h2>
<p>Explain what is TFIDF, mention Cambridge Wolfson fellow.</p>
</section>
<section class="level2" id="latent-dirichlet-allocation-lda">
<h2>Latent Dirichlet Allocation (LDA)</h2>
<p>Explain what is LDA.</p>
</section>
<section class="level2" id="latent-semantic-analysis-lsa">
<h2>Latent Semantic Analysis (LSA)</h2>
<p>Explain what is LSA, and how it differs from LDA wrt to derived topics.</p>
</section>
<section class="level2" id="indexing-and-searching">
<h2>Indexing and Searching</h2>
<p>First implement linear search, then explain random projection and implement a naive version.</p>
</section>
<section class="level2" id="conclusion">
<h2>Conclusion</h2>
<p>TBD</p>
</section>
</section>
</article></div><a href="dataframe.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 16</small>Dataframe for Tabular Data</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>