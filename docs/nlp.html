<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Natural Language Processing - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="natural-language-processing">
<h1>Natural Language Processing</h1>
<p>Text is a dominant media type on the Internet along with images, videos, and audios. Many of our day-to-day tasks involve text analysis. Natural language processing is a powerful tool to extract insights from text copora.</p>
<p>This chapter focusses on the vector space models and topic modelling.</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Survey the literature, give a high-level picture of NLP. Talk about classic NLP … structured and unstructured text …</p>
<p>In this chapter, we will use a <a href="https://github.com/ryanrhymes/owl_dataset/raw/master/news.txt.gz">news dataset</a> crawled from the Internet. It contains 130000 pieces of news from various sources, each line in the file represents one entry.</p>
</section>
<section class="level2" id="text-corpus">
<h2>Text Corpus</h2>
<p>We call a collection of documents a text corpus.</p>
<section class="level3" id="step-by-step-operation">
<h3>Step-by-step Operation</h3>
<p>Show how to build a corpus from a collection of documents, in a step step way.</p>
<p>Preprocess the text, convert all the text into lowercase.</p>
<div class="highlight">
<pre><code class="language-ocaml">let simple_process s =
  Str.split Owl_nlp_utils.regexp_split s
  |&gt; List.filter (fun x -&gt; String.length x &gt; 1)
  |&gt; String.concat " "
  |&gt; String.lowercase_ascii
  |&gt; Bytes.of_string

let preprocess input_file =
  let output_file = input_file ^ ".output" in
  Nlp.Corpus.preprocess simple_process input_file output_file</code></pre>
</div>
<p>Then let’s build vocabulary out of the text corpus.</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_vocabulary input_file =
  let vocab = Nlp.Vocabulary.build input_file in

  (* print out the words of highest frequency *)
  Nlp.Vocabulary.top vocab 10 |&gt;
  Owl.Utils.Array.to_string ~sep:", " fst;

  (* print out the words of lowest frequency*)
  Nlp.Vocabulary.bottom vocab 10 |&gt;
  Owl.Utils.Array.to_string ~sep:", " fst;

  (* save the vocabulary *)
  let output_file = input_file ^ ".vocab" in 
  Nlp.Vocabulary.save vocab output_file</code></pre>
</div>
<p>The progress of building the vocabulary is printed out. After the vocabular is built, the token of the highest frequency is printed out.</p>
<div class="highlight">
<pre><code class="language-text">2020-01-28 20:29:41.592 INFO : processed 13485, avg. 2696 docs/s
2020-01-28 20:29:46.593 INFO : processed 24975, avg. 2497 docs/s
2020-01-28 20:29:51.593 INFO : processed 39728, avg. 2648 docs/s
2020-01-28 20:29:56.595 INFO : processed 53277, avg. 2663 docs/s
2020-01-28 20:30:01.595 INFO : processed 62908, avg. 2516 docs/s
2020-01-28 20:30:06.595 INFO : processed 69924, avg. 2330 docs/s
...
- : string =
"the, to, of, a, and, in, \", s, that, on"
"eichorst, gcs, freeross, depoliticisation, humping, shopable, appurify, intersperse, vyaecheslav, raphaelle"</code></pre>
</div>
<p>Now let’s trim off some most and least frequency words. You can trim either by absolute number or by percent. We use percent here, namely triming off top and bottom 1% of the words.</p>
<div class="highlight">
<pre><code class="language-ocaml">let trim_vocabulary vocab =
  Nlp.Vocabulary.trim_percent ~lo:0.01 ~hi:0.01 vocab</code></pre>
</div>
<p>With a vocabulary at hands, now we are ready to tokenise a piece of text.</p>
<div class="highlight">
<pre><code class="language-ocaml">let tokenise vocab text =
  String.split_on_char ' ' text |&gt; 
  List.map (Nlp.Vocabulary.word2index vocab)</code></pre>
</div>
<p>For example, if we tokenise “this is owl book”, you will get the following output.</p>
<div class="highlight">
<pre><code class="language-text">tokenise vocab "this is an owl book";;
- : int list = [55756; 18322; 109456; 90661; 22362]</code></pre>
</div>
<p>Furthermore, we can now tokenise the whole news collection.</p>
<div class="highlight">
<pre><code class="language-ocaml">let tokenise_all vocab input_file =
  let doc_s = Owl_utils.Stack.make () in

  Owl_io.iteri_lines_of_file
    (fun i s -&gt;
      let t =
        Str.split Owl_nlp_utils.regexp_split s
        |&gt; List.filter (Owl_nlp_vocabulary.exits_w vocab)
        |&gt; List.map (Owl_nlp_vocabulary.word2index vocab)
        |&gt; Array.of_list
      in
      Owl_utils.Stack.push doc_s i)
    input_file;

  doc_s</code></pre>
</div>
<p>Even though this is a simplified case, it well illustrates the typical starting point of text analysis before delving into any topic modelling.</p>
</section>
<section class="level3" id="use-convenient-function">
<h3>Use Convenient Function</h3>
<p>Show how to build corpus using a convenient function in <code>Nlp.Corpus</code>. The convenient function builds the dictionary and tokienise the text corpus at the same time. You can further specify how to trim off the high-frequency and low-frenquency words when calling the function.</p>
<div class="highlight">
<pre><code class="language-ocaml">let main () =
  (* remove duplicates *)
  let ids = Nlp.Corpus.unique "news.txt" "clean.txt" in
  Printf.printf "removed %i duplicates." (Array.length ids);

  (* build vocabulary and tokenise *)
  let corpus = Nlp.Corpus.build ~lo:0.01 ~hi:0.01 "clean.txt" in
  Nlp.Corpus.save corpus "news.corpus";
  Nlp.Corpus.print corpus</code></pre>
</div>
<p>The output is like this …</p>
<div class="highlight">
<pre><code class="language-text">2020-01-28 19:07:05.461 INFO : build up vocabulary ...
2020-01-28 19:07:10.461 INFO : processed 13587, avg. 2717 docs/s
2020-01-28 19:07:15.463 INFO : processed 26447, avg. 2644 docs/s
2020-01-28 19:07:20.463 INFO : processed 43713, avg. 2913 docs/s
2020-01-28 19:07:25.463 INFO : processed 57699, avg. 2884 docs/s
2020-01-28 19:07:30.463 INFO : processed 65537, avg. 2621 docs/s
2020-01-28 19:07:35.463 INFO : processed 76199, avg. 2539 docs/s
...
2020-01-28 19:08:09.125 INFO : convert to binary and tokenise ...
2020-01-28 19:08:14.126 INFO : processed 16756, avg. 3350 docs/s
2020-01-28 19:08:19.126 INFO : processed 32888, avg. 3288 docs/s
2020-01-28 19:08:24.127 INFO : processed 43110, avg. 2873 docs/s
2020-01-28 19:08:29.127 INFO : processed 48362, avg. 2417 docs/s
2020-01-28 19:08:34.130 INFO : processed 52628, avg. 2104 docs/s
2020-01-28 19:08:39.132 INFO : processed 55727, avg. 1857 docs/s
...
corpus info
  file path  : news.txt
  # of docs  : 129968
  doc minlen : 10
- : unit = ()</code></pre>
</div>
<p>The <code>save</code> function will create several files, explain these files …</p>
</section>
<section class="level3" id="iterate-documents">
<h3>Iterate Documents</h3>
<p>Teach how to use <code>Nlp.Corpus.next</code> and etc. to iterate and map documents.</p>
</section>
</section>
<section class="level2" id="vector-space-models">
<h2>Vector Space Models</h2>
<p>Survey, explain what is VSM, documents become vectors, i.e.&nbsp;a point in high-dimensional space. With VSM, we can cluster the documents based on their proximity, i.e.&nbsp;similarity.</p>
</section>
<section class="level2" id="bag-of-words-bow">
<h2>Bag of Words (BOW)</h2>
<p>Explain what is BOW, simply counting the frequency. What are the pros and cons of this method?</p>
<div class="highlight">
<pre><code class="language-ocaml">(* count the term occurrency in a document *)
let term_count htbl doc =
  Array.iter
    (fun w -&gt;
      match Hashtbl.mem htbl w with
      | true  -&gt;
        let a = Hashtbl.find htbl w in
        Hashtbl.replace htbl w (a +. 1.)
      | false -&gt; Hashtbl.add htbl w 1.)
    doc

(* build bag-of-words for the corpus *)
let build_bow corpus =
  Nlp.Corpus.mapi_tok 
    (fun i doc -&gt;
      let htbl = Hashtbl.create 128 in
      term_count htbl;
      htbl)
    corpus</code></pre>
</div>
</section>
<section class="level2" id="term-frequencyinverse-document-frequency-tfidf">
<h2>Term Frequency–Inverse Document Frequency (TFIDF)</h2>
<p>Explain what is TFIDF, mention Cambridge Wolfson fellow. The corpus we have built in the previous section is used as input to the following function.</p>
<p>Explain why TFIDF is better than BOW, what is the motivation. Give an examble to illustrate.</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_tfidf corpus =
  (* configure and build the model *)
  let tf = Nlp.Tfidf.Count in
  let df = Nlp.Tfidf.Idf in
  let model = Nlp.Tfidf.build ~tf ~df corpus in

  (* print and save model *)
  Nlp.Tfidf.print model;
  Nlp.Tfidf.save model "news.tfidf";

  model</code></pre>
</div>
<p>After the model is build, illustrate how to find k similar documents. The following exmaple uses consine similarity, then convert a document into vector using previously trained TFIDF model. Note do NOT teach how to index and how the similarity is calculted here, teach in Indexing and Searching section.</p>
<div class="highlight">
<pre><code class="language-ocaml">let query model doc k =
  (* TODO: change api *)
  let typ = Owl_nlp_similarity.Cosine in
  let vec = Nlp.Tfidf.apply model doc in
  let knn = Nlp.Tfidf.nearest ~typ model vec k in
  knn</code></pre>
</div>
</section>
<section class="level2" id="latent-dirichlet-allocation-lda">
<h2>Latent Dirichlet Allocation (LDA)</h2>
<p>Explain what is LDA. <code>topics</code> is the number of topics. Owl supports the following types of LDA algorithms.</p>
<div class="highlight">
<pre><code class="language-ocaml">type lda_typ =
  | SimpleLDA
  | FTreeLDA
  | LightLDA
  | SparseLDA</code></pre>
</div>
<p>How to train an LDA model.</p>
<div class="highlight">
<pre><code class="language-text">(* change to ocaml when image progation finished *)
let build_lda corpus topics =
  let model = Nlp.Lda.init ~iter:1000 topics in
  let lda_typ = Nlp.Lda.SparseLDA in
  Nlp.Lda.train lda_typ model;
  Owl.Log.info "LDA training finished."</code></pre>
</div>
</section>
<section class="level2" id="latent-semantic-analysis-lsa">
<h2>Latent Semantic Analysis (LSA)</h2>
<p>Explain what is LSA, and how it differs from LDA wrt to derived topics.</p>
</section>
<section class="level2" id="indexing-and-searching">
<h2>Indexing and Searching</h2>
<p>Topic models are effective tools for clustering documents based on their similarity or relevance. We can further use this tool to query relevant document given an input one. In this section, we will go through some techniques on how to index and query model built using the previous topic modeling method.</p>
<section class="level3" id="euclidean-and-consine-similarity">
<h3>Euclidean and Consine Similarity</h3>
<p>Define what is euclidean and consine similarity. Emphasise both are correlated on a high-dimensional ball model.</p>
<p>TODO: use an image to illustrate.</p>
</section>
<section class="level3" id="liear-searching">
<h3>Liear Searching</h3>
<p>First implement linear search, in this case, we do not need index at all, but it is very slow.</p>
<div class="highlight">
<pre><code class="language-ocaml">(* TODO *)</code></pre>
</div>
</section>
<section class="level3" id="use-matrix-multiplication">
<h3>Use Matrix Multiplication</h3>
<p>Show that pairwise distance can be done in a matrix multiplication, which is often highly-optimised GEMM operation.</p>
<div class="highlight">
<pre><code class="language-ocaml">(* TODO *)</code></pre>
</div>
</section>
<section class="level3" id="random-projection">
<h3>Random Projection</h3>
<p>NOTE: give an image illustration on what is random project, but no need to implement. We will leave this in Recommender System Chapter.</p>
</section>
</section>
<section class="level2" id="conclusion">
<h2>Conclusion</h2>
<p>TBD</p>
</section>
</section>
</article></div><a href="dataframe.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 16</small>Dataframe for Tabular Data</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>