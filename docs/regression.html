<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Regression - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="regression">
<h1>Regression</h1>
<p>Regression is an important topic in statistical modelling and machine learning. It’s about modelling problems that includes one or more variables (also called “features” or “predictors”) and making predictions of another variable (“output variable”) based on previous data of predictors.</p>
<p>Regression analysis includes a wide range of models, from linear regression to isotonic regression, each with different theory background and application fields. Introducing all these models are beyond this book. In this chapter, we focus on several common form of regressions, mainly linear regression and logistic regression. We introduce their basic ideas, how they are supported in Owl, and how to use them to solve problems.</p>
<section class="level2" id="model-and-error">
<h2>Model and Error</h2>
<section class="level3" id="standard-errors-of-regression-coefficients">
<h3>Standard Errors of Regression Coefficients</h3>
</section>
<section class="level3" id="model-selection">
<h3>Model Selection</h3>
<p>Feature selection: REFER to ISL book Chap 6.</p>
</section>
</section>
<section class="level2" id="linear-regression">
<h2>Linear Regression</h2>
<p>Linear regression models the relationship of the features and output variable with a linear model. Let’s start with a simple problem where only one feature needs to be considered.</p>
<section class="level3" id="problem-where-to-locate-a-new-mcdonalds-restaurant">
<h3>Problem: Where to locate a new McDonald’s restaurant?</h3>
<p>McDonald’s is no doubt one of the most successful fast food chains in the world. Up to 2018, it has already had more than 37, 000 stores world wide, and surely more is being built as you are reading. One question then is: where to locate a new McDonald’s restaurant?</p>
<p>According to its <a href="https://www.mcdonalds.com/gb/en-gb/help/faq/18665-how-do-you-decide-where-to-open-a-new-restaurant.html#">website</a>, a lot of factors are in play: area population, existing stores in the area, proximity to retail parks, shopping centres, etc. Now let’s simplified this problem by asserting that the potential profit is only related to area population. Suppose you are the decision maker in McDonald’s, and also have access to data of each branch store (profit, population around this branch). Now linear regression would be a good friend when you are deciding where to locate your next branch.</p>
<p>Here list a part of the data (TODO: link to csv file):</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Profit</th>
<th style="text-align: center;">Population</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">21.76</td>
</tr>
<tr class="even">
<td style="text-align: center;">5.49</td>
<td style="text-align: center;">4.26</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6.32</td>
<td style="text-align: center;">5.18</td>
</tr>
<tr class="even">
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">3.08</td>
</tr>
<tr class="odd">
<td style="text-align: center;">18.94</td>
<td style="text-align: center;">22.63</td>
</tr>
<tr class="even">
<td style="text-align: center;">12.82</td>
<td style="text-align: center;">13.50</td>
</tr>
<tr class="odd">
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
</tr>
</tbody>
</table>
<p>Visualising these data can present a clear view.</p>
<p>TODO: CODE and IMAGE in Owl</p>
<p>According to this figure, there is a clear trend that larger population and larger profit are co-related together. But precisely how?</p>
</section>
<section class="level3" id="cost-function">
<h3>Cost Function</h3>
<p>Let’s start with a linear model that assumes the the relationship between these two variables be formalised as: <span class="math display">\[ y = \theta_0~ + \theta_1~x_1 + \epsilon\]</span>, where <span class="math inline">\(y\)</span> denotes the profit we want to predict, and input variable <span class="math inline">\(x_1\)</span> is the population number in this example. Since modelling can hardly make a perfect match with the real data, we use <span class="math inline">\(\epsilon\)</span> to denote the error between our prediction and the data. Specifically, we represent the prediction part as <span class="math inline">\(h(\theta_0, \theta_1)\)</span>: <span class="math display">\[h(\theta_0, \theta_1) = \theta_0~ + \theta_1~x_1\]</span></p>
<p>The <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> are the parameters of this model. Mathematically they decide a line on a plain. We can now choose randomly these parameters and see how the result works, and some of these guesses are just bad intuitively. Our target is to choose suitable parameters so that the line is <em>close</em> to data we observed.</p>
<p>TODO: Figures with data, and also some random lines. Maybe three figures, and two of them are bad fit.</p>
<p>How do we define the line being “close” to the observed data then? One frequently used method is to use the <em>ordinary least square</em> to minimizes the sum of squared distances between the data and line. We have shown the “<span class="math inline">\(x\)</span>-<span class="math inline">\(y\)</span>” pairs in the data above, and we represent the total number of data pairs with <span class="math inline">\(n\)</span>, and thus the <span class="math inline">\(i\)</span>’th pair of data can be represented with <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>. With these notations, we can represent a metric to represent the <em>closeness</em> as:</p>
<p><span class="math display">\[J(\theta_0, \theta_1) = \frac{1}{2n}\sum_{i=1}^{n}(h_{\theta_1, \theta_0}(x_i^2) - y_i)\]</span></p>
<p>In regression, we call this function the <em>cost function</em>. It measures how close the models are to ideal cases, and our target is thus clear: find suitable <span class="math inline">\(\theta\)</span> parameters to minimise the cost function.</p>
<p><strong>TIPS</strong>: Why do we use least square in the cost function? Physically, the cost function <span class="math inline">\(J\)</span> represents the average distance of each data point to the line – by “distance” we mean the the euclidean distace. between a data point and the point on the line with the same x-axis. A reasonable solution can thus be achieved by minimising this average distance. On the other hand, from the statistical point of view, minimizing the sum of squared errors leads to maximizing the likelihood of the data. TODO: explain the relationship between maximum likelihood estimation and least square.</p>
</section>
<section class="level3" id="solving-problem-with-gradient-descent">
<h3>Solving Problem with Gradient Descent</h3>
<p>To give a clearer view, we can visualise the cost function with a contour graph:</p>
<p>IMAGE (CODE if we can do that in Owl)</p>
<p>We can see that cost function varies with parameters <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> with a bowl-like shape curve surface. It is thus natural to recall the gradient descent we have introduced in the previous chapter, and use it to find the minimal point in this bowl-shape surface.</p>
<p>Recall from previous chapter that gradient descent works by starting at one point on the surface, and move gradually towards certain <em>direction</em> at some <em>step size</em>, and hopefully can converge at a local minimum. Let’s use a fixed step size <span class="math inline">\(\alpha\)</span>, and the direction at certain point on the surface can be gotten by using partial derivative on the surface. Therefore, what we need to do is to apply this update process iteratively for both <span class="math inline">\(\theta\)</span> parameters: <span class="math display">\[ \theta_j \leftarrow \theta_j - \alpha~\frac{\partial}{\partial \theta_j}~J(\theta_0, \theta_1), \]</span> where <span class="math inline">\(i\)</span> is 1 or 2.</p>
<p>This process may seem terrible at first sight, but we can calculate it as:</p>
<p><span class="math display">\[ \theta_0 \leftarrow \theta_0 - \frac{\alpha}{n}\sum_{i=1}^{m} (h_{\theta_0, \theta_1}(x_i) - y_i)x_{i0}, \]</span> and <span class="math display">\[ \theta_1 \leftarrow \theta_1 - \frac{\alpha}{n}\sum_{i=1}^{m} (h_{\theta_0, \theta_1}(x_i) - y_i)x_{i1}.\]</span></p>
<p>Here the <span class="math inline">\(x_i0\)</span> and <span class="math inline">\(x_i1\)</span> are just different input features of the <span class="math inline">\(i\)</span>-th row in data. Since currently we only focus on one feature in our problem, <span class="math inline">\(x_i0 = 1\)</span> and <span class="math inline">\(x_i1 = x_i\)</span>. Following these equations, you can manually perform the gradient descent process until it converges. Here is the code.</p>
<div class="highlight">
<pre><code class="language-clike">TODO: CODE and explanation.</code></pre>
</div>
<p>By executing the code, we can get a pair of parameters: <span class="math inline">\(\theta_0 = xxx\)</span> and <span class="math inline">\(\theta-1 = xxx\)</span>. To check if they indeed are suitable parameters, we can visualise them against the input data. The resulting figure shows a line that aligns with input data.</p>
<p>TODO: PLOT: the resulting line against data samples.</p>
<p>Of course, there is no need to use to manually solve a linear regression problem with Owl. It has already provides high-level regression functions for use. For example, <code>ols</code> uses the odinary least square method we have introduced to perform linear regression.</p>
<div class="highlight">
<pre><code class="language-clike">val ols : ?i:bool -&gt; arr -&gt; arr -&gt; arr array</code></pre>
</div>
And we can use that to directly solve the problem, and the resulting parameters are similar to what we have get manually.
<div class="highlight">
<pre><code class="language-clike">CODE and result (no need to figure).</code></pre>
</div>
<p>Another approach is from the perspective of function optimisation instead of regression. We can use the gradient descent optimisation method in Owl and apply it directly on the cost function.</p>
<div class="highlight">
<pre><code class="language-clike">CODE and result (no need to figure).</code></pre>
</div>
</section>
</section>
<section class="level2" id="multiple-regression">
<h2>Multiple Regression</h2>
<p>Back to our McDonald’s problem. We have seen how a new store’s profit can be related to the population of it’s surrounding, and we can even predict it given previous data. Now, remember that in the real world, population is not the only input features that affects the store’s profit. Other factors such as existing stores in the area, proximity to retail parks, shopping centres, etc. also play a role. In that case, how can we extend our one-variable linear regression to the case of multiple variables?</p>
<p>The answer is very straight forward. We just use more parameters, so the model becomes:</p>
<p><span class="math display">\[h(\theta_0, \theta_1, \theta_2, \theta_3, ...) = \theta_0~ + \theta_1~x_1 + \theta_2~x_2 + \theta_3~x_3 ... \]</span></p>
<p>However, to list all the parameters explicitly is not a good idea, especially when the question requires considering thousands or even more features. Therefore, we use the vectorised format in the model: <span class="math display">\[h(\Theta) = \Theta~X^{(i)}\]</span>, where <span class="math inline">\(\Theta = [\theta_0, \theta_1, \theta_2, \theta_3, ...]\)</span>, and <span class="math inline">\(X^{(i)} = [1, x_1, x_2, x_3, ...]^T\)</span> contains all the features from the <span class="math inline">\(i\)</span>th row in data.</p>
<p>Accordingly, the cost function can be represented as: <span class="math display">\[ J(\Theta) = \frac{1}{2n}\sum_{i=1}^{n}(\Theta~X^{(i)} - y^{(i)})^2\]</span>, where <span class="math inline">\(y^{(i)}\)</span> is the output variable value on the <span class="math inline">\(i\)</span>th row of input data.</p>
<p>The derivative and manual gradient descent are left as exercise. Here we only show an example of using the regression function Owl has provided. Similar to the previous problem, we provide some data to this multiple variable problem. Part of the data are listed below:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x_2\)</span></th>
<th style="text-align: center;">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1888</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">255000</td>
</tr>
<tr class="even">
<td style="text-align: center;">1604</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">242900</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1962</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">259900</td>
</tr>
<tr class="even">
<td style="text-align: center;">3890</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">573900</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1100</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">249900</td>
</tr>
<tr class="even">
<td style="text-align: center;">1458</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">464500</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2526</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">469000</td>
</tr>
<tr class="even">
<td style="text-align: center;">2200</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">475000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
</tr>
</tbody>
</table>
The problem has two different features. Again, by using the <code>ols</code> regression function in Owl, we can easily get the multi-variable linear model.
<div class="highlight">
<pre><code class="language-clike">CODE + result</code></pre>
</div>
<section class="level3" id="feature-normalisation">
<h3>Feature Normalisation</h3>
<p>However, getting a result doesn’t mean the end. Using the multi-variable regression problem as example, we would like to discuss an important issue about regression: feature normalisation.</p>
<p>Let’s look at the multi-variable data again. Apparently, the first feature is magnitude larger than the second feature. That means the model and cost function are dominated by the first feature, and a minor change of this column will have a disproportionally large impact on the model.</p>
<p>To overcome this problem, we hope to pre-process the data before the regression, and normalise every features within about [-1, 1]. This step is also called feature scaling. There are many ways to do this, and one of them is the <em>mean normalisation</em>: for a column of features, calculate its mean, and divided by the difference between the largest value and smallest value, as shown in the code:</p>
<div class="highlight">
<pre><code class="language-clike">CODE: normalisation</code></pre>
</div>
<p>Another benefit of performing data normalisation is that gradient descent can be accelerated. The illustration below shows the point.</p>
<p>IMAGES: two image, one ellipses, one circles. with arrows showing the steps towards center.</p>
<p>Normalisation is not only used in regression, but also may other data analysis and machine learning tasks. For example, in computer vision tasks, an image is represented as an ndarray with three dimension. Each element represents an pixel in the image, with a value between 0 and 255. More often than not, this ndarray needs to be normalised in data pre-processed for the next step processing such as image classification.</p>
</section>
<section class="level3" id="analytical-solution">
<h3>Analytical Solution</h3>
<p>Before taking a look at some other forms of regression, let’s discuss solution to the linear regression besides gradient descent. It turns out that there is actually one close form solution to linear regression problems:</p>
<p><span class="math display">\[\Theta = (X^T~X)^{-1}X^Ty\]</span></p>
<p>Suppose the linear model contains <span class="math inline">\(m\)</span> features, and the input data contains <span class="math inline">\(n\)</span> rows, then here <span class="math inline">\(X\)</span> is a <span class="math inline">\(n\times~(m+1)\)</span> matrix representing the features data, and the output data <span class="math inline">\(y\)</span> is a <span class="math inline">\(n\times~1\)</span> matrix. The reason there is m+1 columns in <span class="math inline">\(X\)</span> is that we need an extra constant feature for each data, and it equals to one for each data point.</p>
<p>TODO: where does this equation come from?</p>
<p>With this method, we don’t need to iterate the solutions again and again until converge. We can just compute the result with one pass with the given input data. This calculation can be efficiently performed in Owl using its Linear Algebra module. Let’s use the dataset from multi-variable regression again and perform the computation.</p>
<div class="highlight">
<pre><code class="language-clike">CODE: close form solution.</code></pre>
</div>
<p>TODO: compare the result with previous GD solution.</p>
<p>Compared to the gradient descent solution, the methods does not require multiple iterations, and you also don’t need to worry about hyper-parameters settings such as the choice of learning rate. On the other hand, however, this approach has its own problems.</p>
<p>When the size of <span class="math inline">\(X\)</span>, or the input data, becomes very large, the computation of large linear algebra operations such as matrix multiplication and inversion could become really slow. Or even worse: your computer might don’t even have enough memory to perform the computation. Compare to it, gradient descent proves to work well even when the dataset is large.</p>
<p>Besides, there could be no solution at all using this method. That’s when the <span class="math inline">\(X^T~X\)</span> matrix is non-invertible, e.g.&nbsp;a singular matrix. That could be caused by multiple reasons. Perhaps some of the features are linear dependent, or that there are many redundant features. Then techniques such as choosing feature or regularisation are required.</p>
<p>Most importantly, there is not always a close-form solution for you to use in other regression or machine learning problems. Gradient descent is a much more general solution.</p>
</section>
</section>
<section class="level2" id="non-linear-regressions">
<h2>Non-linear regressions</h2>
<p>If only the world is as simple as linear regression. But that’s not to be. A lot of data can follow other patterns than a linear one. For example, checkout the dataset below:</p>
<p>IMAGE, the dataset that follows a convex curve.</p>
<p>You can try to fit a line into these data, but it’s quite likely that the result would be very fitting. And that requires non-linear models.</p>
<p>In this section, we present two common non-linear regressions: the polynomial regression, and exponential regression. We shows how to use them with examples, and won’t go into details of the math. Refer to [reference] for more details.</p>
<p>In polynomial regression, the relationship between the feature <span class="math inline">\(x\)</span> and the output variable is modelled as an nth degree polynomial in the feature <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[ h(\Theta) = \theta_0 + \theta_1~x + \theta_2~x^2 + \theta_3~x^3 \ldots \]</span>.</p>
<p>The model for exponential regression takes two parameters:</p>
<p><span class="math display">\[ h(\theta_0, \theta_1) = \theta_0~\theta_1^x\]</span>.</p>
<p>Owl provides functions to do both form of regressions:</p>
<div class="highlight">
<pre><code class="language-clike">val exponential : ?i:bool -&gt; arr -&gt; arr -&gt; elt * elt * elt

val poly : arr -&gt; arr -&gt; int -&gt; arr</code></pre>
</div>
<p>Let’s look at how to use them in the code. The dataset is the same as in previous figure, contained in the file <a href="Link">data_03.csv</a>.</p>
<div class="highlight">
<pre><code class="language-clike">CODE: Polynomial. We limit that to 3th order. </code></pre>
</div>
<p>The result we get is: … . That gives us the polynomial model <span class="math inline">\(y = x + x^2 + x^3 + \epsilon\)</span>.</p>
<p>The code for exponential regression is similar:</p>
<div class="highlight">
<pre><code class="language-clike">CODE: exponential reg.</code></pre>
</div>
<p>The result we get is … That leads to a model: <span class="math inline">\(y = ab^x + \epsilon\)</span>.</p>
<p>Let’s see show the models works in fitting data:</p>
<p>IMAGE: data scatter point with two curves.</p>
</section>
<section class="level2" id="regularisation">
<h2>Regularisation</h2>
<p>Regularisation is an important issue in are discussed here, but are by no means limited to the topic of linear regression. You might be able to see them in logistic regression or even clustering.</p>
<section class="level3" id="ols-ridge-lasso-and-elastic_net">
<h3>Ols, Ridge, Lasso, and Elastic_net</h3>
<p>You might notice that</p>
<p><a href="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net">REFER</a></p>
</section>
</section>
<section class="level2" id="logistic-regression">
<h2>Logistic Regression</h2>
<p>So far we have been predicting a value for our problems, but what if we don’t care about is not the value, but a classification? For example, we want to know if this tumour is cancer or not given previous data.</p>
<p>We can of course continue to use linear regression to represent the possibility of one of these result, but one issue is that, it could well be out of the bounds of [0, 1].</p>
<section class="level3" id="sigmoid-function">
<h3>Sigmoid Function</h3>
<p>EQUATION + IMAGE</p>
<p>With this function, instead of <span class="math inline">\(h = \Theta~X\)</span>, the problem can be modelled as:</p>
<p><span class="math display">\[h(\Theta) = g(\Theta~X)\]</span></p>
<p>Now we can interpret this model easily. If it is larger than 0.5, then … else …</p>
</section>
<section class="level3" id="decision-boundary">
<h3>Decision Boundary</h3>
<p>Linear</p>
<p>IMAGE</p>
<p>Non-Linear</p>
<p>IMAGE</p>
<p>Let’s use the non-linear one as practice example.</p>
</section>
<section class="level3" id="cost-function-1">
<h3>Cost Function</h3>
<p>With the new model comes new cost function.</p>
<p>Measure the <em>distance</em>: Previously for linear regression we have the Euclidean distance of prediction and real value.</p>
<p>Now we defined it this way: Cost equation that involves <span class="math inline">\(log\)</span> function. Explain how it comes: maximise the log likelihood</p>
<p>Therefore, we have this cost function:</p>
<p><span class="math display">\[ J(\Theta) = \frac{1}{n}\sum_{i=1}^{n}\textrm{Cost}(h_{\Theta}(x_i), y_i)\]</span></p>
</section>
<section class="level3" id="gradient-descent">
<h3>Gradient Descent</h3>
<p>How to solve this terrible equation?</p>
<p>The sigmoid has a nice property: its derivative is simple:</p>
<p>EQUATION</p>
<p>Therefore, similar to LR, we only need to repeat this step until converges:</p>
<p>EQUATION</p>
<p>Let’s write that in Owl:</p>
<p>CODE #1: plain code</p>
<p>CODE #2: use existing function in Owl</p>
<p>Plotting the boundaries.</p>
</section>
<section class="level3" id="multi-class-classification">
<h3>Multi-class classification</h3>
<p>Similar to the LR problem, you hardly stop at 2 parameters. What if we need to classified an object into one fo multiple classes?</p>
<p>One popular classification problem is the hand-written recognition task. It is… It is a widely used ABC task for Neural Networks, and we will also cover it later in Chapter DNN. For now, we solve that from the logistic regression line of thought.</p>
<p>Dataset description Visualise</p>
<p>Similarly, we extend the cost function towards multi-class:</p>
<p>EQUATION</p>
<p>We can also use the generalised version of GD as before, or directly apply GD method in Owl:</p>
<p>CODE</p>
<p>Let’s apply the model on test data:</p>
<p>result.</p>
<p>Discussion on accuracy and possible improvement. Leave for exercise.</p>
</section>
</section>
<section class="level2" id="support-vector-machine">
<h2>Support Vector Machine</h2>
<p>It’s a similar idea to logistic regression.</p>
<p>Explain the history and basic idea about SVM. It’s difference with Log Reg.</p>
<p>Apply the SVM to the previous problem, with multiple choices of kernel, and then plot the result.</p>
</section>
<section class="level2" id="exercise">
<h2>Exercise</h2>
<ol type="1">
<li>Manual gradient descent and optimizer on multiple variable problem</li>
<li>Regularisation of logistic regression could be used as an excise</li>
</ol>
</section>
</section>
</article></div><a href="neural-network.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 14</small>Deep Neural Networks</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>