<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Regression - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="regression">
<h1>Regression</h1>
<p>Regression is an important topic in statistical modelling and machine learning. It’s about modelling problems which include one or more variables (also called “features” or “predictors”) and making predictions of another variable (“output variable”) based on previous data of predictors.</p>
<p>Regression analysis includes a wide range of models, from linear regression to isotonic regression, each with different theory background and application fields. Explaining all these models are beyond the scope of this book. In this chapter, we focus on several common form of regressions, mainly linear regression and logistic regression. We introduce their basic ideas, how they are supported in Owl, and how to use them to solve problems.</p>
<section class="level2" id="linear-regression">
<h2>Linear Regression</h2>
<p>Linear regression models the relationship of the features and output variable with a linear model. It is the most widely used regression model in research and business and is the easiest to understand, so it makes an ideal starting point for us to build understanding or regression. Let’s start with a simple problem where only one feature needs to be considered.</p>
<section class="level3" id="problem-where-to-locate-a-new-mcdonalds-restaurant">
<h3>Problem: Where to locate a new McDonald’s restaurant?</h3>
<p>McDonald’s is undoubtedly one of the most successful fast food chains in the world. By 2018, it has already opened more than 37,000 stores world wide, and surely more is being built as you are reading. One question then is: where to locate a new McDonald’s restaurant?</p>
<p>According to its <a href="https://www.mcdonalds.com/gb/en-gb/help/faq/18665-how-do-you-decide-where-to-open-a-new-restaurant.html#">website</a>, a lot of factors are in play: area population, existing stores in the area, proximity to retail parks, shopping centres, etc. Now let’s simplified this problem by asserting that the potential profit is only related to area population. Suppose you are the decision maker in McDonald’s, and also have access to data of each branch store (profit, population around this branch). Now linear regression would be a good friend when you are deciding where to locate your next branch.</p>
<p>tbl.&nbsp;1 list a part of the data (TODO: link to csv file). (To be honest, this data set (and most of the dataset used below) is not taken from real data source but taken from that of the ML004 course by Andrew Ng. So perhaps you will be disappointed if you are looking for real data from running McDonald’s.)</p>
<div id="tbl:regression:data01">
<table>
<caption>Table 1: Sample of input data: single feature</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Profit</th>
<th style="text-align: center;">Population</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">21.76</td>
</tr>
<tr class="even">
<td style="text-align: center;">5.49</td>
<td style="text-align: center;">4.26</td>
</tr>
<tr class="odd">
<td style="text-align: center;">6.32</td>
<td style="text-align: center;">5.18</td>
</tr>
<tr class="even">
<td style="text-align: center;">5.56</td>
<td style="text-align: center;">3.08</td>
</tr>
<tr class="odd">
<td style="text-align: center;">18.94</td>
<td style="text-align: center;">22.63</td>
</tr>
<tr class="even">
<td style="text-align: center;">12.82</td>
<td style="text-align: center;">13.50</td>
</tr>
<tr class="odd">
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
</tr>
</tbody>
</table>
</div>
<p>Visualising these data can present a clear view. We can use the code below to do that. It first extracts the two columns data from the data file, converts it to dense matrix, and then visualise the data using the scatter plot.</p>
<p>TOOD: fix the code syntax w.r.t file loading</p>
<div class="highlight">
<pre><code class="language-clike">let data = Owl_io.read_csv ~sep:',' "data_01.csv"
let data = Array.map (fun x -&gt; Array.map float_of_string x) data |&gt; Mat.of_arrays

let x = Mat.get_slice [[];[1]] data
let y = Mat.get_slice [[];[0]] data</code></pre>
</div>
<div class="highlight">
<pre><code class="language-clike">let plot_01 () =
  let h = Plot.create "regdata.png" in
  Plot.scatter ~h x y;
  Plot.output h</code></pre>
</div>
<figure>
<img alt="" style="width:50.0%" id="fig:regression:regdata" title="regdata" src="images/regression/regdata.png"><figcaption>Figure 1: Visualise data for regression problem</figcaption>
</figure>
<p>The visualisation is shown in fig.&nbsp;1. As can be expected, there is a clear trend that larger population and larger profit are co-related together. But precisely how?</p>
</section>
<section class="level3" id="cost-function">
<h3>Cost Function</h3>
<p>Let’s start with a linear model that assumes the the relationship between these two variables be formalised as:</p>
<p><span id="eq:regression:eq00"><span class="math display">\[ y = \theta_0~ + \theta_1~x_1 + \epsilon,\qquad(1)\]</span></span></p>
<p>where <span class="math inline">\(y\)</span> denotes the profit we want to predict, and input variable <span class="math inline">\(x_1\)</span> is the population number in this example. Since modelling can hardly make a perfect match with the real data, we use <span class="math inline">\(\epsilon\)</span> to denote the error between our prediction and the data. Specifically, we represent the prediction part as <span class="math inline">\(h(\theta_0, \theta_1)\)</span>: <span id="eq:regression:eq01"><span class="math display">\[h(\theta_0, \theta_1) = \theta_0~ + \theta_1~x_1\qquad(2)\]</span></span></p>
<p>The <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> are the parameters of this model. Mathematically they decide a line on a plane. We can now choose randomly these parameters and see how the result works, and some of these guesses are just bad intuitively, as shown in fig.&nbsp;2. Our target is to choose suitable parameters so that the line is <em>close</em> to data we observed.</p>
<p>TODO: Figures with data, and also some random lines. Maybe three figures, and two of them are bad fit.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:regression:reg_options" title="reg_options" src="images/regression/reg_options.png"><figcaption>Figure 2: Find possible regression line for given data</figcaption>
</figure>
<p>How do we define the line being “close” to the observed data then? One frequently used method is to use the <em>ordinary least square</em> to minimizes the sum of squared distances between the data and line. We have shown the “<span class="math inline">\(x\)</span>-<span class="math inline">\(y\)</span>” pairs in the data above, and we represent the total number of data pairs with <span class="math inline">\(n\)</span>, and thus the <span class="math inline">\(i\)</span>’th pair of data can be represented with <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>. With these notations, we can represent a metric to represent the <em>closeness</em> as:</p>
<p><span id="eq:regression:eq02"><span class="math display">\[J(\theta_0, \theta_1) = \frac{1}{2n}\sum_{i=1}^{n}(h_{\theta_1, \theta_0}(x_i) - y_i)^2\qquad(3)\]</span></span></p>
<p>In regression, we call this function the <em>cost function</em>. It measures how close the models are to ideal cases, and our target is thus clear: find suitable <span class="math inline">\(\theta\)</span> parameters to minimise the cost function.</p>
<p><strong>TIPS</strong>: Why do we use least square in the cost function? Physically, the cost function <span class="math inline">\(J\)</span> represents the average distance of each data point to the line – by “distance” we mean the the euclidean distace. between a data point and the point on the line with the same x-axis. A reasonable solution can thus be achieved by minimising this average distance. On the other hand, from the statistical point of view, minimizing the sum of squared errors leads to maximizing the likelihood of the data. TODO: explain the relationship between maximum likelihood estimation and least square.</p>
</section>
<section class="level3" id="solving-problem-with-gradient-descent">
<h3>Solving Problem with Gradient Descent</h3>
<p>To give a clearer view, we can visualise the cost function with a contour graph. According to eq.&nbsp;3, the cost function <code>j</code> is implemented as below:</p>
<div class="highlight">
<pre><code class="language-clike">let j theta0 theta1 = 
  let f x = x *. theta1 +. theta0 in
  Mat.(pow_scalar (map f x - y) 2. |&gt; mean') *. 0.5</code></pre>
</div>
<p>We can then visualise this cost function within a certain range using surf and contour graphs:</p>
<div class="highlight">
<pre><code class="language-clike">let plot_02 () = 
  let x, y = Mat.meshgrid (-20.) 10. (-20.) 10. 100 100 in
  let z = Mat.(map2 j x y) in
  let h = Plot.create ~m:1 ~n:2 "reg_cost.png" in
  Plot.subplot h 0 0;
  Plot.(mesh ~h ~spec:[ NoMagColor ] x y z);
  Plot.set_xlabel h "theta0";
  Plot.set_ylabel h "theta1";
  Plot.set_zlabel h "cost";
  Plot.subplot h 0 1;
  Plot.contour ~h x y z;
  Plot.set_xlabel h "theta0";
  Plot.set_ylabel h "theta1";
  Plot.output h</code></pre>
</div>
<p>In fig.&nbsp;3 we can see that cost function varies with parameters <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> with a bowl-like shape curve surface. It is thus natural to recall the gradient descent we have introduced in the previous chapter, and use it to find the minimal point in this bowl-shape surface. (TODO: the figure shows that this dataset is not a very good one for regression problem.)</p>
<figure>
<img alt="" style="width:100.0%" id="fig:regression:cost" title="cost" src="images/regression/reg_cost.png"><figcaption>Figure 3: Visualise the cost function in linear regression problem</figcaption>
</figure>
<p>Recall from previous chapter that gradient descent works by starting at one point on the surface, and move in the <em>direction</em> of steepest desent at some <em>step size</em>, then gradually approach to a local minimum, hopefully as fast as possible. Let’s use a fixed step size <span class="math inline">\(\alpha\)</span>, and the direction at certain point on the surface can be obtained by using partial derivative on the surface. Therefore, what we need to do is to apply this update process iteratively for both <span class="math inline">\(\theta\)</span> parameters: <span id="eq:regression:eq03"><span class="math display">\[ \theta_j \leftarrow \theta_j - \alpha~\frac{\partial}{\partial \theta_j}~J(\theta_0, \theta_1), \qquad(4)\]</span></span> where <span class="math inline">\(i\)</span> is 1 or 2.</p>
<p>This process may seem terrible at first sight, but we can calculate it as:</p>
<p><span id="eq:regression:eq04"><span class="math display">\[ \theta_0 \leftarrow \theta_0 - \frac{\alpha}{n}\sum_{i=1}^{m} (h_{\theta_0, \theta_1}(x_i) - y_i)x_{i0}, \qquad(5)\]</span></span> and <span id="eq:regression:eq05"><span class="math display">\[ \theta_1 \leftarrow \theta_1 - \frac{\alpha}{n}\sum_{i=1}^{m} (h_{\theta_0, \theta_1}(x_i) - y_i)x_{i1}.\qquad(6)\]</span></span></p>
<p>Here the <span class="math inline">\(x_{i0}\)</span> and <span class="math inline">\(x_{i1}\)</span> are just different input features of the <span class="math inline">\(i\)</span>-th row in data. Since currently we only focus on one feature in our problem, <span class="math inline">\(x_i0 = 1\)</span> and <span class="math inline">\(x_i1 = x_i\)</span>. Following these equations, you can manually perform the gradient descent process until it converges.</p>
<div class="highlight">
<pre><code class="language-clike">let alpha = 0.01
let theta0 = ref 10.
let theta1 = ref 10.

for i = 0 to 500 do
    let f x = x *. !theta1 +. !theta0 in
    theta0 := !theta0 -. Mat.(map f x - y |&gt; mean') *. alpha;
    theta1 := !theta1 -. Mat.((map f x - y) * x |&gt; mean') *. alpha 
done</code></pre>
</div>
<p>In the code above, we step the step size <span class="math inline">\(\alpha = 0.01\)</span>, and start from a set of initial parameters: <span class="math inline">\(\theta_0 = 10\)</span> and <span class="math inline">\(\theta_1 = 10\)</span>. We then iteratively update the parameters using 500 iterations. Note that instead of manual summation, we ues the vectorised operations with ndarray.</p>
<p>By executing the code, we can get a pair of parameters: <span class="math inline">\(\theta_0 = 5.14\)</span> and <span class="math inline">\(\theta_1 = 0.55\)</span>. To check if they indeed are suitable parameters, we can visualise them against the input data. The resulting figure fig.&nbsp;4 shows a line that aligns with input data.</p>
<figure>
<img alt="" style="width:60.0%" id="fig:regression:reg_gd" title="reg_gd.png" src="images/regression/reg_gd.png"><figcaption>Figure 4: Validate regression result with original dataset</figcaption>
</figure>
<p>Of course, there is no need to use to manually solve a linear regression problem with Owl. It has already provides high-level regression functions for use. For example, <code>ols</code> function in the <code>Regression</code> module uses the ordinary least square method we have introduced to perform linear regression.</p>
<div class="highlight">
<pre><code class="language-clike">val ols : ?i:bool -&gt; arr -&gt; arr -&gt; arr array</code></pre>
</div>
<p>Here the parameter <code>i</code> shows if the constant parameter <span class="math inline">\(\theta_0\)</span> is used or not. By default it is set to <code>false</code>. We can use this function to directly solve the problem:</p>
<div class="highlight">
<pre><code class="language-clike"># let theta = Regression.D.ols ~i:true x y

val theta : Owl_algodiff_primal_ops.D.arr array =
  [|
         C0 
R0 0.588442 
; 
        C0 
R0 4.72381 
|]</code></pre>
</div>
<p>The resulting parameters are similar to what we have get manually. Another approach is from the perspective of function optimisation instead of regression. We can use the gradient descent optimisation method in Owl and apply it directly on the cost function eq.&nbsp;3. As a matter of fact, the regression functions in Owl are mostly implemented using the <code>minimise_weight</code> function from the optimisation module.</p>
</section>
</section>
<section class="level2" id="multiple-regression">
<h2>Multiple Regression</h2>
<p>TODO: <a href="https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings/data">possible real dataset</a></p>
<p>Back to our McDonald’s problem. We have seen how a new store’s profit can be related to the population of it’s surrounding, and we can even predict it given previous data. Now, remember that in the real world, population is not the only input features that affects the store’s profit. Other factors such as existing stores in the area, proximity to retail parks, shopping centres, etc. also play a role. In that case, how can we extend our one-variable linear regression to the case of multiple variables?</p>
<p>The answer is very straight forward. We just use more parameters, so the model becomes:</p>
<p><span id="eq:regression:eq06"><span class="math display">\[h(\theta_0, \theta_1, \theta_2, \theta_3, ...) = \theta_0~ + \theta_1~x_1 + \theta_2~x_2 + \theta_3~x_3 ... \qquad(7)\]</span></span></p>
<p>However, to list all the parameters explicitly is not a good idea, especially when the question requires considering thousands or even more features. Therefore, we use the vectorised format in the model:</p>
<p><span id="eq:regression:eq065"><span class="math display">\[h(\Theta) = \Theta~X^{(i)},\qquad(8)\]</span></span></p>
<p>where <span class="math inline">\(\Theta = [\theta_0, \theta_1, \theta_2, \theta_3, ...]\)</span>, and <span class="math inline">\(X^{(i)} = [1, x_1, x_2, x_3, ...]^T\)</span> contains all the features from the <span class="math inline">\(i\)</span>th row in data.</p>
<p>Accordingly, the cost function can be represented as:</p>
<p><span id="eq:regression:eq07"><span class="math display">\[ J(\Theta) = \frac{1}{2n}\sum_{i=1}^{n}(\Theta~X^{(i)} - y^{(i)})^2,\qquad(9)\]</span></span></p>
<p>where <span class="math inline">\(y^{(i)}\)</span> is the output variable value on the <span class="math inline">\(i\)</span>th row of input data.</p>
<p>The derivative and manual gradient descent are left as exercise. Here we only show an example of using the regression function Owl has provided. Similar to the previous problem, we provide some data to this multiple variable problem. Part of the data are listed below:</p>
<div id="tbl:regression:data02">
<table>
<caption>Table 2: Sample of input data: multiple features</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x_2\)</span></th>
<th style="text-align: center;">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1888</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">255000</td>
</tr>
<tr class="even">
<td style="text-align: center;">1604</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">242900</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1962</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">259900</td>
</tr>
<tr class="even">
<td style="text-align: center;">3890</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">573900</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1100</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">249900</td>
</tr>
<tr class="even">
<td style="text-align: center;">1458</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">464500</td>
</tr>
<tr class="odd">
<td style="text-align: center;">2526</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">469000</td>
</tr>
<tr class="even">
<td style="text-align: center;">2200</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">475000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
<td style="text-align: center;">…</td>
</tr>
</tbody>
</table>
</div>
The problem has two different features. Again, by using the <code>ols</code> regression function in Owl, we can easily get the multi-variable linear model.
<div class="highlight">
<pre><code class="language-clike">CODE + result</code></pre>
</div>
<section class="level3" id="feature-normalisation">
<h3>Feature Normalisation</h3>
<p>However, getting a result doesn’t mean the end. Using the multi-variable regression problem as example, we would like to discuss an important issue about regression: feature normalisation.</p>
<p>Let’s look at the multi-variable data again. Apparently, the first feature is magnitude larger than the second feature. That means the model and cost function are dominated by the first feature, and a minor change of this column will have a disproportionally large impact on the model.</p>
<p>To overcome this problem, we hope to pre-process the data before the regression, and normalise every features within about [-1, 1]. This step is also called feature scaling. There are many ways to do this, and one of them is the <em>mean normalisation</em>: for a column of features, calculate its mean, and divided by the difference between the largest value and smallest value, as shown in the code:</p>
<div class="highlight">
<pre><code class="language-clike">CODE: normalisation</code></pre>
</div>
<p>Another benefit of performing data normalisation is that gradient descent can be accelerated. The illustration below shows the point.</p>
<p>IMAGES: two image, one ellipses, one circles. with arrows showing the steps towards center.</p>
<p>Normalisation is not only used in regression, but also may other data analysis and machine learning tasks. For example, in computer vision tasks, an image is represented as an ndarray with three dimension. Each element represents an pixel in the image, with a value between 0 and 255. More often than not, this ndarray needs to be normalised in data pre-processed for the next step processing such as image classification.</p>
</section>
<section class="level3" id="analytical-solution">
<h3><a href="#analytical-solution">Analytical Solution</a></h3>
<p>Before taking a look at some other forms of regression, let’s discuss solution to the linear regression besides gradient descent. It turns out that there is actually one close form solution to linear regression problems:</p>
<p><span id="eq:regression:eq075"><span class="math display">\[\Theta = (X^T~X)^{-1}X^Ty\qquad(10)\]</span></span></p>
<p>Suppose the linear model contains <span class="math inline">\(m\)</span> features, and the input data contains <span class="math inline">\(n\)</span> rows, then here <span class="math inline">\(X\)</span> is a <span class="math inline">\(n\times~(m+1)\)</span> matrix representing the features data, and the output data <span class="math inline">\(y\)</span> is a <span class="math inline">\(n\times~1\)</span> matrix. The reason there is m+1 columns in <span class="math inline">\(X\)</span> is that we need an extra constant feature for each data, and it equals to one for each data point.</p>
<p>TODO: where does this equation come from?</p>
<p>With this method, we don’t need to iterate the solutions again and again until converge. We can just compute the result with one pass with the given input data. This calculation can be efficiently performed in Owl using its Linear Algebra module. Let’s use the dataset from multi-variable regression again and perform the computation.</p>
<div class="highlight">
<pre><code class="language-clike">CODE: close form solution.</code></pre>
</div>
<p>TODO: compare the result with previous GD solution.</p>
<p>Compared to the gradient descent solution, the methods does not require multiple iterations, and you also don’t need to worry about hyper-parameters settings such as the choice of learning rate. On the other hand, however, this approach has its own problems.</p>
<p>When the size of <span class="math inline">\(X\)</span>, or the input data, becomes very large, the computation of large linear algebra operations such as matrix multiplication and inversion could become really slow. Or even worse: your computer might don’t even have enough memory to perform the computation. Compare to it, gradient descent proves to work well even when the dataset is large.</p>
<p>Besides, there could be no solution at all using this method. That’s when the <span class="math inline">\(X^T~X\)</span> matrix is non-invertible, e.g.&nbsp;a singular matrix. That could be caused by multiple reasons. Perhaps some of the features are linear dependent, or that there are many redundant features. Then techniques such as choosing feature or regularisation are required.</p>
<p>Most importantly, there is not always a close-form solution for you to use in other regression or machine learning problems. Gradient descent is a much more general solution.</p>
<p><strong>Method from Linalg module</strong></p>
<p>TODO: place these part correctly.</p>
<p>The code snippet below first generates some random data, then using <code>linreg</code> function to perform a simple linear regression and plots the data as well as the regression line.</p>
<div class="highlight">
<pre><code class="language-ocaml">let generate_data () =
  let x = Mat.uniform 500 1 in
  let p = Mat.uniform 1 1 in
  let y = Mat.(x *@ p + gaussian ~sigma:0.05 500 1) in
  x, y

let t1_sol () =
  let x, y = generate_data () in
  let h = Plot.create "plot_00.png" in
  let a, b = Linalg.D.linreg x y in
  let y' = Mat.(x *$ b +$ a) in
  Plot.scatter ~h x y;
  Plot.plot ~h ~spec:[ RGB (0,255,0) ] x y';
  Plot.output h</code></pre>
</div>
<figure>
<img alt="" style="width:90.0%" id="fig:linear-algebra:plot_00" title="linalg plot 00" src="images/linear-algebra/plot_00.png"><figcaption>Figure 5: An example of using linear regression to fit data</figcaption>
</figure>
</section>
</section>
<section class="level2" id="non-linear-regressions">
<h2>Non-linear regressions</h2>
<p>If only the world is as simple as linear regression. But that’s not to be. A lot of data can follow other patterns than a linear one. For example, checkout the dataset below:</p>
<p>IMAGE, the dataset that follows a convex curve.</p>
<p>You can try to fit a line into these data, but it’s quite likely that the result would be very fitting. And that requires non-linear models.</p>
<p>In this section, we present two common non-linear regressions: the polynomial regression, and exponential regression. We shows how to use them with examples, and won’t go into details of the math. Refer to [reference] for more details.</p>
<p>In polynomial regression, the relationship between the feature <span class="math inline">\(x\)</span> and the output variable is modelled as an nth degree polynomial in the feature <span class="math inline">\(x\)</span>:</p>
<p><span id="eq:regression:eq08"><span class="math display">\[ h(\Theta) = \theta_0 + \theta_1~x + \theta_2~x^2 + \theta_3~x^3 \ldots \qquad(11)\]</span></span></p>
<p>The model for exponential regression takes two parameters:</p>
<p><span id="eq:regression:eq09"><span class="math display">\[ h(\theta_0, \theta_1) = \theta_0~\theta_1^x.\qquad(12)\]</span></span></p>
<p>Owl provides functions to do both form of regressions:</p>
<div class="highlight">
<pre><code class="language-clike">val exponential : ?i:bool -&gt; arr -&gt; arr -&gt; elt * elt * elt

val poly : arr -&gt; arr -&gt; int -&gt; arr</code></pre>
</div>
<p>Let’s look at how to use them in the code. The dataset is the same as in previous figure, contained in the file <a href="Link">data_03.csv</a>.</p>
<div class="highlight">
<pre><code class="language-clike">CODE: Polynomial. We limit that to 3th order. </code></pre>
</div>
<p>The result we get is: … . That gives us the polynomial model <span class="math inline">\(y = x + x^2 + x^3 + \epsilon\)</span>.</p>
<p>The code for exponential regression is similar:</p>
<div class="highlight">
<pre><code class="language-clike">CODE: exponential reg.</code></pre>
</div>
<p>The result we get is … That leads to a model: <span class="math inline">\(y = ab^x + \epsilon\)</span>.</p>
<p>Let’s see show the models works in fitting data:</p>
<p>IMAGE: data scatter point with two curves.</p>
</section>
<section class="level2" id="regularisation">
<h2><a href="#regularisation">Regularisation</a></h2>
<p>Regularisation is an important issue in regression, and is widely used in various regression models. The motivation of using regularisation comes from the problem of <em>over-fitting</em> in regression. In statistics, over-fitting means a model is tuned too closely to a particular set of data and it may fail to predict future observations reliably. Let’ use the polynomial regression as an example.</p>
<p>IMAGE: two graphs with the same data, one is fit to the 2nd order, the other fit to the 4th order.</p>
<p>Apparently, the second model fit too closely with the given data, and you can see that it won’t make a good prediction of future output values.</p>
<p>To reduce the effect of higher order parameters, we can penalize these parameters in the cost function. We design the cost function so that the large parameter values leads to higher cost, and therefore by minimising the cost function we keep the parameters relatively small. Actually we don’t need to change the cost functions dramatically. All we need is to add some extra bit at the end, for example, we can do this:</p>
<p><span id="eq:regression:eq10"><span class="math display">\[J(\Theta)=\frac{1}{2n}\left[ \sum_{i=1}{n}(h_{\Theta}(x^{(i)} - y^{(i)}))^2 + \lambda\sum_{j=1}^{m}\theta_j^2 \right].\qquad(13)\]</span></span></p>
<p>Here the sum of squared parameter values is the penalty we add to the original cost function, and <span class="math inline">\(lambda\)</span> is a regularisation control parameter.</p>
<p>That leads to a bit of change in the derivative of <span class="math inline">\(J(\Theta)\)</span> in using gradient descent:</p>
<p><span id="eq:regression:eq11"><span class="math display">\[\theta_j \leftarrow \theta_j - \frac{\alpha}{n} \left[ \sum_{i=1}^{m} (h_{\Theta}(x_i) - y_i)x_{i}^{(j)} - \lambda~\theta_j \right].\qquad(14)\]</span></span></p>
<p>We can now apply the new update procedure in gradient descent code, with a polynomial model up to 4th order.</p>
<div class="highlight">
<pre><code class="language-clike">CODE and IMAGE (data, old overfitted line; new regularised line)
However, it is quite likely that we need to use a multiple variable regression as example, 
since I'm not sure if the functions in the next sections supports polynomial regression;
that requires the overfitting based on these multi-variable data be obvious.</code></pre>
</div>
<p>We can see that by using regularisation the over-fitting problem is solved. Note that we use linear regression in the equation, but regularisation is widely use in all kinds of regressions.</p>
<section class="level3" id="ols-ridge-lasso-and-elastic_net">
<h3>Ols, Ridge, Lasso, and Elastic_net</h3>
<p>You might notice that Owl provides a series of functions other than <code>ols</code>:</p>
<div class="highlight">
<pre><code class="language-clike">val ridge : ?i:bool -&gt; ?alpha:float -&gt; arr -&gt; arr -&gt; arr array

val lasso : ?i:bool -&gt; ?alpha:float -&gt; arr -&gt; arr -&gt; arr array

val elastic_net : ?i:bool -&gt; ?alpha:float -&gt; ?l1_ratio:float -&gt; arr -&gt; arr -&gt; arr array</code></pre>
</div>
<p>What are these functions? The short answer is that: they are for regularisation in regression using different methods. The <code>ridge</code> cost function adds the L2 norm of <span class="math inline">\(\theta\)</span> as the penalty term: <span class="math inline">\(\lambda\sum\theta^2\)</span>, which is what we have introduced. The <code>lasso</code> cost function is similar. It add the L1 norm, or absolute value of the parameter as penalty: <span class="math inline">\(\lambda\sum|\theta|\)</span>. This difference makes <code>lasso</code> to be able to allow for some coefficients to be zero, which is very useful for feature selection. The <code>elastic_net</code> is proposed (by whom?) to combine the penalties of the previous two. What it adds is this: <span id="eq:regression:eq115"><span class="math display">\[\lambda(\frac{1-a}{2}\sum\theta^2 + a\sum|\theta|),\qquad(15)\]</span></span> where <span class="math inline">\(a\)</span> is a control parameter between <code>ridge</code> and <code>lasso</code>. The elastic net method aims to make the feature selection less dependent on input data.</p>
<p>We can thus choose one of these functions to perform regression with regularisation on the dataset in the previous chapter.</p>
<div class="highlight">
<pre><code class="language-clike">CODE using ridge. </code></pre>
</div>
</section>
</section>
<section class="level2" id="logistic-regression">
<h2>Logistic Regression</h2>
<p>So far we have been predicting a value for our problems, whether using linear, polynomial or exponential regression. What if we don’t care about is not the value, but a classification? For example, we have some historical medical data, and want to decide if a tumour is cancer or not based on several features.</p>
<p><strong>liang: logicstic regression is for categorical data analysis … meaning your output var is categorical … the point is to figure out the decision boundry, restructure your language</strong></p>
<p>We can try to continue using linear regression, and the model can be interpreted as the possibility of one of these result. But one problem is that, the prediction value could well be out of the bounds of [0, 1]. Then maybe we need some way to normalise the result to this range?</p>
<section class="level3" id="sigmoid-function">
<h3>Sigmoid Function</h3>
<p>The solution is to use the sigmoid function (or logistic function): <span class="math inline">\(f(x) = \frac{1}{1 + e^{-x}}\)</span>.</p>
<p>As shown in the figure, this function project value within the range of [0, 1]. Applying this function on the returned value of a regression, we can get a model returns value within [0, 1].</p>
<p><span id="eq:regression:eq12"><span class="math display">\[h(\Theta) = f(\Theta~X) = \frac{1}{1 + e^{-\Theta~x}}.\qquad(16)\]</span></span></p>
<p>Now we can interpret this model easily. The function value can be seen as possibility. If it is larger than 0.5, then the classification result is 0, otherwise it returns 1. Remember that in logistic regression we only care about the classification. So for a 2-class classification, returning 0 and 1 is enough.</p>
</section>
<section class="level3" id="decision-boundary">
<h3>Decision Boundary</h3>
<p>The physical meaning of classification is to draw a decision boundary in a hyperplane. For example, if we are using a linear model <span class="math inline">\(h\)</span> within the logistic function, the linear model itself divide the points into two halves in the plane, as shown in the figure.</p>
<p>IMAGE</p>
<p>If we use a non-linear polynomial model, then the plane is divided by curve lines. Suppose <span class="math inline">\(h(x) = \theta_0 + \theta_1~x + \theta_2~x^2\)</span>. According to the property of sigmoid function, “y=1 if g(h(x)) &gt; 0.5” equals to “y=1 if h(x)&gt;0”, and thus the classification is divided by a circle:</p>
<p>IMAGE</p>
<p>Logistic regression uses the linear model as kernel. If you believe your data won’t be linearly separable, or you need to be more robust to outliers, you should look at SVM (see sections below) and look at one of the non-linear kernels.</p>
</section>
<section class="level3" id="cost-function-1">
<h3>Cost Function</h3>
<p>With the new model comes new cost function. Previously in linear regression we measure the cost with least square, or euclidean distance. Now in the logistic regression, we define its cost function as:</p>
<p><span id="eq:regression:eq13"><span class="math display">\[J_{\Theta}(h(x), y) = -log(h(x)), \textrm{if} y = 1, \qquad(17)\]</span></span> or <span id="eq:regression:eq14"><span class="math display">\[J_{\Theta}(h(x), y) = -log(1 - h(x)), \textrm{if} y = 0.\qquad(18)\]</span></span></p>
<p>TODO: explain how to come up with this equation. About maximise the log likelihood. Refer to book scratch.</p>
</section>
<section class="level3" id="gradient-descent">
<h3>Gradient Descent</h3>
<p>Again the question is how to solve this terrible equation? Luckily, The sigmoid function has a nice property: its derivative is simple.</p>
<p><span id="eq:regression:eq15"><span class="math display">\[\frac{\partial J(\Theta)}{\partial \theta_j} = \frac{1}{2n}\sum_{i=1}^{n}(\Theta~X^{(i)} - y^{(i)})^2\qquad(19)\]</span></span></p>
<p>This gradient looks the same to that in linear regression, but it’s actually different, since the definition of <span class="math inline">\(h\)</span> is actually different. Therefore, similar to linear regression, we only need to repeat this gradient descent step until converges. The process is similar to that in linear regression so we will not dig into details again. Instead, we will use the function that Owl provides:</p>
<div class="highlight">
<pre><code class="language-clike">val logistic : ?i:bool -&gt; arr -&gt; arr -&gt; arr array</code></pre>
</div>
<p>We have prepared some data in <a href="Link">data_04.csv</a>. We can perform the regression with these data.</p>
<div class="highlight">
<pre><code class="language-clike">CODE: logistic regression; using polynomial kernel.</code></pre>
</div>
<p>IMAGE: plot the data and resulting boundary.</p>
</section>
<section class="level3" id="multi-class-classification">
<h3>Multi-class classification</h3>
<p>Similar to the LR problem, we can hardly stop at only two parameters. What if we need to classified an object into one fo multiple classes?</p>
<p>One popular classification problem is the hand-written recognition task. It requires the model to recognise a 28x28 grey scale image, representing a hand-written number, to be one of ten numbers, from 0 to 9. It is a widely used ABC task for Neural Networks, and we will also cover it later in Chapter DNN. For now, we solve that from the logistic regression line of thought.</p>
<p>TODO: Dataset description and Visualise</p>
<p>Similarly, we extend the cost function towards multi-class:</p>
<p>EQUATION</p>
<p>We can also use the generalised version of GD as before, or directly apply GD method in Owl:</p>
<div class="highlight">
<pre><code class="language-clike">CODE</code></pre>
</div>
<p>Let’s apply the model on test data:</p>
<p>Result.</p>
<p>Discussion on accuracy and possible improvement. Leave for exercise.</p>
</section>
</section>
<section class="level2" id="support-vector-machine">
<h2>Support Vector Machine</h2>
<p>Support Vector Machine (SVM) is a similar model to logistic regression, but uses non-linear kernel functions. (TODO: explain kernel). SVMs are supervised learning models with associated learning algorithms that analyse data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall. (COPY alert)</p>
<p>Explain the history and basic idea about SVM.</p>
<p>TODO: Apply the SVM to the previous problem, with multiple choices of kernel, and then plot the result.</p>
</section>
<section class="level2" id="model-error-and-selection">
<h2>Model error and selection</h2>
<section class="level3" id="error-metrics">
<h3>Error Metrics</h3>
<p>We have introduced using the least square as a target in minimising the distance between model and data, but it is by no means the only way to assess how good a model is. In this section, we discuss several error metrics for assessing the quality of a model and comparing different models. In testing a model, for each data point, the its real value <span class="math inline">\(y\)</span> and predicted value <span class="math inline">\(y'\)</span>. The difference between these two are called <strong>residual</strong>. In this section, when we say error, we actually mean residual, and do not confuse it with the <span class="math inline">\(\epsilon\)</span> item in the linear model. The latter is the deviation of the observed value from the unobservable true value, and residual means the difference between the observed value and the predicted value.</p>
<p>First, let’s look at two most commonly used metrics:</p>
<ul>
<li><strong>Mean absolute error</strong> (MAE): average absolute value fo residuals, represented by: <span class="math inline">\(\textrm{MAE}=\frac{1}{n}\sum|y - y'|\)</span>.</li>
<li><strong>Mean square error</strong> (MSE): average squared residuals, represented as: <span class="math inline">\(\textrm{MSE}=\frac{1}{n}\sum(y-y')^2\)</span>. This is the method we have previous used in linear regression in this chapter. The part before applying average is called <strong>Residual Sum of Squares</strong> (RSS): <span class="math inline">\(\textrm{RSS}=\sum(y-y')^2\)</span>.</li>
</ul>
<p>The difference between using absolute value and squared value means different sensitivity to outliers. Using the squared residual value, MSE grows quadratically with error. As a result, the outliers are taken into consideration in the regression so as to minimise MSE. On the other hand, by using the absolute error, in MAE each residual contribute proportionally to the metric, and thus the outliers do not have especially large impact on the model fitting. How to choose one of these metrics depends on how you want to treat the outliers in data.</p>
<p>Based on the these two basic metrics, we can derive the definition of other metrics:</p>
<ul>
<li><p><strong>Root mean squared error</strong> (RMSE): it is just the square root of MSE. By applying square root, the unit of error is back to normal and thus easier to interpret. Besides, this metric is similar to the standard deviation and denotes how wide the residuals spread out.</p></li>
<li><p><strong>Mean absolute percentage error</strong> (MAPE): based on MAE, MAPE changes it into percentage representation: <span class="math inline">\(\textrm{MAPE}=\frac{1}{n}\sum |\frac{y - y'}{y}|\)</span>. It denotes the average distance between a model’s predictions and their corresponding outputs in percentage format, for easier interpretation.</p></li>
<li><p><strong>Mean percentage error</strong> (MPE): similar to MAPE, but does not use the absolute value: <span class="math inline">\(\textrm{MPE}=\frac{1}{n}\sum\left(\frac{y - y'}{y} \right)\)</span>. Without the absolute value, the metric can represent it the predict value is larger or smaller than the observed value in data. So unlike MAE and MSE, it’s a relative measurement of error.</p></li>
</ul>
</section>
<section class="level3" id="model-selection">
<h3>Model Selection</h3>
<p>We have already mentioned the issue of feature selection in <a href="#analytical-solution">previous sections</a>. It is common to see that in a multiple regression model, many variables are used in the data and modelling, but only a part of them are actually useful. For example, we can consider the weather factor, such as precipitation quantity, in choosing the location of McDonald’s store, but I suspect its contribution would be marginal at best. By removing these redundant features, we can make a model clearer and increase its interpretability. <a href="#regularisation">Regularisation</a> is one way to downplay these features, and in this section we briefly introduce another commonly used technique: <em>feature selection</em>.</p>
<p>The basic idea of feature selection is simple: choose features from all the possible combinations, test the performance of each model using metric such as RSS. Then choose the best one from them. To put it into detail, suppose we have <span class="math inline">\(n\)</span> features in a multi-variable regression, then for each <span class="math inline">\(i=1, 2, ... n\)</span>, test all the <span class="math inline">\({n\choose i}\)</span> possible models with <span class="math inline">\(i\)</span> variable(s), choose a best one according to its RSS, and we call this model <span class="math inline">\(M_i\)</span>. Once this step is done, we can select the best one from the <span class="math inline">\(n\)</span> models: <span class="math inline">\(M_1, M2, .... M_n\)</span> using <em>certain methods</em>.</p>
<p>You might have already spotted on big problem in this approach: computation complexity. To test all <span class="math inline">\(2^n\)</span> possibilities is a terribly large cost for even medium number of features. Therefore, some computationally efficient approaches are proposed. One of them is the <em>stepwise selection</em>.</p>
<p>The idea of stepwise selection is to build models based on existing best models. We start with one model with zero parameters (always predict the same value regardless of input data) and assume it is the best model. Based on this one, we increase the number of features to one, choose among all the <span class="math inline">\(n\)</span> possible models according to their RSS, name the best one <span class="math inline">\(M_1\)</span>. And based on <span class="math inline">\(M_1\)</span>, we consider adding another feature. Choose among all the <span class="math inline">\(n-1\)</span> possible models according to their RSS, name the best one <span class="math inline">\(M_2\)</span>. So on an so forth. Once we have all the models <span class="math inline">\(M_i, i=1,2,...n\)</span>, we can select the the the best one from them using suitable methods. This process is called “Forward stepwise selection”, and similarly there is also a “Backward stepwise selection”, where you build the model sequence from full features selection <span class="math inline">\(M_n\)</span> down to <span class="math inline">\(M_1\)</span>.</p>
<p>You might notice that we mention using “certain methods” in selecting the best one from these <span class="math inline">\(n\)</span> models. What are these methods? An obvious answer is continue to use RSS etc. as the metric, but the problem is that the model with full features always has the smallest error and then get selected every time. Instead, we need to estimate the test error. We can directly do that using a validation dataset. Otherwise we can make adjustment to the training error such as RSS to include the bias caused by overfitting. Such methods includes: <span class="math inline">\(\textrm{C}_p\)</span>, Akaike information criterion (AIC), Bayesian information criterion (BIC), adjusted <span class="math inline">\(\textrm{R}^2\)</span>, etc. To further dig into these statistical methods is beyond the scope of this book. We recommend specific textbooks such as <span data-cites="james2013introduction" class="citation">(James et al. 2013)</span>.</p>
</section>
</section>
<section class="level2" id="exercise">
<h2>Exercise</h2>
<ol type="1">
<li>Manual gradient descent and optimizer on multiple variable problem</li>
<li>Regularisation of logistic regression could be used as an excise</li>
<li>In regularisation, what would happen if the <span class="math inline">\(\lambda\)</span> is extremely large?</li>
</ol>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
</div>
</section>
</section>
</article></div><a href="neural-network.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 14</small>Deep Neural Networks</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>