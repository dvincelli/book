<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Deep Neural Networks - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="deep-neural-networks">
<h1>Deep Neural Networks</h1>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Brain neuron ect.</p>
</section>
<section class="level2" id="perceptron">
<h2>Perceptron</h2>
<p>The origin; why do we connect at all.</p>
<p>Non-linearity</p>
<p>An example code, but do no use the code for now.</p>
<div class="highlight">
<pre><code class="language-clike">let make_network input_shape =
  input input_shape
  |&gt; linear 300 ~act_typ:Activation.Tanh
  |&gt; linear 10 ~act_typ:Activation.(Softmax 1)
  |&gt; get_network</code></pre>
</div>
</section>
<section class="level2" id="yet-another-regression">
<h2>Yet Another Regression</h2>
<p>To some extend, a deep neural netowrk is nothing but a regression problem in a very high-dimensional space. We need to minimise its cost function by utilising higher-order derivatives. Before looking into the actual <code>Neural</code> module, let’s build a small neural network from scratch.</p>
<p>Follow the previous logistic regression. In this section we use build a simple neural network with a hidden layer, and train its parameters. The task is hand-written recognition.</p>
<p>Basically follow ML004</p>
<section class="level3" id="model-representation">
<h3>Model Representation</h3>
<p>In logistic regression we have… now we need to extend it towards multiple classes. Add an internal layer (why).</p>
<figure>
<img alt="" style="width:100.0%" id="fig:neural-network:simple_nn" title="simple_nn" src="images/neural-network/simple_nn.png"><figcaption>Figure 1: Extend logistic regression to neural network with one hidden layer</figcaption>
</figure>
<p>The data we will use is from <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a>. You can use <code>Owl.Dataset.download_all()</code> to download the dataset.</p>
<div class="highlight">
<pre><code class="language-text">let x, _, y = Dataset.load_mnist_train_data_arr () </code></pre>
</div>
<div class="highlight">
<pre><code class="language-text"># let x_shape, y_shape = 
   Dense.Ndarray.S.shape x, Dense.Ndarray.S.shape y

val x_shape : int array = [|60000; 28; 28; 1|]
val y_shape : int array = [|60000; 10|]</code></pre>
</div>
<p>The label is in the one-hot format:</p>
<div class="highlight">
<pre><code class="language-text">val y : Owl_dense_matrix.S.mat =

        C0  C1  C2  C3  C4  C5  C6  C7  C8  C9
    R0   0   0   0   0   0   1   0   0   0   0
    R1   1   0   0   0   0   0   0   0   0   0
    R2   0   0   0   0   1   0   0   0   0   0
       ... ... ... ... ... ... ... ... ... ...</code></pre>
</div>
<p>It shows the first three labels are 5, 0, and 4.</p>
<figure>
<img alt="" style="width:60.0%" id="fig:neural-network:mnist" title="mnist" src="images/regression/mnist.png"><figcaption>Figure 2: Visualise part of MNIST dataset</figcaption>
</figure>
</section>
<section class="level3" id="forward-propagation">
<h3>Forward Propagation</h3>
<p>Specifically we use a hidden layer of size 25, and the output class is 10.</p>
<p>Since we will use derivatives in training parameters, we construct all the computation using the Algorithmic Differentiation module.</p>
<p>The computation is simple. The logistic regression is repeated:</p>
<p><span class="math display">\[h_\Theta(x) = f(f(x~\theta_0)~\theta_1).\]</span></p>
<p>It can be implemented as:</p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.D
module N = Dense.Ndarray.D

let input_size = 28 * 28
let hidden_size = 25
let classes = 10

let theta0 = Arr (N.uniform [|input_size; hidden_size|])
let theta1 = Arr (N.uniform [|hidden_size; classes|])

let h theta0 theta1 x = 
  let t = Arr.dot x theta0 |&gt; Maths.sigmoid in 
  Arr.dot t theta1 |&gt; Maths.sigmoid</code></pre>
</div>
<p>That’s it. We can now classify an input <code>28x28</code> array into one of the ten classes… except that we can’t. Currently we only use random content as the parameters. We need to train the model and find suitable <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> parameters.</p>
</section>
<section class="level3" id="back-propagation">
<h3>Back propagation</h3>
<p>Training a network is essentially a process of minimising the cost function by adjusting the weight of each layer. The core of training is the backpropagation algorithm. As its name suggests, backpropagation algorithm propagates the error from the end of a network back to the input layer, in the reverse direction of evaluating the network. Backpropagation algorithm is especially useful for those functions whose input parameters <code>&gt;&gt;</code> output parameters.</p>
<p>Backpropagation is the core of all neural networks, actually it is just a special case of reverse mode AD. Therefore, we can write up the backpropagation algorithm from scratch easily with the help of <code>Algodiff</code> module.</p>
<p>Recall in the Regression chapter, training parameters is the process is to find the parameters that minimise the cost function of iteratively. In the case of this neural network, its cost function <span class="math inline">\(J\)</span> is similar to that of logistic regression. Suppose we have <span class="math inline">\(m\)</span> training data pairs, then it can be expressed as:</p>
<p><span id="eq:neural-network:costfun"><span class="math display">\[J(\theta+0, \theta_1) = \frac{1}{m}\sum_{i=1}^m(-y^{(i)}log(h_\Theta(x^{(i)}))-(1 -y^{(i)})log(1-h_\Theta(x^{(i)}))).\qquad(1)\]</span></span></p>
<div class="highlight">
<pre><code class="language-ocaml">let j t0 t1 x y =
  let z = h t0 t1 x in 
  Maths.add
    (Maths.cross_entropy y z)
    (Maths.cross_entropy Arr.(sub (ones (shape y)) y)
       Arr.(sub (ones (shape z)) z))</code></pre>
</div>
<p>Here the the <code>cross_entropy y x</code> means <span class="math inline">\(-h~\log(x)\)</span>.</p>
<p>In the regression chapter, to find the suitable parameters that minimise <span class="math inline">\(J\)</span>, we iteratively apply:</p>
<p><span class="math display">\[ \theta_j \leftarrow \theta_j - \alpha~\frac{\partial}{\partial \theta_j}~J(\theta_0, \theta_1)\]</span></p>
<p>until it converges. The same also applies here. But the partial derivative is not intuitive to give a analytical solution. But actually we don’t have to now that we are using the AD module. The partial derivatives of both parameters can be correctly calculated. We have show in the Algorithmic Differentiation chapter how it can be done in Owl:</p>
<div class="highlight">
<pre><code class="language-text">let x', y' = Dataset.draw_samples x y 1
let cost = j t0 t1 (Arr x') (Arr y')
let _ = reverse_prop (F 1.) cost
let theta0' =  adjval t0 |&gt; unpack_arr
let theta1' =  adjval t1 |&gt; unpack_arr</code></pre>
</div>
<p>That’s it for one iteration. We get <span class="math inline">\(\frac{\partial}{\partial \theta_j}~J(\theta_0, \theta_1)\)</span>, and then can iteratively update the <span class="math inline">\(\Theta\)</span> parameters.</p>
<p>TODO: finish this example with accuracy value.</p>
</section>
</section>
<section class="level2" id="feed-forward-network">
<h2>Feed Forward Network</h2>
<p>In the next step, we revise the previous example, with a bit of more details added.</p>
<p>First, the previous example mixes all the computation together. We need to add the abstraction of <em>layers</em> (Explain). The following code defines the layer and network type, both are OCaml record types.</p>
<p>Also note that for each layer, besides the matrix multiplication, we also added an extra <em>bias</em> (Explain). Each linear layer performs the following calculation where <span class="math inline">\(a\)</span> is a non-linear activation function.</p>
<p><span class="math display">\[ y = a(x \times w + b) \]</span></p>
<p>Each layer consists of three components: weight <code>w</code>, bias <code>b</code>, and activation function <code>a</code>. A network is just a collection of layers.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.S

type layer = {
  mutable w : t;
  mutable b : t;
  mutable a : t -&gt; t;
}

type network = { layers : layer array }</code></pre>
</div>
<p>Despite of the complicated internal structure, we can treat a neural network as a function, which is fed with input data and outputs predictions. The question is how to evaluate a network. Evaluating a network can be decomposed as a sequence of evaluation of layers.</p>
<p>The output of one layer will feed into the next layer as its input, moving forward until it reaches the end. The following two lines shows how to evaluate a neural network in <em>forward mode</em>.</p>
<div class="highlight">
<pre><code class="language-ocaml">let run_layer x l = Maths.((x *@ l.w) + l.b) |&gt; l.a

let run_network x nn = Array.fold_left run_layer x nn.layers</code></pre>
</div>
<p>The <code>run_network</code> can generate what equals to the <span class="math inline">\(h_\Theta(x)\)</span> function in previous section.</p>
<p>Here we note there is an extra function <code>a</code>. It is the activation function. (Explain activation function and why they are necessary.) Previously we use the <code>sigmoid</code> function, but that’s not the only option. We can also use <code>tanh</code> and <code>softmax</code>.</p>
<p>In this small example, we will only use two layers, <code>l0</code> and <code>l1</code>. <code>l0</code> uses a <code>784 x 40</code> matrix as weight, and <code>tanh</code> as activation function. <code>l1</code> is the output layer and <code>softmax</code> is the cost function.</p>
<div class="highlight">
<pre><code class="language-ocaml">let l0 = {
  w = Maths.(Mat.uniform 784 40 * F 0.15 - F 0.075);
  b = Mat.zeros 1 300;
  a = Maths.tanh;
}

let l1 = {
  w = Maths.(Mat.uniform 40 10 * F 0.15 - F 0.075);
  b = Mat.zeros 1 10;
  a = Maths.softmax ~axis:1;
}

let nn = {layers = [|l0; l1|]}</code></pre>
</div>
<p>This definition is plain, but there is still one thing to say. <em>Initialisation</em>: previously we use a uniformly random array, but choosing a good initial status is important. Explain. Here we use…</p>
<p><strong>Training</strong></p>
<p>The loss function is constructed in the same way.</p>
<div class="highlight">
<pre><code class="language-ocaml">let loss_fun nn x y = 
  let t = tag () in
  Array.iter (fun l -&gt;
    l.w &lt;- make_reverse l.w t;
    l.b &lt;- make_reverse l.b t;
  ) nn.layers;
  Maths.(cross_entropy y (run_network x nn) / (F (Mat.row_num y |&gt; float_of_int)))</code></pre>
</div>
<p>(Explain why we use only one <code>cross_entropy</code>)</p>
<div class="highlight">
<pre><code class="language-ocaml">let backprop nn eta x y =
  let loss = loss_fun nn x y in 
  reverse_prop (F 1.) loss;
  Array.iter (fun l -&gt;
    l.w &lt;- Maths.((primal l.w) - (eta * (adjval l.w))) |&gt; primal;
    l.b &lt;- Maths.((primal l.b) - (eta * (adjval l.b))) |&gt; primal;
  ) nn.layers;
  loss |&gt; unpack_flt</code></pre>
</div>
<p>The <code>backprop</code> also uses the same procedure as previous example. The partial derivative is gotten using <code>adjval</code>, and the parameter <code>w</code> and <code>b</code> of each layer are updated accordingly. It then uses the gradient descent method we introduced in previous example. The learning rate <code>eta</code> is fixed.</p>
<p><strong>Test</strong></p>
<p>We need to see how well our trained model works. The <code>test</code> function performs model inference and compares the predictions with the labelled data. By doing so, we can evaluate the accuracy of a neural network.</p>
<div class="highlight">
<pre><code class="language-ocaml">let test nn x y =
  Dense.Matrix.S.iter2_rows (fun u v -&gt;
    let p = run_network (Arr u) nn |&gt; unpack_arr in
    Dense.Matrix.Generic.print p;
    Printf.printf "prediction: %i\n" (let _, i = Dense.Matrix.Generic.max_i p in i.(1))
  ) (unpack_arr x) (unpack_arr y)</code></pre>
</div>
<p>Finally, we put all the previous parts together. The following code starts the training for 999 iterations.</p>
<div class="highlight">
<pre><code class="language-ocaml">let main () =
  let x, _, y = Dataset.load_mnist_train_data () in
  for i = 1 to 999 do
    let x', y' = Dataset.draw_samples x y 100 in
    backprop nn (F 0.01) (Arr x') (Arr y')
    |&gt; Owl_log.info "#%03i : loss = %g" i
  done;
  let x, y, _ = Dataset.load_mnist_test_data () in
  let x, y = Dataset.draw_samples x y 10 in
  test nn (Arr x) (Arr y)</code></pre>
</div>
<p>When the training starts, our application keeps printing the value of loss function in the end of each iteration. From the output, we can see the value of loss function keeps decreasing quickly after training starts.</p>
<div class="highlight">
<pre><code class="language-text">2019-11-12 01:04:14.632 INFO : #001 : loss = 2.54432
2019-11-12 01:04:14.645 INFO : #002 : loss = 2.48446
2019-11-12 01:04:14.684 INFO : #003 : loss = 2.33889
2019-11-12 01:04:14.696 INFO : #004 : loss = 2.28728
2019-11-12 01:04:14.709 INFO : #005 : loss = 2.23134
2019-11-12 01:04:14.720 INFO : #006 : loss = 2.21974
2019-11-12 01:04:14.730 INFO : #007 : loss = 2.0249
2019-11-12 01:04:14.740 INFO : #008 : loss = 1.96638</code></pre>
</div>
<p>After training finished, we test the accuracy of the network. Here is one example where we input hand-written 3. The vector below shows the prediction, we see the model says with <span class="math inline">\(90.14%\)</span> chance it is a number 3. Quite accurate!</p>
<figure>
<img alt="" style="width:40.0%" id="fig:neural-network:plot01" title="plot_01" src="images/neural-network/plot_01.png"><figcaption>Figure 3: Prediction from the model</figcaption>
</figure>
<p>TODO: replace with code.</p>
</section>
<section class="level2" id="neural-network-module">
<h2>Neural Network Module</h2>
<p>More layers, but you can find that previous approach is hard to scale.</p>
<p>The <code>Neural</code> module is actually very similar to the naive framework we just built, but with more compete support to various neurons.</p>
<p>Owl is designed as a general-purpose numerical library, and I never planned to make it yet another framework for deep neural networks. The original motivation of including such a neural network module was simply for demo purpose, since in almost every presentation I had been to, there were always the same question from audience: <em>“can owl do deep neural network by the way?”</em></p>
<p>In the end, we became curious about this question myself, although the perspective was slightly different. I was very sure I could implement a proper neural network framework atop of Owl, but I didn’t know how easy it is. I think it is an excellent opportunity to test Owl’s capability and expressiveness in developing complicated analytical applications.</p>
<p>The outcome is wonderful. It turns out with Owl’s architecture and its internal functionality (Algodiff, CGraph, etc.), combined with OCaml’s powerful module system, implementing a full featured neural network module only requires approximately 3500 LOC. Yes, you heard me, 3500 LOC, and it beats TensorFlow’s performance on CPU (by the time we measured in 2018).</p>
<p>In this section we talk about the deesign of NN module</p>
<section class="level3" id="module-structure">
<h3>Module Structure</h3>
<p>The <a href="https://github.com/ryanrhymes/owl/blob/master/lib/neural/owl_neural.ml">Owl.Neural</a> provides two submodules <code>S</code> and <code>D</code> for both single precision and double precision neural networks. In each submodule, it contains the following modules to allow you to work with the structure of the network and fine-tune the training.</p>
<ul>
<li><code>Graph</code> : create and manipulate the neural network structure.</li>
<li><code>Init</code> : control the initialisation of the weights in the network.</li>
<li><code>Activation</code> : provide a set of frequently used activation functions.</li>
<li><code>Params</code> : maintains a set of training parameters.</li>
<li><code>Batch</code> : the batch parameter of training.</li>
<li><code>Learning_Rate</code> : the learning rate parameter of training.</li>
<li><code>Loss</code> : the loss function parameter of training.</li>
<li><code>Gradient</code> : the gradient method parameter of training.</li>
<li><code>Momentum</code> : the momentum parameter of training.</li>
<li><code>Regularisation</code> : the regularisation parameter of training.</li>
<li><code>Clipping</code> : the gradient clipping parameter of training.</li>
<li><code>Checkpoint</code> : the checkpoint parameter of training.</li>
<li><code>Parallel</code> : provide parallel computation capability, need to compose with Actor engine. (Experimental, a research project in progress.)</li>
</ul>
</section>
<section class="level3" id="model-definition">
<h3>Model Definition</h3>
<p>I have implemented a set of commonly used neurons in <a href="https://github.com/ryanrhymes/owl/blob/master/lib/neural/owl_neural_neuron.ml">Owl.Neural.Neuron</a>. Each neuron is a standalone module and adding a new type of neuron is much easier than adding a new one in Tensorflow or other framework thanks to Owl’s <a href="https://github.com/ryanrhymes/owl/blob/master/lib/owl_algodiff_generic.mli">Algodiff</a> module.</p>
<p><code>Algodiff</code> is the most powerful part of Owl and offers great benefits to the modules built atop of it. In neural network case, we only need to describe the logic of the forward pass without worrying about the backward propagation at all, because the <code>Algodiff</code> figures it out automatically for us thus reduces the potential errors. This explains why a full-featured neural network module only requires less than 3.5k lines of code. Actually, if you are really interested, you can have a look at Owl’s <a href="https://github.com/ryanrhymes/owl/blob/master/examples/feedforward.ml">Feedforward Network</a> which only uses a couple of hundreds lines of code to implement a complete Feedforward network.</p>
<p>In practice, you do not need to use the modules defined in <a href="https://github.com/ryanrhymes/owl/blob/master/lib/neural/owl_neural_neuron.ml">Owl.Neural.Neuron</a> directly. Instead, you should call the functions in <a href="https://github.com/ryanrhymes/owl/blob/master/lib/neural/owl_neural_graph.ml">Graph</a> module to create a new neuron and add it to the network. Currently, Graph module contains the following neurons.</p>
<p><code>input</code>, <code>activation</code>, <code>linear</code>, <code>linear_nobias</code>, <code>embedding</code>, <code>recurrent</code>, <code>lstm</code>, <code>gru</code>, <code>conv1d</code>, <code>conv2d</code>, <code>conv3d</code>, <code>max_pool1d</code>, <code>max_pool2d</code>, <code>avg_pool1d</code>, <code>avg_pool2d</code>, <code>global_max_pool1d</code>, <code>global_max_pool2d</code>, <code>global_avg_pool1d</code>, <code>global_avg_pool2d</code>, <code>fully_connected</code>, <code>dropout</code>, <code>gaussian_noise</code>, <code>gaussian_dropout</code>, <code>alpha_dropout</code>, <code>normalisation</code>, <code>reshape</code>, <code>flatten</code>, <code>lambda</code>, <code>add</code>, <code>mul</code>, <code>dot</code>, <code>max</code>, <code>average</code>, <code>concatenate</code></p>
<p>These neurons should be sufficient for creating from simple MLP to the most complicated Google’s Inception network.</p>
</section>
<section class="level3" id="model-training">
<h3>Model Training</h3>
<p>Owl provides a very functional way to construct a neural network. You only need to provide the shape of the date in the first node (often <code>input</code> neuron), then Owl will automatically infer the shape for you in the downstream nodes which saves us a lot of efforts and significantly reduces the potential bugs.</p>
<p>Let’s use the single precision neural network as an example. To work with single precision networks, you need to use/open the following modules</p>
<div class="highlight">
<pre><code class="language-ocaml">
  open Owl
  open Neural.S
  open Neural.S.Graph
  open Neural.S.Algodiff
</code></pre>
</div>
<p>The code below creates a small convolutional neural network of six layers. Usually, the network definition always starts with <code>input</code> neuron and ends with <code>get_network</code> function which finalises and returns the constructed network. We can also see the input shape is reserved as a passed in parameter so the shape of the data and the parameters will be inferred later whenever the <code>input_shape</code> is determined.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let make_network input_shape =
    input input_shape
    |&gt; lambda (fun x -&gt; Maths.(x / F 256.))
    |&gt; conv2d [|5;5;1;32|] [|1;1|] ~act_typ:Activation.Relu
    |&gt; max_pool2d [|2;2|] [|2;2|]
    |&gt; dropout 0.1
    |&gt; fully_connected 1024 ~act_typ:Activation.Relu
    |&gt; linear 10 ~act_typ:Activation.(Softmax 1)
    |&gt; get_network
</code></pre>
</div>
<p>Next, I will show you how the <code>train</code> function looks like. The first three lines in the <code>train</code> function is for loading the <code>MNIST</code> dataset and print out the network structure on the terminal. The rest lines defines a <code>params</code> which contains the training parameters such as batch size, learning rate, number of epochs to run. In the end, we call <code>Graph.train</code> to kick off the training process.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let train () =
    let x, _, y = Dataset.load_mnist_train_data_arr () in
    let network = make_network [|28;28;1|] in
    Graph.print network;

    let params = Params.config
      ~batch:(Batch.Mini 100) ~learning_rate:(Learning_Rate.Adagrad 0.005) 2.
    in
    Graph.train ~params network x y |&gt; ignore
</code></pre>
</div>
<p>After the training is finished, you can call <code>Graph.model</code> to generate a functional model to perform inference. Moreover, <code>Graph</code> module also provides functions such as <code>save</code>, <code>load</code>, <code>print</code>, <code>to_string</code> and so on to help you in manipulating the neural network.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let predict network data =
    let model = Graph.model network in
    let predication = model data in
    predication
</code></pre>
</div>
<p>You can have a look at Owl’s <a href="https://github.com/ryanrhymes/owl/blob/master/examples/mnist_cnn.ml">MNIST CNN example</a> for more details and run the code by yourself.</p>
</section>
<section class="level3" id="model-inference">
<h3>Model Inference</h3>
<p>TBD</p>
</section>
</section>
<section class="level2" id="convolution-neural-network">
<h2>Convolution Neural Network</h2>
<p>Introduce CNN</p>
<p>More about the structure in NN module/Optimise module</p>
<p>Implement the same MNIST task with CNN.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let make_network input_shape =
    input input_shape
    |&gt; lambda (fun x -&gt; Maths.(x / F 256.))
    |&gt; conv2d [|5;5;1;32|] [|1;1|] ~act_typ:Activation.Relu
    |&gt; max_pool2d [|2;2|] [|2;2|]
    |&gt; dropout 0.1
    |&gt; fully_connected 1024 ~act_typ:Activation.Relu
    |&gt; linear 10 ~act_typ:Activation.(Softmax 1)
    |&gt; get_network
</code></pre>
</div>
<p><strong>Applications</strong></p>
<p>For more applications, please check the image recognition, NST, and instance segmentation cases.</p>
</section>
<section class="level2" id="recurrent-neural-network">
<h2>Recurrent Neural Network</h2>
<p>Include GRU</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let make_network wndsz vocabsz =
    input [|wndsz|]
    |&gt; embedding vocabsz 40
    |&gt; lstm 128
    |&gt; linear 512 ~act_typ:Activation.Relu
    |&gt; linear vocabsz ~act_typ:Activation.(Softmax 1)
    |&gt; get_network
</code></pre>
</div>
<p>The generated computation graph is way more complicated due to LSTM’s internal recurrent structure. You can download the <a href="https://raw.githubusercontent.com/wiki/ryanrhymes/owl/image/plot_030.pdf">PDF file 1</a> for better image quality.</p>
</section>
<section class="level2" id="long-short-term-memory">
<h2>Long Short Term Memory</h2>
</section>
<section class="level2" id="generative-adversarial-network">
<h2>Generative adversarial network</h2>
</section>
<section class="level2" id="summary">
<h2>Summary</h2>
</section>
</section>
</article></div><a href="nlp.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 15</small>Natural Language Processing</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>