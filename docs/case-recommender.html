<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Case - Recommender System - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="case---recommender-system">
<h1>Case - Recommender System</h1>
<p>Refer to <span data-cites="7462177" class="citation">(Wang et al. 2016)</span> <span data-cites="7840682" class="citation">(Hyvönen et al. 2016)</span></p>
<section class="level2" id="arch">
<h2>Architecture</h2>
<p>At the core, Kvasir implements an LSA-based index and search service, and its architecture can be divided into two subsystems as  and . Figure  illustrates the general workflow and internal design of the system. The frontend is currently implemented as a lightweight extension in Chrome browser. The browser extension only sends the page URL back to the KServer whenever a new tab/window is created. The KServer running at the backend retrieves the content of the given URL then responds with the most relevant documents in a database. The results are formatted into JSON strings. The extension presents the results in a friendly way on the page being browsed. From user perspective, a user only interacts with the frontend by checking the list of recommendations that may interest him.</p>
<p>To connect to the frontend, the backend exposes one simple  as below, which gives great flexibility to all possible frontend implementations. By loosely coupling with the backend, it becomes easy to mash-up new services on top of Kvasir. Line 1 and 2 give an example request to Kvasir service.  indicates that  contains a URL, otherwise  contains a piece of text if . Line 4-9 present an example response from the server, which contains the metainfo of a list of similar articles. Note that the frontend can refine or rearrange the results based on the metainfo (e.g., similarity or timestamp).</p>
<div class="highlight">
<pre><code class="language-json">POST
https://api.kvasir/query?type=0&amp;info=url

{"results": [
  {"title": document title,
   "similarity": similarity metric,
   "page_url": link to the document,
   "timestamp": document create date}
]}</code></pre>
</div>
<p>The backend system implements indexing and searching functionality which consist of five components: Crawler, Cleaner, DLSA, PANNS and KServer. Three components (i.e., Cleaner, DLSA and PANNS) are wrapped into one library since all are implemented on top of Apache Spark. The library covers three phases as text cleaning, database building, and indexing. We briefly present the main tasks in each component as below.</p>
<p> collects raw documents from the Web then compiles them into two data sets. One is the English Wikipedia dump, and another is compiled from over 300 news feeds of the high-quality content providers such as BBC, Guardian, Times, Yahoo News, MSNBC, and etc. Table  summarizes the basic statistics of the data sets. Multiple instances of the Crawler run in parallel on different machines. Simple fault-tolerant mechanisms like periodical backup have been implemented to improve the robustness of crawling process. In addition to the text body, the Crawler also records the timestamp, URL and title of the retrieved news as metainfo, which can be further utilized to refine the search results. %%% The raw text corpus is copied to HDFS periodically to reduce the risk of data loss.</p>
<p> cleans the unstructured text corpus and converts the corpus into term frequency-inverse document frequency (TF-IDF) model. In the preprocessing phase, we clean the text by removing HTML tags and stopwords, deaccenting, tokenization, etc. The dictionary refers to the vocabulary of a language model, its quality directly impacts the model performance. To build the dictionary, we exclude both extremely rare and extremely common terms, and keep <span class="math inline">\(10^5\)</span> most popular ones as . More precisely, a term is considered as rare if it appears in less than 20 documents, while a term is considered as common if it appears in more than 40% of documents.</p>
<p> builds up an LSA-based model from the previously constructed TF-IDF model. Technically, the TF-IDF itself is already a vector space language model. The reason we seldom use TF-IDF directly is because the model contains too much noise and the dimensionality is too high to process efficiently even on a modern computer. To convert a TF-IDF to an LSA model, DLSA’s algebraic operations involve large matrix multiplications and time-consuming SVD. We initially tried to use MLib to implement DLSA. However, MLlib is unable to perform SVD on a data set of <span class="math inline">\(10^5\)</span> features with limited RAM, we have to implement our own stochastic SVD on Apache Spark using rank-revealing technique. Section  discusses DLSA in details.</p>
<p> builds the search index to enable fast <span class="math inline">\(k\)</span>-NN search in high dimensional LSA vector spaces. Though dimensionality has been significantly reduced from TF-IDF (<span class="math inline">\(10^5\)</span> features) to LSA (<span class="math inline">\(10^3\)</span> features), <span class="math inline">\(k\)</span>-NN search in a <span class="math inline">\(10^3\)</span>-dimension space is still a great challenge especially when we try to provide responsive services. Naive linear search using one CPU takes over 6 seconds to finish in a database of 4 million entries, which is unacceptably long for any realistic services. PANNS implements a parallel RP-tree algorithm which makes a reasonable tradeoff between accuracy and efficiency. PANNS is the core component in the backend system and Section  presents its algorithm in details.</p>
<p> runs within a web server, processes the users requests and replies with a list of similar documents. KServer uses the index built by PANNS to perform fast search in the database. The ranking of the search results is based on the cosine similarity metric. A key performance metric for KServer is the service time. We wrapped KServer into a Docker image and deployed multiple KServer instances on different machines to achieve better performance. We also implemented a simple round-robin mechanism to balance the request loads among the multiple KServers.</p>
<p>Kvasir architecture provides a great potential and flexibility for developers to build various interesting applications on different devices, e.g., semantic search engine, intelligent Twitter bots, context-aware content provision, and etc.</p>
</section>
<section class="level2" id="preprocess-data">
<h2>Preprocess Data</h2>
</section>
<section class="level2" id="build-topic-models">
<h2>Build Topic Models</h2>
<p>The vector space model belongs to algebraic language models, where each document is represented with a row vector. Each element in the vector represents the weight of a term in the dictionary calculated in a specific way. E.g., it can be simply calculated as the frequency of a term in a document, or slightly more complicated TF-IDF. The length of the vector is determined by the size of the dictionary (i.e., number of features). A text corpus containing <span class="math inline">\(m\)</span> documents and a dictionary of <span class="math inline">\(n\)</span> terms will be converted to an <span class="math inline">\(A = m \times n\)</span> row-based matrix. Informally, we say that <span class="math inline">\(A\)</span> grows taller if the number of documents (i.e., <span class="math inline">\(m\)</span>) increases, and grows fatter if we add more terms (i.e., <span class="math inline">\(n\)</span>) in the dictionary. LSA utilizes SVD to reduce <span class="math inline">\(n\)</span> by only keeping a small number of linear combinations of the original features. To perform SVD, we need to calculate the covariance matrix <span class="math inline">\(C = A^T \times A\)</span>, which is a <span class="math inline">\(n \times n\)</span> matrix and is usually much smaller than <span class="math inline">\(A\)</span>.</p>
<figure>
<img alt="" id="fig:case-recommender:revealing" title="plot_06" src="images/case-recommender/plot_06.png"><figcaption>Figure 1: Rank-revealing reduces dimensionality to perform in-memory SVD</figcaption>
</figure>
<p>We can easily parallelize the calculation of <span class="math inline">\(C\)</span> by dividing <span class="math inline">\(A\)</span> into <span class="math inline">\(k\)</span> smaller chunks of size <span class="math inline">\([\frac{m}{k}] \times n\)</span>, so that the final result can be obtained by aggregating the partial results as <span class="math inline">\(C = A^T \times A = \sum_{i=1}^{k} A^T_i \times A_i \label{eq:1}\)</span>. However, a more serious problem is posed by the large number of columns, i.e., <span class="math inline">\(n\)</span>. The SVD function in MLlib is only able to handle tall and thin matrices up to some hundreds of features. For most of the language models, there are often hundreds of thousands features (e.g., <span class="math inline">\(10^5\)</span> in our case). The covariance matrix <span class="math inline">\(C\)</span> becomes too big to fit into the physical memory, hence the native SVD operation in MLlib of Spark fails as the first subfigure of Figure fig.&nbsp;1 shows.</p>
<p>In linear algebra, a matrix can be approximated by another matrix of lower rank while still retaining approximately properties of the matrix that are important for the problem at hand. In other words, we can use another thinner matrix <span class="math inline">\(B\)</span> to approximate the original fat <span class="math inline">\(A\)</span>. The corresponding technique is referred to as rank-revealing QR estimation . A TF-IDF model having <span class="math inline">\(10^5\)</span> features often contains a lot of redundant information. Therefore, we can effectively thin the matrix <span class="math inline">\(A\)</span> then fit <span class="math inline">\(C\)</span> into the memory. Figure fig.&nbsp;1 illustrates the algorithmic logic in DLSA, which is essentially a distributed stochastic SVD implementation.</p>
</section>
<section class="level2" id="index-text-corpus">
<h2>Index Text Corpus</h2>
<figure>
<img alt="" id="fig:case-recommender:projection" title="plot_01" src="images/case-recommender/plot_01.png"><figcaption>Figure 2: Projection on different random lines</figcaption>
</figure>
<figure>
<img alt="" id="fig:case-recommender:search" title="plot_01" src="images/case-recommender/plot_02.png"><figcaption>Figure 3: Construct a binary search tree from the reandom projection</figcaption>
</figure>
<p>Figure fig.&nbsp;3 illustrates how binary search can be built.</p>
<p>With an LSA model at hand, finding the most relevant document is equivalent to finding the nearest neighbours for a given point in the derived vector space, which is often referred to as k-NN problem. The distance is usually measured with the cosine similarity of two vectors. However, neither naive linear search nor conventional  tree is capable of performing efficient search in such high dimensional space even though the dimensionality has been significantly reduced from <span class="math inline">\(10^5\)</span> to <span class="math inline">\(10^3\)</span> by LSA.</p>
<p>Nonetheless, we need not locate the exact nearest neighbours in practice. In most cases, slight numerical error (reflected in the language context) is not noticeable at all, i.e., the returned documents still look relevant from the user’s perspective. By sacrificing some accuracy, we can obtain a significant gain in searching speed.</p>
<p>The general idea of RP-tree algorithm used here is clustering the points by partitioning the space into smaller subspaces recursively. Technically, this can be achieved by any tree-based algorithms. Given a tree built from a database, we answer a nearest neighbour query <span class="math inline">\(q\)</span> in an efficient way, by moving <span class="math inline">\(q\)</span> down the tree to its appropriate leaf cell, and then return the nearest neighbour in that cell. However in several cases <span class="math inline">\(q\)</span>’s nearest neighbour may well lie within a different cell.</p>
<p>Figure fig.&nbsp;2 gives a naive example on a 2-dimension vector space. First, a random vector <span class="math inline">\(x\)</span> is drawn and all the points are projected onto <span class="math inline">\(x\)</span>. Then we divide the whole space into half at the mean value of all projections (i.e., the blue circle on <span class="math inline">\(x\)</span>) to reduce the problem size. For each new subspace, we draw another random vector for projection, and this process continues recursively until the number of points in the space reaches the predefined threshold on cluster size. We can construct a binary tree to facilitate the search. As we can see in the first subfigure of Figure fig.&nbsp;2, though the projections of <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> seem close to each other on <span class="math inline">\(x\)</span>, <span class="math inline">\(C\)</span> is actually quite distant from <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>However, it has been shown that such misclassifications become arbitrarily rare as the iterative procedure continues by drawing more random vectors and performing corresponding splits. More precisely, in  the authors show that under the assumption of some intrinsic dimensionality of a subcluster (i.e., nodes of a tree structure), its descendant clusters will have a much smaller diameter, hence can include the points that are expected to be more similar to each other. Herein the diameter is defined as the distance between the furthest pair of data points in a cell. Such an example is given in Figure fig.&nbsp;2, where <span class="math inline">\(y\)</span> successfully separates <span class="math inline">\(C\)</span> from <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>Another kind of misclassification is that two nearby points are unluckily divided into different subspaces, e.g., points <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span> in the left panel of Figure fig.&nbsp;2. To get around this issue, the authors in  proposed a tree structure (i.e., spill tree) where each data point is stored in multiple leaves, by following overlapping splits. Although the query time remains essentially the same, the required space is significantly increased.<br>
In this work we choose to improve the accuracy by building multiple RP-trees. We expect that the randomness in tree construction will introduce extra variability in the neighbours that are returned by several RP-trees for a given query point. This can be taken as an advantage in order to mitigate the second kind of misclassification while searching for the nearest neighbours of a query point in the combined search set. However, in this case one would need to store a large number of random vectors at every node of the tree, introducing significant storage overhead as well. For a corpus of 4 million documents, if we use <span class="math inline">\(10^5\)</span> random vectors (i.e., a cluster size of 20), and each vector is a <span class="math inline">\(10^3\)</span>-dimension real vector (32-bit float number), the induced storage overhead is about 381.5~MB for each RP-tree. Therefore, such a solution leads to a huge index of <span class="math inline">\(47.7\)</span>~GB given <span class="math inline">\(128\)</span> RP-trees are included, or <span class="math inline">\(95.4\)</span>~GB given <span class="math inline">\(256\)</span> RP-trees.</p>
<p>The huge index size not only consumes a significant amount of storage resources, but also prevents the system from scaling up after more and more documents are collected. One possible solution to reduce the index size is reusing the random vectors. Namely, we can generate a pool of random vectors once, then randomly choose one from the pool each time when one is needed. However, the immediate challenge emerges when we try to parallelize the tree building on multiple nodes, because we need to broadcast the pool of vectors onto every node, which causes significant network traffic.</p>
<p>To address this challenge, we propose to use a pseudo random seed in building and storing search index. Instead of maintaining a pool of random vectors, we just need a random seed for each RP-tree. The computation node can build all the random vectors on the fly from the given seed. From the model building perspective, we can easily broadcast several random seeds with negligible traffic overhead instead of a large matrix in the network, therefore we improve the computation efficiency. From the storage perspective, we only need to store one 4-byte random seed for each RP-tree. In such a way, we are able to successfully reduce the storage overhead from <span class="math inline">\(47.7\)</span>~GB to <span class="math inline">\(512\)</span>~B for a search index consisting of <span class="math inline">\(128\)</span> RP-trees (with cluster size 20), or from <span class="math inline">\(95.4\)</span>~GB to only <span class="math inline">\(1\)</span>~KB if <span class="math inline">\(256\)</span> RP-trees are used.</p>
<section class="level3" id="optimise-data-structure">
<h3>Optimise Data Structure</h3>
<figure>
<img alt="" style="width:100.0%" title="plot_03" src="images/case-recommender/plot_03.png"><figcaption>Increase either leaf size or number of trees, but which is better?</figcaption>
</figure>
<p>A RP-tree helps us to locate a cluster which is likely to contain some of the <span class="math inline">\(k\)</span> nearest neighbours for a given query point. Within the cluster, a linear search is performed to identify the best candidates. Regarding the design of PANNS, we have two design options in order to improve the searching accuracy. Namely, given the size of the aggregated cluster which is taken as the union of all the target clusters from every tree, we can %: we can either use , or use .</p>
<ul>
<li>either use ,</li>
<li>or use .</li>
</ul>
<p>We expect that when using more trees the probability of a query point to fall very close to a splitting hyperplane should be reduced, thus it should be less likely for its nearest neighbours to lie in a different cluster. By reducing such misclassifications, the searching accuracy is supposed to be improved. Based on our knowledge, although there are no previous theoretical results that may justify such a hypothesis in the field of nearest neighbour search algorithms, this concept could be considered as a combination strategy similar to those appeared in ensemble clustering, a very well established field of research . Similar to our case, ensemble clustering algorithms improve clustering solutions by fusing information from several data partitions. In our further study on this particular part of the proposed system we intend to extend the probabilistic schemes developed in  in an attempt to discover the underlying theoretical properties suggested by our empirical findings. In particular, we intend to similarly provide theoretical bounds for failure probability and show that such failures can be reduced by using more RP-trees.</p>
<p>To experimentally investigate this hypothesis we employ a subset of the Wikipedia database for further analysis. In what follows, the data set contains <span class="math inline">\(500,000\)</span> points and we always search for the <span class="math inline">\(50\)</span> nearest neighbours of a query point. Then we measure the searching accuracy by calculating the amount of actual nearest neighbours found.</p>
<p>We query <span class="math inline">\(1,000\)</span> points in each experiment. The results presented in Figure  correspond to the mean values of the aggregated nearest neighbours of the <span class="math inline">\(1,000\)</span> query points discovered by PANNS out of <span class="math inline">\(100\)</span> experiment runs. Note that <span class="math inline">\(x\)</span>-axis represents the “size of search space” which is defined by the number of unique points within the union of all the leaf clusters that the query point fall in. Therefore, given the same search space size, using more tress indicates that the leaf clusters become smaller.</p>
<p>%%% figure</p>
<p>As we can see in Figure , for a given <span class="math inline">\(x\)</span> value, the curves move upwards as we use more and more trees, indicating that the accuracy improves. As shown in the case of 50 trees, almost <span class="math inline">\(80\%\)</span> of the actual nearest neighbours are found by performing a search over the <span class="math inline">\(10\%\)</span> of the data set.</p>
<p>To further illustrate the benefits of using as many RP-trees as possible, we present in Figure  the results where the size of search space remains approximately constant while the number of trees grows and subsequently the cluster size shrinks accordingly. As shown, a larger number of trees leads to the better accuracy. E.g., the accuracy is improved about <span class="math inline">\(62.5\%\)</span> by increasing the number of trees from <span class="math inline">\(2\)</span> to <span class="math inline">\(18\)</span>.</p>
<p>%%% figure</p>
<pre><code>Finally in Figure \ref{fig:test3} similar outcome is observed when the average size of the leaf clusters remains approximately constant and the number of trees increases. In these experiments, we choose two specific cluster sizes for comparisons, i.e., cluster size $77$ and $787$. Both are just average leaf cluster sizes resulted from the termination criterion in the tree construction procedure which pre-sets a maximum allowed size of a leaf cluster (here $100$ and $1000$ respectively, selected for illustration purposes as any other relative set up gives similar results). 
In addition, we also draw a random subset for any given size from the whole data set to serve as a baseline. As we see, the accuracy of the random subset has a linear improvement rate which is simply due to the linear growth of its search space. As expected, the RP-tree solutions are significantly better than the random subset, and cluster size $77$ consistently outperforms cluster size $787$ especially when the search space is small. </code></pre>
<p>%%% figure</p>
<p>Our empirical results clearly show the benefits of using more trees instead of using larger clusters for improving search accuracy. Moreover, regarding the searching performance, since searching can be easily parallelized, using more trees will not impact the searching time.</p>
<figure>
<img alt="" style="width:100.0%" title="plot_04" src="images/case-recommender/plot_04.png"><figcaption>We do not need to store the actual vector at each node. Instead, we can use a random seed to generate on the fly. In a leaf cluster, only the indices of vectors in the original data set are stored.</figcaption>
</figure>
</section>
<section class="level3" id="optimise-index-algorithm">
<h3>Optimise Index Algorithm</h3>
<p>refer to II.C in <span data-cites="7840682" class="citation">(Hyvönen et al. 2016)</span></p>
<figure>
<img alt="" style="width:90.0%" title="plot_05" src="images/case-recommender/plot_05.png"><figcaption>Illustration of parallelising the computation.</figcaption>
</figure>
<p>Blue dotted lines are critical boundaries. The computations in the child-branches cannot proceed without finishing the computation in the parent node. There is no critical boundary. All the projections can be done in just one matrix multiplication. Therefore, the parallelism can be maximised.</p>
<p>In classic RP trees, a different random vector is used at each inner node of a tree, whereas we use the same random vector for all the sibling nodes of a tree. This choice does not affect the accuracy at all because a query point is routed down each of the trees only once; hence, the query point is projected onto a random vector ri sampled from the same distribution at each level of a tree. This means that the query point is projected onto i.i.d. random vectors ….</p>
</section>
</section>
<section class="level2" id="search-articles">
<h2>Search Articles</h2>
</section>
<section class="level2" id="make-it-live">
<h2>Make It Live</h2>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-7840682">
<p>Hyvönen, V., T. Pitkänen, S. Tasoulis, E. Jääsaari, R. Tuomainen, L. Wang, J. Corander, and T. Roos. 2016. “Fast Nearest Neighbor Search Through Sparse Random Projections and Voting.” In <em>2016 Ieee International Conference on Big Data (Big Data)</em>, 881–88. <a href="https://doi.org/10.1109/BigData.2016.7840682">https://doi.org/10.1109/BigData.2016.7840682</a>.</p>
</div>
<div id="ref-7462177">
<p>Wang, L., S. Tasoulis, T. Roos, and J. Kangasharju. 2016. “Kvasir: Scalable Provision of Semantically Relevant Web Content on Big Data Framework.” <em>IEEE Transactions on Big Data</em> 2 (3): 219–33. <a href="https://doi.org/10.1109/TBDATA.2016.2557348">https://doi.org/10.1109/TBDATA.2016.2557348</a>.</p>
</div>
</div>
</section>
</section>
</article></div><a href="case-finance.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 34</small>Case - Applications in Finance</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>