<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Case - Image Recognition - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="case---image-recognition">
<h1>Case - Image Recognition</h1>
<p>How can a computer take an image and answer questions like “what is in this picture? A cat, dog, or something else?” In the last few years the field of machine learning has made tremendous progress on addressing this difficult problem. In particular, Deep Neural Network (DNN) can achieve reasonable performance on visual recognition tasks — matching or exceeding human performance in some domains.</p>
<p>There exist many good deep learning frameworks that can be used to do image classification, such as TensorFlow, Caffe, Torch, etc. But what if your choice of language is Functional Programming Language such as OCaml? It has long been thought that OCaml is not suitable for advanced computation tasks like machine learning. And now we have Owl.</p>
<section class="level2" id="image-processing">
<h2>Image Processing</h2>
<p>Let’s start with the image representation and processing in OCaml.</p>
<p>An input image is broken down into pixels. For a black and white image, those pixels are interpreted as a 2D array (for example, 2x2 pixels). Every pixel has a value between 0 and 255. (Zero is completely black and 255 is completely white. The greyscale exists between those numbers.) For a color image, this is a 3D array with a blue layer, a green layer, and a red layer. Each one of those colors has its own value between 0 and 255. The color can be found by combining the values in each of the three layers. (COPY ALERT)</p>
<p>Image processing is challenging, since OCaml does not provide powerful functions to manipulate images. Though there are image processing libraries such as <a href="http://gallium.inria.fr/camlimages/">CamlImages</a>, but we don’t want to add extra liabilities to Owl itself.</p>
<p>To this end, we choose the non-compressed image format PPM. A PPM file is a 24-bit color image formatted using a text format. It stores each pixel with a number from 0 to 65536, which specifies the color of the pixel. Therefore, we can just use ndarray in Owl and convert that directly to PPM image without using any external libraries. We only need to take care of header information during this process. For example, here is the code for converting an 3-dimensional array in Owl <code>img</code> into a ppm file.</p>
<div class="highlight">
<pre><code class="language-ocaml">module N = Dense.Ndarray.S

let save_image_to_file img outname = 
  (* metadata *)
  let shape = N.shape img in
  assert (Array.length(shape) = 3);
  let h = shape.(0) in
  let w = shape.(1) in
  let num_col = 255 in

  (* divide *)
  let r = N.get_slice [[];[];[1]] img in 
  let r = N.reshape r [|h; w|] in
  let g = N.get_slice [[];[];[2]] img  in
  let g = N.reshape g [|h; w|] in
  let b = N.get_slice [[];[];[0]] img in 
  let b = N.reshape b [|h; w|] in

  (* merge r, g, b to one [|h; 3*w|] matrix *)
  let img_mat = Dense.Matrix.S.zeros h (3 * w) in
  Dense.Matrix.S.set_slice [[];[0;-1;3]] img_mat r;
  Dense.Matrix.S.set_slice [[];[1;-1;3]] img_mat g;
  Dense.Matrix.S.set_slice [[];[2;-1;3]] img_mat b;

  (* rotate *)
  let img_mat = Dense.Matrix.S.rotate img_mat 90 in 
  let img_arr = Dense.Matrix.S.to_arrays img_mat in 

  (* change to line *)
  let img_str = Bytes.make (w * h * 3) ' ' in 
  let ww = 3 * w in 
  for i = 0 to ww - 1 do
    for j = 0 to h - 1 do
      let ch = img_arr.(i).(j) |&gt; int_of_float |&gt; char_of_int in
      Bytes.set img_str ((h - 1 -j) * ww + i) ch;
    done
  done;

  let header = "P6\n" ^ string_of_int(h) ^ " " ^ string_of_int(w) ^ "\n" ^ string_of_int(num_col) ^ "\n" in 
  let img_final = Bytes.concat (Bytes.of_string " ") [header |&gt; Bytes.of_string; img_str] in 
  Owl_io.write_file outname (Bytes.to_string img_final)</code></pre>
</div>
<p>In this function we first get the required meta data such as image size, and then combine the three channels of a image (three slices of the input ndarray) into a single line of string by correct order. The finally we construct these information into bytes and then write to the output PPM file.</p>
<p>Similarly, to read an PPM image file into ndarray in Owl, we treat the ppm file line by line with <code>input_line</code> function. The metadata such as version and comments are ignored. We get the metadata such as width and heigh from the header.</p>
<div class="highlight">
<pre><code class="language-text">...
let w_h_line = input_line fp in
let w, h = Scanf.sscanf w_h_line "%d %d" (fun w h -&gt; w, h) in
...</code></pre>
</div>
<p>Then according these information, we read the rest of data into a large bytes with <code>Pervasive.really_input</code>. Note that under 32bit OCaml, this will only work when reading strings up to about 16 megabytes.</p>
<div class="highlight">
<pre><code class="language-text">...
let img = Bytes.make (w * h * 3) ' ' in
really_input fp img 0 (w * h * 3);
close_in fp;
...</code></pre>
</div>
<p>Then we need to re-arrange the data in the bytes into a matrix.</p>
<div class="highlight">
<pre><code class="language-clike">let imf_o = Array.make_matrix (w * 3) h 0.0 in
  
let ww = 3 * w  in
for i = 0 to ww - 1 do
  for j = 0 to h - 1 do
    imf_o.(i).(j) &lt;- float_of_int (int_of_char (Bytes.get img ((h - 1 - j ) * ww + i)));
  done
done;</code></pre>
</div>
<p>This matrix can then be further processed into a ndarray with proper slicing.</p>
<div class="highlight">
<pre><code class="language-clike">...
let m = Dense.Matrix.S.of_arrays img in
let m = Dense.Matrix.S.rotate m 270 in
let r = N.get_slice [[];[0;-1;3]] m in
let g = N.get_slice [[];[1;-1;3]] m in
let b = N.get_slice [[];[2;-1;3]] m in
...</code></pre>
</div>
<p>There are other functions such as reading in ndarray from PPM file. The full image processing code can be viewed in <a href="https://gist.github.com/jzstark/86a1748bbc898f2e42538839edba00e1">this gist</a>.</p>
<p>Of course, most of the time we have to deal with image of other more common format such as PNG and JPEG. For the conversion from these format to PPM or the other way around, we use the tool <a href="https://www.imagemagick.org/">ImageMagick</a>.</p>
</section>
<section class="level2" id="building-blocks-of-image-recognition">
<h2>Building Blocks of Image Recognition</h2>
<p>In the regression chapter we have seen how the multi-classification problem can be solved with neural networks that contains several hidden layers. If the input is an image, then every single pixel acts as a input feature.</p>
<p>One important improvement that the Convolution Neural Network make is that it uses filters in the convolution operation. As a result, instead of using the whole image as an array of features, the image is divided into a number of tiles. They will then serve as the basic feature of the network’s prediction.</p>
<p>We cannot discuss the theory behind building computer vision models, and the existing models vary greatly in its characteristics, but they share similar building blocks, the most important ones are convolution layer, RELU activation layer, pooling layer, and fully connected layer. This whole process is shown in fig.&nbsp;1.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:case-image-inception:workflow" title="workflow" src="images/case-image-inception/cnn_workflow.png"><figcaption>Figure 1: Workflow of image classification</figcaption>
</figure>
<p>In general, layers of convolution retrieve information from detailed to more abstracted gradually. In a DNN, The lower layers of neurons retrieve information about simple shapes such as edges and points. Going higher, the neurons can capture complex structures, such as the tire of a car, the face of a cat, etc. Close to the top layers, the neurons can retrieve and abstract complex ideas, such as “car”, “cat”, etc. And then finally generates the classification results.</p>
<p>This is basically how we detect images. We don’t look at every single pixel of an image. We see features like a hat, a red dress, a tattoo, and so on. There’s so much information going into our eyes at all times that we couldn’t possibly deal with every single pixel of it. We’re allowing our model to do the same thing. The result of this is the convolved feature map. It’s smaller than the original input image. This makes it easier and faster to deal with. Do we lose information? Some, yes. But at the same time, the purpose of the feature detector is to detect features, which is exactly what this does. We create many feature maps to get our first convolution layer. This allows us to identify many different features that the program can use to learn. (COPY ALERT)</p>
<p>The ReLU (rectified linear unit) layer is another step to our convolution layer. You’re applying an activation function onto your feature maps to increase non-linearity in the network. This is because images themselves are highly non-linear. It removes negative values from an activation map by setting them to zero. Convolution is a linear operation with things like element wise matrix multiplication and addition. The real-world data we want our CNN to learn will be non-linear. We can account for that with an operation like ReLU. You can use other operations like tanh or sigmoid. ReLU, however, is a popular choice because it can train the network faster without any major penalty to generalization accuracy. (COPY ALERT)</p>
<p>The last thing you want is for your network to look for one specific feature in an exact shade in an exact location. That’s useless for a good CNN! You want images that are flipped, rotated, squashed, and so on. You want lots of pictures of the same thing so that your network can recognize an object (say, a leopard) in all the images. No matter what the size or location. No matter what the lighting or the number of spots, or whether that leopard is fast asleep or crushing prey. You want spatial variance! You want flexibility. That’s what pooling is all about. Pooling progressively reduces the size of the input representation. It makes it possible to detect objects in an image no matter where they’re located. Pooling helps to reduce the number of required parameters and the amount of computation required. It also helps control overfitting. (COPY ALERT)</p>
<p>(Image and text copy <a href="https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb">src</a>.)</p>
</section>
<section class="level2" id="building-inceptionv3-network">
<h2>Building InceptionV3 Network</h2>
<p>Proppsed by Christian Szegedy et. al., <a href="https://arxiv.org/abs/1512.00567">InceptionV3</a> is one of Google’s latest effort to do image recognition. It is trained for the <a href="http://www.image-net.org/challenges/LSVRC/">ImageNet Large Visual Recognition Challenge</a>. This is a standard task in computer vision, where models try to classify entire images into 1000 classes, like “Zebra”, “Dalmatian”, and “Dishwasher”, etc. Compared with previous DNN models, InceptionV3 has one of the most complex networks architectures in computer vision.</p>
<p>The design of image recognition networks is about the tradeoff between computation cost, memory usage, and accuracy. Just increasing model size and computation cost tends to increase the accuracy, but the benefit will decrease soon. To solve this problem, compared to previous similar networks, the Inception architecture aims to perform well with strict constraints on memory and computational budget. This design follows several principles, such as balancing the width and depth of the network, and performing spatial aggregation over lower dimensional embeddings can lead to small loss in representational power of networks. The resulting Inception network architectures has high performance and a relatively modest computation cost compared to simpler, more monolithic architectures.</p>
<p>Here is the overall architecture of this network (<a href="https://cloud.google.com/tpu/docs/inception-v3-advanced">src</a>):</p>
<figure>
<img alt="" style="width:95.0%" id="fig:case-image-inception:inceptionv3" title="inceptionv3" src="images/case-image-inception/inceptionv3.png"><figcaption>Figure 2: Network Architecture of InceptionV3</figcaption>
</figure>
<p>We can see that the whole network can be divided into several parts, and the inception module A, B, and C are both repeated based on one structure. We can define this network easily in Owl.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Owl
open Owl_types
open Neural.S
open Neural.S.Graph

let conv2d_bn ?(padding=SAME) kernel stride nn =
  conv2d ~padding kernel stride nn
  |&gt; normalisation ~training:false ~axis:3
  |&gt; activation Activation.Relu</code></pre>
</div>
<p>Here the <code>conv2d_bn</code> is a basic building block used in this network, consisting of a convolution layer, a normalisation layer, and a Relu activation layer.</p>
<div class="highlight">
<pre><code class="language-ocaml">let mix_typ1 in_shape bp_size nn =
  let branch1x1 = conv2d_bn [|1;1;in_shape;64|] [|1;1|] nn in
  let branch5x5 = nn
    |&gt; conv2d_bn [|1;1;in_shape;48|] [|1;1|]
    |&gt; conv2d_bn [|5;5;48;64|] [|1;1|]
  in
  let branch3x3dbl = nn
    |&gt; conv2d_bn [|1;1;in_shape;64|] [|1;1|]
    |&gt; conv2d_bn [|3;3;64;96|]  [|1;1|]
    |&gt; conv2d_bn [|3;3;96;96|]  [|1;1|]
  in
  let branch_pool = nn
    |&gt; avg_pool2d [|3;3|] [|1;1|]
    |&gt; conv2d_bn [|1;1;in_shape; bp_size |] [|1;1|]
  in
  concatenate 3 [|branch1x1; branch5x5; branch3x3dbl; branch_pool|]</code></pre>
</div>
<p><code>mix_typ1</code> is repeated for three times in the Inception module A.</p>
<div class="highlight">
<pre><code class="language-ocaml">let mix_typ3 nn =
  let branch3x3 = conv2d_bn [|3;3;288;384|] [|2;2|] ~padding:VALID nn in
  let branch3x3dbl = nn
    |&gt; conv2d_bn [|1;1;288;64|] [|1;1|]
    |&gt; conv2d_bn [|3;3;64;96|] [|1;1|]
    |&gt; conv2d_bn [|3;3;96;96|] [|2;2|] ~padding:VALID
  in
  let branch_pool = max_pool2d [|3;3|] [|2;2|] ~padding:VALID nn in
  concatenate 3 [|branch3x3; branch3x3dbl; branch_pool|]</code></pre>
</div>
<p><code>mix_typ3</code> builds the first grid size reduction module. This module implements an efficient feature map downsizing function.</p>
<div class="highlight">
<pre><code class="language-ocaml">let mix_typ4 size nn =
  let branch1x1 = conv2d_bn [|1;1;768;192|] [|1;1|] nn in
  let branch7x7 = nn
    |&gt; conv2d_bn [|1;1;768;size|] [|1;1|]
    |&gt; conv2d_bn [|1;7;size;size|] [|1;1|]
    |&gt; conv2d_bn [|7;1;size;192|] [|1;1|]
  in
  let branch7x7dbl = nn
    |&gt; conv2d_bn [|1;1;768;size|] [|1;1|]
    |&gt; conv2d_bn [|7;1;size;size|] [|1;1|]
    |&gt; conv2d_bn [|1;7;size;size|] [|1;1|]
    |&gt; conv2d_bn [|7;1;size;size|] [|1;1|]
    |&gt; conv2d_bn [|1;7;size;192|] [|1;1|]
  in
  let branch_pool = nn
    |&gt; avg_pool2d [|3;3|] [|1;1|]
    |&gt; conv2d_bn [|1;1; 768; 192|] [|1;1|]
  in
  concatenate 3 [|branch1x1; branch7x7; branch7x7dbl; branch_pool|]</code></pre>
</div>
<p><code>mix_typ4</code> is similar to <code>mix_typ1</code>, and is the building block of the Inception module B.</p>
<div class="highlight">
<pre><code class="language-ocaml">let mix_typ8 nn =
  let branch3x3 = nn
    |&gt; conv2d_bn [|1;1;768;192|] [|1;1|]
    |&gt; conv2d_bn [|3;3;192;320|] [|2;2|] ~padding:VALID
  in
  let branch7x7x3 = nn
    |&gt; conv2d_bn [|1;1;768;192|] [|1;1|]
    |&gt; conv2d_bn [|1;7;192;192|] [|1;1|]
    |&gt; conv2d_bn [|7;1;192;192|] [|1;1|]
    |&gt; conv2d_bn [|3;3;192;192|] [|2;2|] ~padding:VALID
  in
  let branch_pool = max_pool2d [|3;3|] [|2;2|] ~padding:VALID nn in
  concatenate 3 [|branch3x3; branch7x7x3; branch_pool|]</code></pre>
</div>
<p><code>mix_typ8</code> is the second grid size reduction module.</p>
<div class="highlight">
<pre><code class="language-ocaml">let mix_typ9 input nn =
  let branch1x1 = conv2d_bn [|1;1;input;320|] [|1;1|] nn in
  let branch3x3 = conv2d_bn [|1;1;input;384|] [|1;1|] nn in
  let branch3x3_1 = branch3x3 |&gt; conv2d_bn [|1;3;384;384|] [|1;1|] in
  let branch3x3_2 = branch3x3 |&gt; conv2d_bn [|3;1;384;384|] [|1;1|] in
  let branch3x3 = concatenate 3 [| branch3x3_1; branch3x3_2 |] in
  let branch3x3dbl = nn |&gt; conv2d_bn [|1;1;input;448|] [|1;1|] |&gt; conv2d_bn [|3;3;448;384|] [|1;1|] in
  let branch3x3dbl_1 = branch3x3dbl |&gt; conv2d_bn [|1;3;384;384|] [|1;1|]  in
  let branch3x3dbl_2 = branch3x3dbl |&gt; conv2d_bn [|3;1;384;384|] [|1;1|]  in
  let branch3x3dbl = concatenate 3 [|branch3x3dbl_1; branch3x3dbl_2|] in
  let branch_pool = nn |&gt; avg_pool2d [|3;3|] [|1;1|] |&gt; conv2d_bn [|1;1;input;192|] [|1;1|] in
  concatenate 3 [|branch1x1; branch3x3; branch3x3dbl; branch_pool|]</code></pre>
</div>
<p>The final part is Inception module C, which repeats two <code>mix_type9</code> function. With all these parts ready, we can put them together into the whole network.</p>
<div class="highlight">
<pre><code class="language-ocaml">let make_network img_size =
  input [|img_size;img_size;3|]
  |&gt; conv2d_bn [|3;3;3;32|] [|2;2|] ~padding:VALID
  |&gt; conv2d_bn [|3;3;32;32|] [|1;1|] ~padding:VALID
  |&gt; conv2d_bn [|3;3;32;64|] [|1;1|]
  |&gt; max_pool2d [|3;3|] [|2;2|] ~padding:VALID
  |&gt; conv2d_bn [|1;1;64;80|] [|1;1|] ~padding:VALID
  |&gt; conv2d_bn [|3;3;80;192|] [|1;1|] ~padding:VALID
  |&gt; max_pool2d [|3;3|] [|2;2|] ~padding:VALID
  |&gt; mix_typ1 192 32 |&gt; mix_typ1 256 64 |&gt; mix_typ1 288 64
  |&gt; mix_typ3
  |&gt; mix_typ4 128 |&gt; mix_typ4 160 |&gt; mix_typ4 160 |&gt; mix_typ4 192
  |&gt; mix_typ8
  |&gt; mix_typ9 1280 |&gt; mix_typ9 2048
  |&gt; global_avg_pool2d
  |&gt; linear 1000 ~act_typ:Activation.(Softmax 1)
  |&gt; get_network</code></pre>
</div>
<p>The full code is listed in <a href="https://gist.github.com/jzstark/9428a62a31dbea75511882ab8218076f">this gist</a>. Even if you are not quite familiar with Owl or OCaml, it must still be quite surprising to see the network that contains 313 neuron nodes can be constructed using only about 150 lines of code. And we are talking about one of the most complex neural networks for computer vision.</p>
<p>Besides InceptionV3, you can also easily construct other popular image recognition networks, such as <a href="https://gist.github.com/pvdhove/a05bf0dbe62361b9c2aff89d26d09ba1">ResNet50</a>, <a href="https://gist.github.com/jzstark/f5409c44d6444921a8ceec00e33c42c4">VGG16</a>, <a href="https://gist.github.com/jzstark/c424e1d1454d58cfb9b0284ba1925a48">SqueezeNet</a> etc. with elegant Owl code.</p>
</section>
<section class="level2" id="preparing-weights">
<h2>Preparing Weights</h2>
<p>Only building a network structure is not enough. Another important aspect is proper weights of a neural network. It can be achieved by training on GBs of image data for days or longer on powerful machine clusters.</p>
<p>The training is usually done via supervised learning using a large set of labelled images. Although Inception v3 can be trained from many different labelled image sets, ImageNet is a common dataset of choice. ImageNet has over ten million URLs of labelled images. About a million of the images also have bounding boxes specifying a more precise location for the labelled objects. For the Inception model, the ImageNet dataset is composed of 1,331,167 images which are split into training and evaluation datasets containing 1,281,167 and 50,000 images, respectively. (<a href="https://cloud.google.com/tpu/docs/inception-v3-advanced">COPY ALERT</a>) The training of this model can take hundreds of hours of training on multiple high-performance GPUs.</p>
<p>However, not everyone has access to such large resource. Another option is more viable: importing weights from existed pre-trained TensorFlow models, which are currently widely available in model collections such as <a href="https://github.com/fchollet/deep-learning-models/">this one</a>.</p>
<p>The essence of weights is list of ndarrays, which is implemented using <code>Bigarray</code> in OCaml. So the basic idea is to find a intermediate representation so that we can exchange the ndarray in NumPy and <code>Bigarray</code> in OCaml. In our implementation, we choose to use the <a href="https://portal.hdfgroup.org/display/HDF5/HDF5">HDF5</a> as this intermediate data exchange format. In Python, we use the <a href="https://www.h5py.org/">h5py</a> library, and in OCaml we use <a href="https://github.com/vbrankov/hdf5-ocaml">hdf5-ocaml</a>.</p>
<p>The method to save or load hdf5 data files are fixed, but the methods to retrieve data from model files vary depending on the type of original files. For example, if we choose to import weight form a TensorFlow model, we do something like this to achieves the weight data of each layer:</p>
<div class="highlight">
<pre><code class="language-python">...
reader = tf.train.NewCheckpointReader(checkpoint_file)
for key in layer_names:
    data=reader.get_tensor(key).tolist()
...</code></pre>
</div>
<p>In a keras, it’s a bit more straightforward:</p>
<div class="highlight">
<pre><code class="language-python">...
for layer in model.layers:
    weights = layer.get_weights()
...</code></pre>
</div>
<p>In the OCaml side, we first create a Hashtable and read all the HDF5 key-value pairs into it. Each value is saved as a double precision Owl ndarray.</p>
<div class="highlight">
<pre><code class="language-text">...
let h = Hashtbl.create 50 in
let f = H5.open_rdonly h5file  in
for i = 0 to (Array.length layers - 1) do
  let w = H5.read_float_genarray f layers.(i) C_layout in
  Hashtbl.add h layers.(i) (Dense.Ndarray.Generic.cast_d2s w)
done;
...</code></pre>
</div>
<p>And then we can use the mechanism in the Neural Network model to load these values from the hashtable to networks:</p>
<div class="highlight">
<pre><code class="language-text">...
let wb = Neuron.mkpar n.neuron in 
Printf.printf "%s\n" n.name; 
wb.(0) &lt;- Neuron.Arr (Hashtbl.find layers n.name);
Neuron.update n.neuron wb
...</code></pre>
</div>
<p>It is very important to make clear the difference in naming of each layer in different platforms, since the creator of the original model may choose any name for each layer. Other differences have also to be taken care of. For example, the <code>beta</code> and <code>gamma</code> weights in the batch normalisation layer is represented as two different values in TensorFlow model, but they belong to the same layer in Owl. Also, some times the dimensions has to be swapped in a ndarray during this weight conversion.</p>
<p>Note that this is one-off work. Once you successfully update the network with weights, the weights can be saved using <code>Graph.save_weights</code>, without haveing to repeat all these steps again. We have already prepared the weights for the InceptionV3 model and other similar models, and the users don’t have to worry about all these trivial model exchanging detail.</p>
</section>
<section class="level2" id="running-inference">
<h2>Running Inference</h2>
<p>Prepare one image on your computer. It can be of any common image format (jpg, png, gif, etc.) and size. If you’re not sure which image to use, here is one choice we use in the rest of this chapter:</p>
<figure>
<img alt="" style="width:50.0%" id="fig:case-image-inception:panda" src="images/case-image-inception/panda.png"><figcaption>Figure 3: Panda image that is used for image recognition task</figcaption>
</figure>
<p>Let’s do the image classification with one line of code:</p>
<div class="highlight">
<pre><code class="language-clike">owl -run 6dfed11c521fb2cd286f2519fb88d3bf</code></pre>
</div>
<p>That’s it. This one-liner is all you need to do to see a image classification example in action. Here is the output (assume using the panda image previously mentioned):</p>
<div class="highlight">
<pre><code class="language-clike">Top 5 Predictions:
Prediction #0 (96.20%) : giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca
Prediction #1 (0.12%) : lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens
Prediction #2 (0.06%) : space shuttle
Prediction #3 (0.04%) : soccer ball
Prediction #4 (0.03%) : indri, indris, Indri indri, Indri brevicaudatus</code></pre>
</div>
<p><strong>Code Detail</strong></p>
<p>This one line of code uses the Zoo system to import the code from <a href="https://gist.github.com/jzstark/6dfed11c521fb2cd286f2519fb88d3bf">this gist</a>. Let’s look at the the code in detail:</p>
<div class="highlight">
<pre><code class="language-clike">#!/usr/bin/env owl
open Owl

(* Import InceptionV3 Library *)
#zoo "9428a62a31dbea75511882ab8218076f"

let _ = 
  let img = "panda.png" in
  let labels = InceptionV3.infer img in
  let top = 5 in 
  let labels_json   = InceptionV3.to_json ~top labels in
  let labels_tuples = InceptionV3.to_tuples labels in

  Printf.printf "\nTop %d Predictions:\n" top;
  Array.iteri (fun i x -&gt; 
    let cls, prop = x in 
    Printf.printf "Prediction #%d (%.2f%%) : %s\n" i (prop *. 100.) cls;
  ) labels_tuples</code></pre>
</div>
<p>You need 5 steps to do image classification with InceptionV3:</p>
<ol type="1">
<li><p>Import external code/libraries using Zoo in Owl. Using <code>#zoo "gist-id"</code> enables you to use code modules defined in other Gists. Here we want to use <a href="https://gist.github.com/jzstark/9428a62a31dbea75511882ab8218076f">InceptionV3</a> modules. It defines the InceptionV3 network architecture and loads weights of the network. The downloaded code are cached in <code>$HOME/.owl/zoo</code> directory. The InceptionV3 module provides three APIs:</p>
<ul>
<li><p><code>infer</code>: Service that performs image recognition tasks over client images. It accept a string that specify the location of a local image. Its return value is a 1x1000 N-dimension array, each element is a float number between 0 and 1, indicating the possibility that the image belongs to one of the 1000 classes from ImageNet.</p></li>
<li><p><code>to_json</code>: Convert the inferred result to a raw JSON string. Parameter <code>top</code>: an int value to specify the top-N likeliest labels to return. Default value is 5.</p></li>
<li><p><code>to_json</code>: Convert the inferred result to an array of tuples, each tuple contains label name (“class”, string) and the probability (“prop”, float, between 0 and 1) of target image being in that class.</p></li>
</ul></li>
<li><p>Load InceptionV3 model with one line of code.</p></li>
<li><p>Designate an absolute path of your input image. Here we use the <code>extend_zoo_path</code> util function to automatically find the “panda.png” image contained in the Gist itself.</p></li>
<li><p>Run inference with the neural network model and the input image, and then decode the result, getting top-N (N defaults to 5) predictions in human-readable format. The output is an array of tuple, each tuple consists of a string for classified type description, and a float number ranging from 0 to 100 to represent the percentage probability of the input image actually being this type.</p></li>
<li><p>If you want, you can pretty-print the result on your screen.</p></li>
</ol>
<p>If you are not interested in installing anything, <a href="http://demo.ocaml.xyz/">here</a> is a web-based demo of this image classification application powered by Owl. Please feel free to play with it! And the server won’t store your image. Actually, if you are so keen to protect your personal data privacy, then you definitely should try to pull the code here and fast build a local image processing service without worrying your images being seen by anybody else!</p>
</section>
<section class="level2" id="applications">
<h2>Applications</h2>
</section>
</section>
</article></div><a href="case-obj-detect.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 30</small>Case - Instance Segmentation</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>