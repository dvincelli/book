<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Optimisation - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="optimisation">
<h1>Optimisation</h1>
<p>Optimisation is one of the fundamental functionality in numerical computation. In this chapter, we will briefly introduce the optimisation methods. We will use Owl to implement some of these methods.</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Mathematical optimisation deals with the problem of finding minimums or maximums of a function. The solution can be numerical if closed-form expression does not exist. An optimisation problem has the form:</p>
<p><span class="math display">\[\textrm{minimise} f_0(\mathbf{x}),\]</span> <span id="eq:optimisation:def"><span class="math display">\[\textrm{subject to} f_i(\mathbf{x}) \leq b_i, i = 1, 2, \ldots, m. \qquad(1)\]</span></span></p>
<p>Here <span class="math inline">\(\mathbf{x}\)</span> is a vector that contains all the <em>optimisation variable</em>: <span class="math inline">\(\mathbf{x} = [x_0, x_1, ... x_n]\)</span>. Function <span class="math inline">\(f_0 : \mathbf{R}^n \rightarrow \mathbf{R}\)</span> is the optimisation target, and is called an <em>objective function</em>, or <em>cost function</em>. A optimisation problem could be bounded by zero or more <em>constraints</em>. <span class="math inline">\(f_i : \mathbf{R}^n \rightarrow \mathbf{R}\)</span> in a constraint is called a <em>constraint function</em>, which are bounded by the <span class="math inline">\(b_i\)</span>’s. The target is to find the optimal variable values <span class="math inline">\(\mathbf{x}^{*}\)</span> so that <span class="math inline">\(f_0\)</span> can take on a maximum or minimum value.</p>
<p>A optimisation problem formalises the idea “maximum benefit/minimise cost with given constraint”, which is a widely applicable topic in many real world problems: scheduling computation/network resources, optimisation of investment portfolio, fitting math model based on observed data, logistics, aero engineering, competitive games … Optimisation has already been applied in many areas.</p>
<p>In eq.&nbsp;1, if for all the objective function and constraint function, we have:</p>
<p><span id="eq:optimisation:linear"><span class="math display">\[ f_i(\alpha~x+\beta~y) = \alpha~f_i(x) + \beta~f_i(y),\qquad(2)\]</span></span></p>
<p>the optimisation problem is then called <em>linear optimisation</em>. It is an important class of optimisation problems. If we change the “<span class="math inline">\(=\)</span>” to “<span class="math inline">\(\leq\)</span>” in eq.&nbsp;2 make all the functions to be <em>convex</em>, and the problem then becomes <em>convex optimisation</em>, which can be seen as a generalised linear optimisation. In opimisation world, convexity is considerd as the wathershed between easy and difficult problems, because for most convex problems, there are efficient algorithmic solutions.</p>
<p>Linear optimisation is important because non-negativity is a usual constraint on real world quantities, and that people are often interested in additive bounds. Besides, many problems can be approximated by a linear model. Though still limited by actual problem size, the solution of most linear optimisation problems are already known and provided by off-the-shelf software tools. The text book <span data-cites="boyd2004convex" class="citation">(Boyd and Vandenberghe 2004)</span> focus exclusively on the topic of convex optimisation.</p>
<p>Compare to linear optimisation, sovling <em>non-linear optimisation</em> problems can still be very challenging. Finding a <em>global</em> solution that maximises or minimises the non-linear objective function is often quite time-consuming, even for only a small set of variables. Therefore, global optimisation of a non-linear problem is normally only used when absolutely necessary. For example, if a system pressure test is modelled as an optimisation problem, given a small number of variants in the system, and a global extreme value has to find to test if the system is robust enough. Otherwise, a <em>local</em> maximum or minimum is normally used instead as an approximation. In most engineering applications, a local extreme value is good enough. Even though optimisation cannot promise a true extremism, and is easily affected by algorithm parameters and initial guess in iterative algorithms, as a trade-off, local optimisation is much faster and thus still widely used.</p>
<p>Looking back at eq.&nbsp;1, if we remove the constraints, then it becomes an <em>unconstrained optimisation</em> problem. If <span class="math inline">\(f\)</span> is convex and differentiable, this problem can be seen as finding the root of the derivative of <span class="math inline">\(f\)</span> so that <span class="math inline">\(f'(x^*) = 0\)</span>. As for the constrained version, we have introduced the linear programming where all the functions are linear. There are also other types of optimisations such as quadratic programming, semi-definite programming, etc. One subset of constrained optimisation, the <em>equality constrained optimisation</em> where all the constraints are expressed in the form of equality <span class="math inline">\(Ax=b\)</span>. This set of problem can be simplified into the corresponding unconstrained problems.</p>
<p>You can see that the topic of optimisation covers a wide range of topics and we can only give a very brief introduction here. In this chapter, we mostly cover the unconstrained and local optimisation. We will cover the other more advanced content briefly in the end of this chapter, and refer readers to classic books such as <span data-cites="boyd2004convex" class="citation">(Boyd and Vandenberghe 2004)</span> and <span data-cites="fletcher2013practical" class="citation">(Fletcher 2013)</span> for more information.</p>
<p>(NOTE: if we decide to add linear programming later, we can extend the constrained)</p>
<p>(NOTE: We need a lot of illustrations to show the gradient process)</p>
</section>
<section class="level2" id="numerical-differentiation-vs.-algorithmic-differentiation">
<h2>Numerical Differentiation VS. Algorithmic Differentiation</h2>
<p>The derivative/gradient is used extensively in solving optimisation problems. Therefore, let’s start this chapter with understanding the difference of the two ways to compute derivatives: <em>algorithmic differentiation</em> and <em>numerical differentiation</em>.</p>
<p>We have talked about algorithmic differentiation* in detail in the previous chapter. What is this numerical differentiation then? It’s actually simple according to the definition of derivative itself:</p>
<p><span id="eq:optimisation:numdiff"><span class="math display">\[f'(x) = \lim_{\delta~\to~0}\frac{f(x+\delta) - f(x)}{\delta}.\qquad(3)\]</span></span></p>
<p>This method is pretty easy to follow: evaluate the given <span class="math inline">\(f\)</span> at point <span class="math inline">\(x\)</span>, and then choose a suitable small amount <span class="math inline">\(\delta\)</span>, add it to the original <span class="math inline">\(x\)</span> and then re-evaluate the function. Then the derivative can be calculated using eq.&nbsp;3. We can implement this method easily using OCaml:</p>
<div class="highlight">
<pre><code class="language-ocaml">let _eps = 0.00001

let diff f x = (f (x +. _eps) -. f x) /. _eps</code></pre>
</div>
<p><strong>liang: x can in the middle, left, and right. Show that it is optimal to be in the middle.</strong></p>
<p>We can apply it to a simple case. Let’s say, we want to find the derivative fo <span class="math inline">\(f(x) = sin(x)\)</span> at point <span class="math inline">\(x=1\)</span>. Basic calculus tells us that it should be equals to <span class="math inline">\(cos(1) = 0.5403\)</span>. Here is the code:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let f = Maths.cos;;
&gt;val f : float -&gt; float = &lt;fun&gt;
let d = diff f 1. ;;
&gt;val d : float = -0.841473686319371583
</code></pre>
</div>
<p>Looks good.</p>
<p>Owl has provided numerical differentiation. It’s close to the interface of that of Algodiff:</p>
<div class="highlight">
<pre><code class="language-clike">val diff : (elt -&gt; elt) -&gt; elt -&gt; elt
(** derivative of ``f : scalar -&gt; scalar``. *)

val diff2 : (elt -&gt; elt) -&gt; elt -&gt; elt
(** second order derivative of ``f : float -&gt; float``. *)

val grad : (arr -&gt; elt) -&gt; arr -&gt; arr
(** gradient of ``f : vector -&gt; scalar``. *)

val jacobian : (arr -&gt; arr) -&gt; arr -&gt; arr
(** jacobian of ``f : vector -&gt; vector``. *)

val jacobianT : (arr -&gt; arr) -&gt; arr -&gt; arr
(** transposed jacobian of ``f : vector -&gt; vector``. *)</code></pre>
</div>
<p>Looks nice, much easier than Algodiff’s approach, right? No.&nbsp;Sadly, this naive numerical solution can lead to large errors in reality. There are two source of errors: truncating error and round-off error.</p>
<p>The <em>truncating error</em> comes from the fact that eq.&nbsp;3 is only an approximation of the true gradient value. We can see their difference with Tayler expansion:</p>
<p><span class="math display">\[f(x+h) = f(x) + hf'(x) + \frac{h^2}{2}f^{''}(\sigma_h)\]</span></p>
<p>Here <span class="math inline">\(h\)</span> is the step size and <span class="math inline">\(\sigma_h\)</span> is in the range of <span class="math inline">\([x, x+h]\)</span>. This can be transformed into:</p>
<p><span class="math display">\[-frac{h^2}{2}f^{''}(\sigma_h)= f'(x) - \frac{f(x+h) - f(x)}{h}.\]</span></p>
<p>This represent the truncation error in the approximation. For example, for function <span class="math inline">\(f(x) = sin(x)\)</span>, <span class="math inline">\(f''(x) = -sin(x)\)</span>. Suppose we want to calculate the derivative at <span class="math inline">\(x=1\)</span> numerically using a step size of 0.01, then the truncation error should be in the range <span class="math inline">\(\frac{0.01^2}{2}[sin(1), sin(1.01)]\)</span>. Here we can see the effect of this truncation error, by using a improper step size:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let d = 
  let _eps = 0.1 in
  let diff f x = (f (x +. _eps) -. f x) /. _eps in 
  diff f 1.
;;
&gt;val d : float = -0.867061844425624506
</code></pre>
</div>
<p>Another source of error is the round-off error. Looking back at eq.&nbsp;3, we need to calculate <span class="math inline">\(f(x+h) - f(x)\)</span>, the subtraction of two almost the same number. That leads to large round-off errors. For example, let’s choose a very small step size:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let d = 
  let _eps = 5E-16 in
  let diff f x = (f (x +. _eps) -. f x) /. _eps in 
  diff f 1.
;;
&gt;val d : float = -0.888178419700125121
</code></pre>
</div>
<p>Actually if we use a even smaller step size <span class="math inline">\(1e-16\)</span>, the result becomes 0, which means the round-off error is large enough that <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(f(x+h)\)</span> are deemed the same by the computer.</p>
<p>Compared to numerical differentiation, Algodiff guarantees a true derivative value without loss of accuracy. If you compute the the derivative of <code>sin</code>, then the returned results would use the <code>cos</code> function. For the rest of this chapter, we prefer to use the algorithmic differentiation to compute derivatives when required, but of course you can also use the numerical differentiation.</p>
</section>
<section class="level2" id="root-finding">
<h2>Root Finding</h2>
<p>We have seen some examples of root finding in the Math chapter. <em>Root finding</em> is the process which tries to find zeroes or <em>roots</em> of continuous functions. It is not an optimisation problem, but these two topics are closely related. I would be beneficial for users to learn about the methods used in optimisation if they understand how the root finding algorithm work, e.g.&nbsp;how to the root by bracketing and how to find target in an iterative manner.</p>
<section class="level3" id="bisect-newton-secant-and-iqi">
<h3>Bisect, Newton, Secant, and IQI</h3>
<p>First, the Bisection method. Use <span class="math inline">\(\sqrt{2}\)</span> as an example, just show the string of number here:<span class="math inline">\(1\frac{1}{2}, 1\frac{1}{4}, 1\frac{3}{8}, 1\frac{5}{16} \ldots\)</span>. (DETAIL) Owl provides <code>Owl_maths_root.bisec</code> method. (NOTE: we can have a example or even paste the Owl impl. here if we want to beef this section up, but let’s keep thing concise for now.) This method converges slowly, but it is a solid and reliable method.</p>
<p>Newton method utilises the derivative of objective function <span class="math inline">\(f\)</span>. It starts with a initial value <span class="math inline">\(x_0\)</span>, and follows this process:</p>
<p><span id="eq:optimisation:newton"><span class="math display">\[x_{n+1} = x_{n} - \frac{f(x_n)}{f'(x_n)}.\qquad(4)\]</span></span></p>
<p>We can use the Algorithm Differention module in Owl to do that. In the next example we find the root of <span class="math inline">\(x^2 - 2 = 0\)</span>, i.e., find an approximate value of <span class="math inline">\(\sqrt{2}\)</span>. The Owl code is just a plain translation of eq.&nbsp;4.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.D

let f x = Maths.(x ** (F 2.) - (F 2.))

let _ = 
    let x = ref 1. in
    for _ = 0 to 6 do
        let g = diff f (F !x) |&gt; unpack_elt in 
        let v = f (F !x) |&gt; unpack_elt in 
        x := !x -. v /. g;
        Printf.printf "%.15f\n" !x
    done</code></pre>
</div>
The resulting sequence is very short compared to the bisection method:
<div class="highlight">
<pre><code class="language-clike">1.500000000000000
1.416666666666667
1.414215686274510
1.414213562374690
1.414213562373095
1.414213562373095
1.414213562373095</code></pre>
</div>
<p>The Newton method is very efficient: it has quadratic convergence which means the square of the error at one iteration is proportional to the error at the next iteration. It is the basis of many powerful numerical methods (such as?)</p>
<p>If <span class="math inline">\(f\)</span> is not smooth or computing derivative is not always available, we need to approximate the tangent at one point with a secant through two points. This is called a <em>Secant Method</em>:</p>
<p><span id="eq:optimisation:secant"><span class="math display">\[ f'(x) \approx \frac{f(x_n) - f(x_{n-1})}{x_n - x_{n-1}}.\qquad(5)\]</span></span></p>
<p>In the Secant method, two points are used at each iteration, the <em>Inverse Quadratic Interpolation</em> (IQI) method then uses three points to do that. DETAIL. Benefit/Limit.</p>
</section>
<section class="level3" id="brents-method">
<h3>Brent’s Method</h3>
<p>The <em>Brent’s Method</em> is generally considered the best of the root-finding routines. It combines the robustness of Bisection methods, and the iteration speed of Secant and IQI methods. The idea is to use the fast algorithm if possible, and turn to the slow but reliable method when in doubt.</p>
<p>A description of this method.</p>
<p>This method is what we use in the examples in “Math” chapter. We have also implemented it in Owl. (Paste the code if necessary, but for now just keep the root-finding section short)</p>
</section>
</section>
<section class="level2" id="univariate-function-optimisation">
<h2>Univariate Function Optimisation</h2>
<p>Now that we have briefly introduced how root-finding works and some classic methods, let’s move on to the main topic of this chapter: unconstrained optimisation problems. Let’s start with the simple case that only one variable in the objective function. We will introduce the optimisation methods for multivariate functions in the next section, and they all apply to the univariate case, but the specific algorithms can work faster. Besides, understanding the optimisation of univariate functions can be a good step before getting to know the multivariate ones.</p>
<section class="level3" id="use-derivatives">
<h3>Use Derivatives</h3>
<p>If a function is continuous and differentiable, then one obvious solution to find extreme values is to locate where the derivatives equals 0:</p>
<p><span class="math display">\[f'(x) = 0\]</span></p>
<p>This leads us back to our root finding solutions. If you already know the analytical form of <span class="math inline">\(f'(x)\)</span>, it’s good. For example, if <span class="math inline">\(f(x) = x^2 - x\)</span>, then you can directly find root for <span class="math inline">\(g(x) = 2x-1\)</span>. Otherwise, you can use the differentiation functions in owl. Let’s look at an example. The objective function is in a hump shape:</p>
<p><span class="math display">\[f(x) = \frac{1}{(x-0.3)^2 + 0.01} + \frac{1}{(x-0.9)^2 + 0.04} -6\]</span></p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.D

let f x = Maths.(
    (F 1.) / ((x - F 0.3) ** (F 2.) + F 0.01) + 
    (F 1.) / ((x - F 0.9) ** (F 2.) + F 0.04) - F 6.)

let g = diff f

let f' x = f (F x) |&gt; unpack_flt 

let g' x = g (F x) |&gt; unpack_flt </code></pre>
</div>
<p>Visualise the image:</p>
<div class="highlight">
<pre><code class="language-ocaml">
let _ = 
  let h = Plot.create ~m:1 ~n:2 "plot_hump.png" in
  Plot.set_pen_size h 1.5;
  Plot.subplot h 0 0;
  Plot.plot_fun ~h f' 0. 2.;
  Plot.set_ylabel h "f(x)";
  Plot.subplot h 0 1;
  Plot.plot_fun ~h g' 0. 2.;
  Plot.set_ylabel h "f'(x)";
  Plot.output h</code></pre>
</div>
<figure>
<img alt="" style="width:90.0%" id="fig:optimisation:hump" src="images/optimisation/plot_hump.png"><figcaption>Figure 1: The hump function and its derivative function</figcaption>
</figure>
<p>And then you can find the extreme values using the root finding algorithm, such as Brent’s:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Owl_maths_root.brent g' 0. 0.4;;
&gt;- : float = 0.30037562625819042
Owl_maths_root.brent g' 0.4 0.7;;
&gt;- : float = 0.63700940626897
Owl_maths_root.brent g' 0.7 1.0;;
&gt;- : float = 0.892716303287079405
</code></pre>
</div>
<p>The issue is that you cannot be certain which is maximum and which is minimum.</p>
</section>
<section class="level3" id="golden-section-search">
<h3>Golden Section Search</h3>
<p>Here we face the similar question again: what if computing derivative of the function is difficult or not available? That leads us to some search-based approach. We have seen how we can keep reducing a pair of range to find the root of a function. A close analogue in optimisation is also a search method called <em>Gold Section Search</em>. It’s an optimisation method that does not require calculating derivatives. It is one choice to do optimisation if your function has a discontinuous first or second derivative.</p>
<p>The basic idea is simple. It also relies on keep reducing a “range” until it is small enough. The difference is that, instead of using only two numbers, this search method uses three numbers: <code>[a, b, c]</code>. It contains two ranges: <code>[a,b]</code> and <code>[b, c]</code>. For every iteration, we need to find a new number <code>d</code> within one of the the two ranges. For example, if we choose the <code>d</code> within <code>[b, c]</code>, and if <span class="math inline">\(f(b) &gt; f(d)\)</span>, then the new triplet becomes <code>[b, d, c]</code>, otherwise the new triplet is chosen as <code>[a, b, d]</code>. With the approach, the range of this triplet keep reducing until it is small enough and the minimum value can thus be found.</p>
<p>Then the only question is: how to choose the suitable <code>d</code> point at each step. This approach first chooses the larger the two ranges, either <code>[a, b]</code> or <code>[b, c]</code>. And then instead of choosing the middle point in that range, it uses the fractional distance 0.38197 from the central point of the triplet. The name comes from the ratio and length of range is closely related with the golden ratio. This method is slow but robust. It guarantees that each new iteration will bracket the minimum to an range just 0.61803 times the size of the previous one.</p>
</section>
</section>
<section class="level2" id="multivariate-function-optimisation">
<h2>Multivariate Function Optimisation</h2>
<p>The methods for univariate scenarios can be extended to solving multivariate optimisation problems. The analogue of derivative here is a ndarray called <em>gradient</em>. Similarly, you have two options: to use gradient, or not.</p>
<section class="level3" id="nelder-mead-simplex-method">
<h3>Nelder-Mead Simplex Method</h3>
<p>First, similar to the Golden Section Search or Brent’s, you can always opt for a non-gradient method, which is as slow as it is robust. One such method we can use is the <em>Nelder-Mead Simplex Method</em>. As its name shows, it is probably the simplest way to minimize a fairly well-behaved function. It simply goes downhill in a straightforward way, without special assumptions about the objective function.</p>
<p>EXPLAIN the algorithm in detail, perhaps with illustration. According to the Numerical Recipe, “the downhill simplex method now takes a series of steps, most steps just moving the point of the simplex where the function is largest (“highest point”) through the opposite face of the simplex to a lower point. These steps are called reflections, and they are constructed to conserve the volume of the simplex (and hence maintain its nondegeneracy). When it can do so, the method expands the simplex in one or another direction to take larger steps. When it reaches a “valley floor”, the method contracts itself in the transverse direction and tries to ooze down the valley."</p>
<p>There are some other method that does not rely on computing gradients such as Powell’s method. If the function is kind of smooth, this method can find the direction in going downhill, but instead of computing gradient, it relies on a one-dimensional optimisation method to do that, and therefore faster than the simplex method. We will not talk about this method in detail.</p>
</section>
<section class="level3" id="gradient-descent-methods">
<h3>Gradient Descent Methods</h3>
<p>A <em>descent method</em> is an iterative optimisation process. The idea is to start from a initial value, and then find a certain <em>search direction</em> along a function to decrease the value by certain <em>step size</em> until it converges to a local minimum. This process can be illustrated in fig.&nbsp;2(<a href="https://cedar.buffalo.edu/~srihari/talks/NCC1.ML-Overview.pdf">source</a>).</p>
<figure>
<img alt="" style="width:80.0%" id="fig:optimisation:gradient" src="images/optimisation/gradient.png"><figcaption>Figure 2: Reach the local minimum by iteratively moving downhill</figcaption>
</figure>
<p>Therefore, we can describe the <span class="math inline">\(n\)</span>-th iteration of descent method as:</p>
<ol type="1">
<li>calculate a descent direction <span class="math inline">\(d\)</span>;</li>
<li>choose a step size <span class="math inline">\(\alpha\)</span>;</li>
<li>update the location: <span class="math inline">\(x_{n+1} = x_n + \alpha~d\)</span>.</li>
</ol>
<p>Repeat this process until a stopping condition is met, such as the update is smaller than a threshold.</p>
<p>Among the descent methods, the <em>Gradient Descent</em> method is one of the most widely used algorithms to perform optimisation and the most common way to optimize neural networks. we will talk about it in the Neural Network chapter.</p>
<p>Based on this process, Gradient Descent method uses the function gradient to decide its direction <span class="math inline">\(d\)</span>. The precess can be described as:</p>
<ol type="1">
<li>calculate a descent direction <span class="math inline">\(-\nabla~f(x_n)\)</span>;</li>
<li>choose a step size <span class="math inline">\(\alpha\)</span>;</li>
<li>update the location: <span class="math inline">\(x_{n+1} = x_n + \alpha~\nabla~f(x_n)\)</span>.</li>
</ol>
<p>Here <span class="math inline">\(\nabla\)</span> denotes the gradient, and the distance <span class="math inline">\(\alpha\)</span> it moves along certain direction is also called <em>learning rate</em>. In a gradient descent process, when looking for the minimum, the point always follow the direction that is against the direction (represented by the negative gradient)</p>
<p>We can easily implement this process with the algorithmic differentiation module in Owl. Let’s look at an example. Here we use define the <a href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock function</a> which is usually used as performance test for optimisation problems. The function is defined as:</p>
<p><span id="eq:optimisation:rosenbrock"><span class="math display">\[f(x, y) = (a - x)^2 + b(y-x^2)^2.\qquad(6)\]</span></span></p>
<p>The parameters are usually set as <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=100\)</span>.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.D
module N = Dense.Ndarray.D

let rosenbrock a =
    let x = Mat.get a 0 0 in 
    let y = Mat.get a 0 1 in
    Maths.( (F 100.) * (y - (x ** (F 2.))) ** (F 2.) + (F 1. - x) ** (F 2.) |&gt; sum')</code></pre>
</div>
<p>Now we hope to apply the gradient descent method and observe the optimisation trajectory.</p>
<div class="highlight">
<pre><code class="language-ocaml">let a = N.of_array [|2.; -0.5|] [|1; 2|] 
let traj = ref (N.copy a)
let a = ref a
let eta = 0.0001
let n = 200</code></pre>
</div>
<p>As preparation, we use the initial starting point <code>[2, -0.5]</code>. The step size <code>eta</code> is set to <code>0.0001</code>, and the iteration number is 100. Then we can perform the iterative descent process. You can also run this process in a recursive manner.</p>
<div class="highlight">
<pre><code class="language-ocaml">let _ =
  for i = 1 to n - 1 do
    let u = grad rosenbrock (Arr !a) |&gt; unpack_arr in 
    a := N.(sub !a (scalar_mul eta u));
    traj := N.concatenate [|!traj; (N.copy !a)|]
  done</code></pre>
</div>
<p>We apply the <code>grad</code> method on the Rosenbrock function iteratively, and the updated data <code>a</code> is stored in the <code>traj</code> array. Finally, let’s visualise the trajectory of the optimisation process.</p>
<div class="highlight">
<pre><code class="language-text">let _ =
    let a, b = Dense.Matrix.D.meshgrid (-2.) 2. (-1.) 3. 50 50 in
    let c = N.(scalar_mul 100. (pow_scalar (sub b (pow_scalar a 2.)) 2.) + (pow_scalar (scalar_sub 1. a) 2.)) in

    let h = Plot.create ~m:1 ~n:2 "plot_gradients.png" in
    Plot.subplot h 0 0;
    Plot.(mesh ~h ~spec:[ NoMagColor ] a b c);

    Plot.subplot h 0 1;
    Plot.contour ~h a b c;

    let vx = N.get_slice [[]; [0]] !traj in
    let vy = N.get_slice [[]; [1]] !traj in
    Plot.plot ~h vx vy;
    Plot.output h</code></pre>
</div>
<p>We first create a meshgrid based on the Rosenbrock function to visualise the 3D image, and then on the 2D contour image of the same function we plot how the result of the optimisation is updated, from the initial starting porint towards a local minimum point. The visualisation results are shown in fig.&nbsp;3. On the right figure the black line shows the moving trajectory. You can image it moving downwards along the slope in the right side figure.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:optimisation:gd_rosenbrock" title="gd_rosenbrock" src="images/optimisation/gd_rosenbrock.png"><figcaption>Figure 3: Optimisation process of gradient descent on multivariate function</figcaption>
</figure>
<p>In Owl we provide a <code>minimise_fun</code> function to do that.</p>
<div class="highlight">
<pre><code class="language-clike">val minimise_fun :  ?state:Checkpoint.state -&gt; Params.typ -&gt; (t -&gt; t) -&gt; t  -&gt; Checkpoint.state * t</code></pre>
</div>
<p>This function minimises <code>f : x -&gt; y</code> w.r.t <code>x</code>; <code>x</code> is a ndarray, and <code>y</code> is a scalar value. This function is implemented using gradient descent.</p>
<p>EXPLAIN in detail, such as checkpoint etc.</p>
<p>TODO: explain, perhaps with a bit theory or visual aid, to show why gradient descent is much more efficient that the previous non-gradient methods.</p>
</section>
<section class="level3" id="conjugate-gradient-method">
<h3>Conjugate Gradient Method</h3>
<p>One problem with the Gradient Descent is that it does not perform well on all functions. For example, if the function forms a steep and narrow value, then using gradient descent will take many small steps to reach the minimum, even if the function is in a perfect quadratic form.</p>
<p>The <em>Conjugate Gradient</em> method can solve this problem. (HISTORY.) It is similar to Gradient Descent, but the new direction does not follow the new gradient, but somehow <em>conjugated</em> to the old gradients and to all previous directions traversed.</p>
<p>EXPLAIN in detail.</p>
<p>Instead of <span class="math inline">\(-\nabla~f(x_n)\)</span>, CG choose another way to calculate the descent direction: EQUATION of CG</p>
<p>fig.&nbsp;4 compares the</p>
<figure>
<img alt="" style="width:60.0%" id="fig:optimisation:gradients" title="gradients" src="images/optimisation/gradients.png"><figcaption>Figure 4: Compare conjugate gradient and gradient descent</figcaption>
</figure>
<p>Both GD and CG are abstracted in a module in Owl Besides the classic gradient descent and conjugate gradient, there are more methods that can be use to specify the descent direction: CD by Fletcher, NonlinearCG….</p>
<div class="highlight">
<pre><code class="language-clike">let run = function
    | GD -&gt; fun _ _ _ _ g' -&gt; Maths.neg g'
    | CG -&gt;
        fun _ _ g p g' -&gt;
          let y = Maths.(g' - g) in
          let b = Maths.(sum' (g' * y) / (sum' (p * y) + _f 1e-32)) in
          Maths.(neg g' + (b * p))
    | CD -&gt;
        fun _ _ g p g' -&gt;
          let b = Maths.(l2norm_sqr' g' / sum' (neg p * g)) in
          Maths.(neg g' + (b * p))
    ...</code></pre>
</div>
<p>Explain</p>
<p>We also Give them a brief introduction here, but refer to paper and book for more details.</p>
</section>
<section class="level3" id="newton-and-quasi-newton-methods">
<h3>Newton and Quasi-Newton Methods</h3>
<p>There is also a Newton Method in optimisation (it is not to be confused with the newton method used in root-finding). Still following the basic process of descent method, newton method starts from a initial point and then repeat the process:</p>
<ol type="1">
<li>compute the descent direction: <span class="math inline">\(d = -\frac{\nabla~f(x_n)}{\nabla^{2}~f(x_n)}\)</span></li>
<li>choose a step size <span class="math inline">\(\alpha\)</span>;</li>
<li>update the location: <span class="math inline">\(x_{n+1} = x_n + \alpha~d\)</span>.</li>
</ol>
<p>Here <span class="math inline">\(\nabla^{2}~f(x_n)\)</span> denotes the second-order derivatives of function <span class="math inline">\(f\)</span>. For a scalar-valued function, it can be represented by a square matrix called <em>Hessian matrix</em>, denoted by <span class="math inline">\(\mathbf{H}\)</span>. Therefore the update process of newton method can be expressed as:</p>
<p><span class="math display">\[x_{n+1} = x_n - \alpha~\mathbf{H_n}^{-1}\nabla~f(x_n).\]</span></p>
<p>This can also be implemented in Owl with algorithmic differentiation.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Owl
open Algodiff.D

let rec newton ?(eta=F 0.01) ?(eps=1e-6) f x =
  let g, h = (gradhessian f) x in
  if (Maths.l2norm' g |&gt; unpack_flt) &lt; eps then x
  else newton ~eta ~eps f Maths.(x - eta * g *@ (inv h))

let _ =
  let f x = Maths.(cos x |&gt; sum') in
  let y = newton f (Mat.uniform 1 2) in
  Mat.print y</code></pre>
</div>
<p>Once nice property about the newton’s method is its rate of convergence: it converges quadratically. However, one big problem with the newton method is the problem size. In the real world applications, it is not rare to see optimisation problems with thousands, millions or more variants. In these cases, it is impractical to compute the Hessian matrix, not to mention its inverse.</p>
<p>Towards this end, the <em>Quasi-newton</em> methods are proposed. The basic idea is to iteratively build up an approximation of the inverse of Hessian matrix. Their convergence is fast, but not as efficient as newton method. It takes about <span class="math inline">\(n\)</span> quasi-newton iterations to progress similarly as the newton method. The most important method in this category is BFGS (Broyden-Fletcher-Goldfarb-Shanno), named after its four authors.</p>
<p>EXPLAIN briefly.</p>
<p>The Limited-BFGS (L-BFGS) address the memory usage issue in BFGS. Instead of propagating updates over all iterations, this method only keeps updates from the last <span class="math inline">\(m\)</span> iterations.</p>
</section>
</section>
<section class="level2" id="global-optimisation-and-constrained-optimisation">
<h2>Global Optimisation and Constrained Optimisation</h2>
<p>This chapter mainly focuses on unconstrained optimisation, mostly to find local optimal. In the rest of this chapter we will give a very very brief introduction to global optimisation and constrained optimisation</p>
<p>The basic idea of global optimisation is to provide effective search methods and heuristics to traverse the search space effectively. One method is to start from sufficient number of initial points and find the local optimal, the choose the smallest/largest value from them. Another heuristic is to try stepping away from a local optimal value by taking a finite amplitude step away from it, perform the optimisation method, and see if it leads to a better solution or still the same.</p>
<p>One example of algorithm: Simulated Annealing Methods.</p>
<p>The constrained optimisation is another large topic we haven’t covered in this chapter.</p>
<p>Application of linear programming and non-linear programming.</p>
<p>Their current status (how difficult to solve etc.), classic methods, and some existing tools.</p>
<p>Their connection with the unconstrained method we have introduced.</p>
<p>Refer to these book for more detail.</p>
</section>
<section class="level2" id="references">
<h2>References</h2>
<p>REFERENCE BOOK in writing: Numerical methods in engineering with Python 3, by Jaan Kiusalaas.</p>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-boyd2004convex">
<p>Boyd, Stephen, and Lieven Vandenberghe. 2004. <em>Convex Optimization</em>. Cambridge university press.</p>
</div>
<div id="ref-fletcher2013practical">
<p>Fletcher, Roger. 2013. <em>Practical Methods of Optimization</em>. John Wiley &amp; Sons.</p>
</div>
</div>
</section>
</section>
</article></div><a href="regression.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 13</small>Regression</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>