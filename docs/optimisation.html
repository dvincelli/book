<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Optimisation - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="optimisation">
<h1>Optimisation</h1>
<p>(The basic idea of this chapter: give a general introduction of this topic regardless of Owl; when Owl has corresponding function, we provide a simple example.)</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Mathematical optimization deals with the problem of finding numerically minimums, maximums or zeros of a function. In this context, the function is called cost function, or objective function.</p>
<p>REFERENCE: Practical Methods of Optimization by Fletcher.</p>
<p>Basic theories.</p>
</section>
<section class="level2" id="scalar-functions">
<h2>Scalar Functions</h2>
</section>
<section class="level2" id="multivariate-function">
<h2>Multivariate Function</h2>
<section class="level3" id="gradient-descent-method">
<h3>Gradient Descent Method</h3>
</section>
<section class="level3" id="conjugate-gradient-method">
<h3>Conjugate Gradient Method</h3>
</section>
<section class="level3" id="newton-method">
<h3>Newton Method</h3>
</section>
<section class="level3" id="l-bfgs">
<h3>L-BFGS</h3>
</section>
<section class="level3" id="other-algorithms">
<h3>Other Algorithms</h3>
</section>
</section>
<section class="level2" id="root-finding">
<h2>Root Finding</h2>
</section>
<section class="level2" id="optimisation-in-practice-training-neural-network">
<h2>Optimisation in Practice: Training Neural Network</h2>
<p>Adagrad etc in action; where do they fit in the previous theory?</p>
<p>NOTE: perhaps also add in global optimisation section. Linear Programming?â€¦</p>
<p>Also, I need to at least explain what is hessian/jacobian etc. in this chapter.</p>
</section>
</section>
</article></div><a href="regression.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 13</small>Regression</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>