<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Case - Neural Style Transfer - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="case---neural-style-transfer">
<h1>Case - Neural Style Transfer</h1>
<p>What is Neural Style Transfer (NST)? It is a pretty cool application of Deep Neural Networks (DNN), “the process of using DNN to migrate the semantic content of one image to different styles”.</p>
<p>The process is actually very simple, as the title image shows, this application takes two images A and B as input. Let’s say A is “Mona Lisa” of Da Vinci, and B is “The Starry Night” of Vincent van Gogh.</p>
<p>We then specify A as the content image and B as the style image, then what a NST application can produce? Boom! A new Mona Lisa, but with the style of Van Gogh (see the middle of title image)! If you want another style, just replace image B and run the application again. Impressionism, abstractionism, classical art, you name it.</p>
<p>The figure below illustrate this point. You can apply different art styles to the same street view, or apply the same “Starry Sky” style to any pictures. Isn’t it amazing?</p>
<figure>
<img alt="" id="fig:case-nst:example_01" src="images/case-nst/nst_example.png"><figcaption>Figure 1: Example of applying neural style transfer on a street view picture</figcaption>
</figure>
<section class="level2" id="content-and-style-reconstruction">
<h2>Content and Style Reconstruction</h2>
<p><span data-cites="gatys2015neural" class="citation">(Gatys, Ecker, and Bethge 2015)</span> first propose to use DNN to let programmes to create artistic images of high perceptual quality. The examples above may look like magic, but surely its not. In this section, we will first introduce the intuition about how the neural style transfer algorithm works. For more formal and detailed introduction, please visit the <a href="https://arxiv.org/abs/1508.06576">original paper</a>.</p>
<p>The basic idea is plain: we want to get an image whose content is similar to one image and its artistic style close to the other image. Of course, to make the algorithm work, first we need to express this sentence in mathematical form so that computers can understand it. Let’s assume for a moment we have already know that, then style transfer can be formalised as an optimisation problem. Given a content image <code>c</code> and a style image <code>s</code>, our target is to get an output image <code>x</code> so that it minimises:</p>
<p><span class="math display">\[f(x) = \verb|content_distance|(x, c) + \verb|style_distance|(x, s)\]</span></p>
<p>Here the “distance” between two feature map is calculated by the euclidean distance between the two ndarrays.</p>
<p>You may remember from the regression or neural network chapters that training process is also an optimisation process. However, do not mistake the optimisation in NST as regression or DNN training. For the latter one, there is the function <span class="math inline">\(f_w\)</span> that contains parameter <span class="math inline">\(w\)</span> and the training process optimise <span class="math inline">\(w\)</span> to minimise <span class="math inline">\(f_w(x)\)</span>. The optimisation in NST is more like the traditional optimisation problem: we have a function <span class="math inline">\(f\)</span>, and we start we an initial input <span class="math inline">\(x_0\)</span> and update it iteratively until we have satisfying <span class="math inline">\(x\)</span> that minimise the target function.</p>
<p>Now we can comeback to the key problem. While we human beings can kind of feel the style of a paint and visually recognise the contents in a picture, how can we mathematically express the “content” and the “style” of an image? That’s where the convolution network comes to help. DNNs, especially the ones that are used for computer vision tasks, are found to be an convenient tool to capture the characteristics of an image. We have demonstrate in the previous chapter how CNNs are good at spotting the “features” in an image layer by layer. Therefore, in the next two sub-sections, we will explain how it can be used to express the content and style feature of an image.</p>
<p>We have introduced several CNN architectures to perform image detection task in the previous chapter. We choose to use VGG19 since its follows a simple linear stack structure and is proved to have good performance. We have built the VGG19 network structure in <a href="https://gist.github.com/jzstark/da5cc7f771bc8d9699cedc491b23f856">this gist</a>. It contains 38 layers in total.</p>
<section class="level3" id="content-reconstruction">
<h3>Content Reconstruction</h3>
<p>From the image detection case, we know that the CNN extract features layer by layer until the features are so abstract that it can give an answer such as “this is a car” “this is an apple” etc. Therefore, we can use the feature map to reconstruct content of an image.</p>
<p>But which layer’s output should we use as a suitable indication of the image content? Let’s perform a simplified version of NST: we only care about re-constructing the content of the input image, so our target is to minimise:</p>
<p><span class="math display">\[f(x) = \verb|content_distance|(x, c)\]</span></p>
<p>As an example, we use fig.&nbsp;2 as the target content. (This image <a href="https://ccsearch.creativecommons.org/photos/f4024dc8-ce39-4e86-acfd-47532fef824d">“Tourists in Nanzen-Ji Hojo”</a> by blieusong is licensed under CC BY-SA 2.0.)</p>
<figure>
<img alt="" style="width:40.0%" id="fig:case-nst:content-example" title="hojo tourists" src="images/case-nst/hojo.png"><figcaption>Figure 2: Example content image in neural style transfer</figcaption>
</figure>
<p>Suppose we choose the output of <code>idx</code> layer as the chosen feature map to represent the content. First, we need to compute the target feature map:</p>
<div class="highlight">
<pre><code class="language-clike">let fill_content_targets x net = 
  let selected_topo = Array.sub nn.topo 0 (idx + 1) in 
  run' selected_topo x</code></pre>
</div>
<p>The function <code>fill_content_targets</code> takes the content image and the VGG network as input, and returns the target feature map as output. We only need to compute the feature map of the target content image once.</p>
<p>Here the <code>run'</code> function is implemented by accumulating the inference result along the selected part of the network, from the network input until the chosen layer, instead of processing the whole network:</p>
<div class="highlight">
<pre><code class="language-clike">let run' topo x =
  let last_node_output = ref (F 0.) in
  Array.iteri (fun i n -&gt;
    let input  = if i = 0 then x else !last_node_output in 
    let output = run [|input|] n.neuron in
    last_node_output := output;
  ) topo;
  !last_node_output</code></pre>
</div>
<p>Then we can start optimising the input image <code>x</code>. Let’s set the initial <code>x</code> to be a “white noise” image that only contains random pixels. This image has the same shape as content image.</p>
<div class="highlight">
<pre><code class="language-text">let input_shape = Dense.Ndarray.S.shape content_img in
Dense.Ndarray.S.(gaussian input_shape |&gt; scalar_mul 0.256)</code></pre>
</div>
<p>The feature map of the input image <code>x</code> is still calcuated using the same process show in function <code>fill_content_targets</code>. We call the resulting feature map <code>response</code>, then the loss value can be calculated with the L2Norm of the difference between two feature maps, and then normalised with the feature map size.</p>
<div class="highlight">
<pre><code class="language-clike">let c_loss response target = 
  let loss = Maths.((pow (response - target) (F 2.)) |&gt; sum') in 
  let _, h, w, feature = get_shape target in 
  let c = float_of_int ( feature * h * w ) in
  Maths.(loss / (F c))</code></pre>
</div>
<p>Once the loss value is calculated, we can apply optimisers. Here we use the <code>minimise_fun</code> from <code>Optimise</code> module. The target function can be described as:</p>
<div class="highlight">
<pre><code class="language-clike">let g x = 
  let response = fill_losses x in 
  c_loss response target</code></pre>
</div>
<p>All it performs is what we just described: first calculating the feature map <code>response</code> of input image at a certain layer, and then compute the distance between it and the target content feature map as loss value.</p>
<p>Finally, we can perform the optimisation iterations:</p>
<div class="highlight">
<pre><code class="language-clike">let state, img = Optimise.minimise_fun params g (Arr input_img) in
let x' = ref img in
while Checkpoint.(state.current_batch &lt; state.batches) do
  Checkpoint.(state.stop &lt;- false);
  let a, img = Optimise.minimise_fun ~state params g !x' in 
  x' := img
done;</code></pre>
</div>
<p>We keep updating the image <code>x</code> for fixed number of iterations. Particularly, we use the Adam adaptive learning rate method, for it proves to be quite effective in style transfer optimisation:</p>
<div class="highlight">
<pre><code class="language-clike">let params = Params.config
    ~learning_rate:(Learning_Rate.Adam (learning_rate, 0.9, 0.999))
    ~checkpoint:(Checkpoint.Custom chkpt)
    iter</code></pre>
</div>
<p>Using the process above, we return to the problem of choosing a suitable layer as the indication of the image content. In this 38-layer VGG network, we choose these layers: 2, 7, 12, 21, 30. Then we can compare the optimisation result to see the effect of image reconstruction. Each one is generated using 100 iterations.</p>
<p>IMAGE: a 1x5 images</p>
<p>It is shown that, the content information is kept accurate at the lower level. Along the processing hierarchy of the network, feature map produced by the lower layer cares more about the small features that at the pixel level, while the higher layer gives more abstract information but less details to help with content reconstruction.</p>
</section>
<section class="level3" id="style-recreation">
<h3>Style Recreation</h3>
<p>Then similarly, we explore the other end of this problem: we only care about recreating an image with only the style of an input image:</p>
<p>EQUATION: minimise style(I_style)</p>
<p>We use this image as input style target:</p>
<figure>
<img alt="" style="width:50.0%" id="fig:case-nst:style-example" title="hokusai" src="images/case-nst/hokusai.png"><figcaption>Figure 3: Example style image in neural style transfer</figcaption>
</figure>
<p>Expressing the style is bit more complex that content, which directly uses the feature map it self.</p>
<p>Some theory: why Gram etc.</p>
<p>We try different combinations of features. 1, 1 + 2, …., 1 + 2 + 3 + 4 + 5.</p>
<p>Part of the CODE.</p>
<p>The result is shown below:</p>
<p>IMAGE: 1x5</p>
<p>You can see that, contrary to content, going deeper in CNN gives more information about feature.</p>
</section>
</section>
<section class="level2" id="building-a-nst-network">
<h2>Building a NST Network</h2>
<p>Now that we have seen these two extremes, it’s straightforward to understand the theory of style transfer.</p>
<p>control the proportion with weights.</p>
<p>The result:</p>
<p>IMAGE: 1x5, from white noise, by different steps.</p>
<p>we simply this process…</p>
<p>The details: Loss function, pre-trained weight, optimiser, etc.</p>
<p>I’ve implement an NST application with Owl. All the code (about 180 lines) is included in <a href="https://gist.github.com/jzstark/6f28d54e69d1a19c1819f52c5b16c1a1">this Gist</a>. This application uses the VGG19 network structure to capture the content and style characteristics of images. The pre-trained network file is also included. It relies on <code>ImageMagick</code> to manipulate image format conversion and resizing. Please make sure it is installed before running.</p>
</section>
<section class="level2" id="running-nst">
<h2>Running NST</h2>
<p>This application provides a simple interfaces to use. Here is an example showing how to use it with two lines of code:</p>
<div class="highlight">
<pre><code class="language-clike">#zoo "6f28d54e69d1a19c1819f52c5b16c1a1"

Neural_transfer.run 
  ~ckpt:50 
  ~src:"path/to/content_img.jpg" 
  ~style:"path/to/style_img.jpg" 
  ~dst:"path/to/output_img.png" 250.;;</code></pre>
</div>
<p>The first line download gist files and imported this gist as an OCaml module, and the second line uses the <code>run</code> function to produce an output image to your designated path. It’s syntax is quite straightforward, and you may only need to note the final parameter. It specifies how many iterations the optimisation algorithm runs. Normally 100 ~ 500 iterations is good enough.</p>
<p>This module also supports saving the intermediate images to the same directory as output image every N iterations (e.g.&nbsp;<code>path/to/output_img_N.png</code>). <code>N</code> is specified by the <code>ckpt</code> parameter, and its default value is 50 iterations. If users are already happy with the intermediate results, they can terminate the program without waiting for the final output image.</p>
<p>That’s all it takes! If you don’t have suitable input images at hand, the gist already contains exemplar content and style images to get you started.</p>
<p>More examples can be seen on our <a href="http://demo.ocaml.xyz/neuraltrans.html">demo</a> page.</p>
</section>
<section class="level2" id="extending-nst">
<h2>Extending NST</h2>
<p>Many variants. Most notably:</p>
<ul>
<li>Deep Photo Style Transfer <span data-cites="luan2017deep" class="citation">(Luan et al. 2017)</span></li>
<li>Image-to-Image Translation <span data-cites="zhu2017unpaired" class="citation">(Zhu et al. 2017)</span></li>
</ul>
<p>Industry applications:</p>
<p>One of the variants is the Fast Style Transfer. Suitable for fast rendering with fixed style. We will introduce it next.</p>
</section>
<section class="level2" id="fast-style-transfer">
<h2>Fast Style Transfer</h2>
<p>Paper: <span data-cites="Johnson2016Perceptual" class="citation">(Johnson, Alahi, and Fei-Fei 2016)</span></p>
<p>One disadvantage of NST is that it could take a very long time to rendering an image, and if you want to change to another content or style image, then you have to wait a long time for the training again. If you want to render some of your best (or worst) selfies fast and send to your friends, NST is perhaps not a perfect choice.</p>
<p>This problem then leads to another application: Fast Neural Style Transfer (FST). FST sacrifice certain degrees of flexibility, which is that you cannot choose style images at will. But as a result, you only need to feed your content image to a DNN, finish an inference pass, and then the output will be the rendered styled image as you expected! The best part is that, one inference pass is much much faster that keep running a training phase.</p>
<section class="level3" id="theory">
<h3>Theory</h3>
</section>
<section class="level3" id="building-fst-network">
<h3>Building FST Network</h3>
<p>Based on the <a href="https://github.com/lengstrom/fast-style-transfer">TensorFlow implementation</a>, we have implemented a FST application in Owl, and it’s not complicated. Here is the network structure:</p>
<div class="highlight">
<pre><code class="language-ocaml">open Owl
open Neural.S
open Neural.S.Graph
open Neural.S.Algodiff
module N = Dense.Ndarray.S

(** Network Structure *)

let conv2d_layer ?(relu=true) kernel stride nn  =
  let result = 
    conv2d ~padding:SAME kernel stride nn
    |&gt; normalisation ~decay:0. ~training:true ~axis:3
  in
  match relu with
  | true -&gt; (result |&gt; activation Activation.Relu)
  | _    -&gt; result

let conv2d_trans_layer kernel stride nn = 
  transpose_conv2d ~padding:SAME kernel stride nn
  |&gt; normalisation ~decay:0. ~training:true ~axis:3
  |&gt; activation Activation.Relu

let residual_block wh nn = 
  let tmp = conv2d_layer [|wh; wh; 128; 128|] [|1;1|] nn
    |&gt; conv2d_layer ~relu:false [|wh; wh; 128; 128|] [|1;1|]
  in 
  add [|nn; tmp|]

let make_network h w = 
  input [|h;w;3|]
  |&gt; conv2d_layer [|9;9;3;32|] [|1;1|]
  |&gt; conv2d_layer [|3;3;32;64|] [|2;2|]
  |&gt; conv2d_layer [|3;3;64;128|] [|2;2|]
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; conv2d_trans_layer [|3;3;128;64|] [|2;2|]
  |&gt; conv2d_trans_layer [|3;3;64;32|] [|2;2|]
  |&gt; conv2d_layer ~relu:false [|9;9;32;3|] [|1;1|]
  |&gt; lambda (fun x -&gt; Maths.((tanh x) * (F 150.) + (F 127.5)))
  |&gt; get_network</code></pre>
</div>
</section>
<section class="level3" id="running-fst">
<h3>Running FST</h3>
<p>That’s it. Given suitable weights, running an inference pass on this DNN is all it takes to get a styled image. Like NST, we have wrapped all things up in a <a href="https://gist.github.com/jzstark/f937ce439c8adcaea23d42753f487299">Gist</a>, and provide a simple user interface to users. Here is an example:</p>
<div class="highlight">
<pre><code class="language-clike">#zoo "f937ce439c8adcaea23d42753f487299"

FST.list_styles ();; (* show all supported styles *)
FST.run ~style:1 "path/to/content_img.png" "path/to/output_img.jpg" </code></pre>
</div>
<p>The <code>run</code> function mainly takes one content image and output to a new image file, the name of which is designated by the user. The image could be of any popular formats: jpeg, png, etc. This gist contains exemplar content images for you to use.</p>
<p>Note that we did say “given suitable weights”. A set of trained weight for the FST DNN represents a unique artistic style. We have already include six different weight files for use, and the users just need to pick one of them and load them into the DNN, without worrying about how to train these weights.</p>
<p>Current we support six art styles: “<a href="https://bit.ly/2nBW0ae">Udnie</a>” by Francis Picabia, “<a href="https://bit.ly/2nKk8Hl">The Great Wave off Kanagawa</a>” by Hokusai, “<a href="https://bit.ly/2KA7FAY">Rain Princess</a>” by Leonid Afremov, “<a href="https://bit.ly/2rS1fWQ">La Muse</a>” by Picasso, “<a href="https://bit.ly/1CvJz5d">The Scream</a>” by Edvard Munch, and “<a href="https://bit.ly/2wVfizH">The shipwreck of the Minotaur</a>” by J. M. W. Turner</p>
<p>Yes, maybe six styles are not enough for you, but think about it, you can now render any of your image to a nice art style fast, maybe about half a minute, or even faster if you are using GPU or other accelerators. Here is a teaser that renders one city view image to all these amazing art styles.</p>
<p><img id="fig:case-obj-detect:example_03" src="images/case-nst/example_fst00.png"></p>
<p>If you are still not persuaded, here is our ultimate solution for you: a [demo] website, where you can choose a style, upload an image, get yourself a cup of coffee, and then checkout the rendered image. To push things even further, we apply FST to some videos frame-by-frame, and put them together to get some artistic videos, as shown in this <a href="https://www.youtube.com/watch?v=cFOM-JnyJv4&amp;list=PLGt9zVony2zVSiHZb8kwwXfcmCuOH2W-H">Youtube list</a>. And all of these are implemented in Owl.</p>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-gatys2015neural">
<p>Gatys, Leon A, Alexander S Ecker, and Matthias Bethge. 2015. “A Neural Algorithm of Artistic Style.” <em>arXiv Preprint arXiv:1508.06576</em>.</p>
</div>
<div id="ref-Johnson2016Perceptual">
<p>Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. 2016. “Perceptual Losses for Real-Time Style Transfer and Super-Resolution.” In <em>European Conference on Computer Vision</em>.</p>
</div>
<div id="ref-luan2017deep">
<p>Luan, Fujun, Sylvain Paris, Eli Shechtman, and Kavita Bala. 2017. “Deep Photo Style Transfer.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 4990–8.</p>
</div>
<div id="ref-zhu2017unpaired">
<p>Zhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 2223–32.</p>
</div>
</div>
</section>
</section>
</section>
</article></div><a href="case-gpu.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 32</small>Case - Using Accelerators</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>