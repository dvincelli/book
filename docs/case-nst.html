<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Case - Neural Style Transfer - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="case---neural-style-transfer">
<h1>Case - Neural Style Transfer</h1>
<p>What is Neural Style Transfer (NST)? It is a pretty cool application of Deep Neural Networks (DNN), “the process of using DNN to migrate the semantic content of one image to different styles”.</p>
<p>Well it may sounds a little bit scary, but the idea is very simple, as the title image shows, this application takes two images A and B as input. Let’s say A is “Mona Lisa” of Da Vinci, and B is “The Starry Night” of Vincent van Gogh.</p>
<p>We then specify A as the content image and B as the style image, then what a NST application can produce? Boom! A new Mona Lisa, but with the style of Van Gogh (see the middle of title image)! If you want another style, just replace image B and run the application again. Impressionism, abstractionism, classical art, you name it.</p>
<p>The figure below illustrate this point (<a href="http://genekogan.com/works/style-transfer/">src</a>). You can apply different art styles to the same “Mona Lisa”, or apply the same “Starry Sky” style to any pictures, even a normal daily street view. Isn’t it amazing?</p>
<figure>
<img alt="" id="fig:case-nst:mona-lisa" title="Mona Lisa" src="images/case-nst/mona_lisa.png"><figcaption>Figure 1: Mona Lisa</figcaption>
</figure>
<section class="level2" id="theory">
<h2>Theory</h2>
<p>Based on the original paper <span data-cites="gatys2015neural" class="citation">(Gatys, Ecker, and Bethge 2015)</span>. Without going into details, I will briefly introduce the math behind NST, so please feel free to ignore this part. Refer to the <a href="https://arxiv.org/abs/1508.06576">original paper</a> for more details if you are interested.</p>
<p>The basic idea:</p>
<p>But how do we express that?</p>
<p>The answer is gradient descent Remember from the regression and neural networks. Our goal is to train a model to minimise the difference between prediction and known label.</p>
<p>But instead of weight, this time we need to minimise the input. Use equation <code>f(x)</code> to show this point.</p>
<p>The NST can be seen as an optimisation problem: given a content image <code>c</code> and a style image <code>s</code>. Suppose we know the ways to extract the style and content of an image, then our target is to get an output image <code>x</code> so that it minimises:</p>
<p><span class="math display">\[f(x) = \verb|content_distance|(x, c) + \verb|style_distance|(x, s)\]</span></p>
<p>This equation can be easily translated as: I want to get such an image that its content is close to <code>c</code> , but its style similar to <code>s</code> .</p>
<p>Now comes the problem: we can kind of feel the style of a paint, we can visually recognise the contents in a picture, but how can we calculate them mathematically? For that we need the help of convolution neural networks. DNNs, especially the ones that are used for computer vision tasks, are found to be an convenient tool to capture the content and style characteristics of an image (details emitted here for now). Then the euclidean distance of these characteristics are used to express the <code>content_distance()</code> and <code>style_distance()</code> functions.</p>
<section class="level3" id="content-recreation">
<h3>Content Recreation</h3>
<p>From the image detection case, we know that the CNN extract features layer by layer until the features are so abstract that it can give an answer such as “this is a car” “this is an apple” etc.</p>
<p>Explain why we choose certain layers to express content.</p>
<p>As an example, we can perform a simplified version of NST: we only care about re-recreating the content of the input image:</p>
<p>EQUATION: minimise content(I_content)</p>
<p>We use this image as input content target:</p>
<p>IMAGE</p>
<p>Part of the CODE.</p>
<p>We start with a white noise image, and then try the effect of choosing features from different layers.</p>
<p>IMAGE: a 1x5 images</p>
<p>It is shown that, the content information is kept accurate at the lower level.</p>
<p>Explain a bit of the theory behind this phenomena.</p>
</section>
<section class="level3" id="style-recreation">
<h3>Style Recreation</h3>
<p>Then similarly, we explore the other end of this problem: we only care about recreating an image with only the style of an input image:</p>
<p>EQUATION: minimise style(I_style)</p>
<p>We use this image as input style target:</p>
<p>IMAGE</p>
<p>Expressing the style is bit more complex that content, which directly uses the feature map it self.</p>
<p>Some theory: why Gram etc.</p>
<p>We try different combinations of features. 1, 1 + 2, …., 1 + 2 + 3 + 4 + 5.</p>
<p>Part of the CODE.</p>
<p>The result is shown below:</p>
<p>IMAGE: 1x5</p>
<p>You can see that, contrary to content, going deeper in CNN gives more information about feature.</p>
</section>
</section>
<section class="level2" id="building-a-nst-network">
<h2>Building a NST Network</h2>
<p>Now that we have seen these two extremes, it’s straightforward to understand the theory of style transfer.</p>
<p>control the proportion with weights.</p>
<p>The result:</p>
<p>IMAGE: 1x5, from white noise, by different steps.</p>
<p>we simply this process…</p>
<p>The details: Loss function, pre-trained weight, optimiser, etc.</p>
<p>I’ve implement an NST application with Owl. All the code (about 180 lines) is included in <a href="https://gist.github.com/jzstark/6f28d54e69d1a19c1819f52c5b16c1a1">this Gist</a>. This application uses the VGG19 network structure to capture the content and style characteristics of images. The pre-trained network file is also included. It relies on <code>ImageMagick</code> to manipulate image format conversion and resizing. Please make sure it is installed before running.</p>
</section>
<section class="level2" id="running-nst">
<h2>Running NST</h2>
<p>This application provides a simple interfaces to use. Here is an example showing how to use it with two lines of code:</p>
<div class="highlight">
<pre><code class="language-clike">#zoo "6f28d54e69d1a19c1819f52c5b16c1a1"

Neural_transfer.run 
  ~ckpt:50 
  ~src:"path/to/content_img.jpg" 
  ~style:"path/to/style_img.jpg" 
  ~dst:"path/to/output_img.png" 250.;;</code></pre>
</div>
<p>The first line download gist files and imported this gist as an OCaml module, and the second line uses the <code>run</code> function to produce an output image to your designated path. It’s syntax is quite straightforward, and you may only need to note the final parameter. It specifies how many iterations the optimisation algorithm runs. Normally 100 ~ 500 iterations is good enough.</p>
<p>This module also supports saving the intermediate images to the same directory as output image every N iterations (e.g.&nbsp;<code>path/to/output_img_N.png</code>). <code>N</code> is specified by the <code>ckpt</code> parameter, and its default value is 50 iterations. If users are already happy with the intermediate results, they can terminate the program without waiting for the final output image.</p>
<p>That’s all it takes! If you don’t have suitable input images at hand, the gist already contains exemplar content and style images to get you started. I have to say I had a lot lot of fun playing with it – please allow me to introduce you one of my work using the exemplar images:</p>
<figure>
<img alt="" id="fig:case-nst:example_01" src="images/case-nst/nst_example.png"><figcaption>Figure 2: Example of applying neural style transfer on a street view picture</figcaption>
</figure>
<p>More examples can be seen on our <a href="http://demo.ocaml.xyz/neuraltrans.html">demo</a> page.</p>
</section>
<section class="level2" id="extending-nst">
<h2>Extending NST</h2>
<p>Many variants. Most notably:</p>
<ul>
<li>Deep Photo Style Transfer <span data-cites="luan2017deep" class="citation">(Luan et al. 2017)</span></li>
<li>Image-to-Image Translation <span data-cites="zhu2017unpaired" class="citation">(Zhu et al. 2017)</span></li>
</ul>
<p>Industry applications:</p>
<p>One of the variants is the Fast Style Transfer. Suitable for fast rendering with fixed style. We will introduce it next.</p>
</section>
<section class="level2" id="fast-style-transfer">
<h2>Fast Style Transfer</h2>
<p>Paper: <span data-cites="Johnson2016Perceptual" class="citation">(Johnson, Alahi, and Fei-Fei 2016)</span></p>
<p>One disadvantage of NST is that it could take a very long time to rendering an image, and if you want to change to another content or style image, then you have to wait a long time for the training again. If you want to render some of your best (or worst) selfies fast and send to your friends, NST is perhaps not a perfect choice.</p>
<p>This problem then leads to another application: Fast Neural Style Transfer (FST). FST sacrifice certain degrees of flexibility, which is that you cannot choose style images at will. But as a result, you only need to feed your content image to a DNN, finish an inference pass, and then the output will be the rendered styled image as you expected! The best part is that, one inference pass is much much faster that keep running a training phase.</p>
<section class="level3" id="theory-1">
<h3>Theory</h3>
</section>
<section class="level3" id="building-fst-network">
<h3>Building FST Network</h3>
<p>Based on the <a href="https://github.com/lengstrom/fast-style-transfer">TensorFlow implementation</a>, we have implemented a FST application in Owl, and it’s not complicated. Here is the network structure:</p>
<div class="highlight">
<pre><code class="language-ocaml">open Owl
open Neural.S
open Neural.S.Graph
open Neural.S.Algodiff
module N = Dense.Ndarray.S

(** Network Structure *)

let conv2d_layer ?(relu=true) kernel stride nn  =
  let result = 
    conv2d ~padding:SAME kernel stride nn
    |&gt; normalisation ~decay:0. ~training:true ~axis:3
  in
  match relu with
  | true -&gt; (result |&gt; activation Activation.Relu)
  | _    -&gt; result

let conv2d_trans_layer kernel stride nn = 
  transpose_conv2d ~padding:SAME kernel stride nn
  |&gt; normalisation ~decay:0. ~training:true ~axis:3
  |&gt; activation Activation.Relu

let residual_block wh nn = 
  let tmp = conv2d_layer [|wh; wh; 128; 128|] [|1;1|] nn
    |&gt; conv2d_layer ~relu:false [|wh; wh; 128; 128|] [|1;1|]
  in 
  add [|nn; tmp|]

let make_network h w = 
  input [|h;w;3|]
  |&gt; conv2d_layer [|9;9;3;32|] [|1;1|]
  |&gt; conv2d_layer [|3;3;32;64|] [|2;2|]
  |&gt; conv2d_layer [|3;3;64;128|] [|2;2|]
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; conv2d_trans_layer [|3;3;128;64|] [|2;2|]
  |&gt; conv2d_trans_layer [|3;3;64;32|] [|2;2|]
  |&gt; conv2d_layer ~relu:false [|9;9;32;3|] [|1;1|]
  |&gt; lambda (fun x -&gt; Maths.((tanh x) * (F 150.) + (F 127.5)))
  |&gt; get_network</code></pre>
</div>
</section>
<section class="level3" id="running-fst">
<h3>Running FST</h3>
<p>That’s it. Given suitable weights, running an inference pass on this DNN is all it takes to get a styled image. Like NST, we have wrapped all things up in a <a href="https://gist.github.com/jzstark/f937ce439c8adcaea23d42753f487299">Gist</a>, and provide a simple user interface to users. Here is an example:</p>
<div class="highlight">
<pre><code class="language-clike">#zoo "f937ce439c8adcaea23d42753f487299"

FST.list_styles ();; (* show all supported styles *)
FST.run ~style:1 "path/to/content_img.png" "path/to/output_img.jpg" </code></pre>
</div>
<p>The <code>run</code> function mainly takes one content image and output to a new image file, the name of which is designated by the user. The image could be of any popular formats: jpeg, png, etc. This gist contains exemplar content images for you to use.</p>
<p>Note that we did say “given suitable weights”. A set of trained weight for the FST DNN represents a unique artistic style. We have already include six different weight files for use, and the users just need to pick one of them and load them into the DNN, without worrying about how to train these weights.</p>
<p>Current we support six art styles: “<a href="https://bit.ly/2nBW0ae">Udnie</a>” by Francis Picabia, “<a href="https://bit.ly/2nKk8Hl">The Great Wave off Kanagawa</a>” by Hokusai, “<a href="https://bit.ly/2KA7FAY">Rain Princess</a>” by Leonid Afremov, “<a href="https://bit.ly/2rS1fWQ">La Muse</a>” by Picasso, “<a href="https://bit.ly/1CvJz5d">The Scream</a>” by Edvard Munch, and “<a href="https://bit.ly/2wVfizH">The shipwreck of the Minotaur</a>” by J. M. W. Turner</p>
<p>Yes, maybe six styles are not enough for you, but think about it, you can now render any of your image to a nice art style fast, maybe about half a minute, or even faster if you are using GPU or other accelerators. Here is a teaser that renders one city view image to all these amazing art styles.</p>
<p><img id="fig:case-obj-detect:example_03" src="images/case-nst/example_fst00.png"></p>
<p>If you are still not persuaded, here is our ultimate solution for you: a [demo] website, where you can choose a style, upload an image, get yourself a cup of coffee, and then checkout the rendered image. To push things even further, we apply FST to some videos frame-by-frame, and put them together to get some artistic videos, as shown in this <a href="https://www.youtube.com/watch?v=cFOM-JnyJv4&amp;list=PLGt9zVony2zVSiHZb8kwwXfcmCuOH2W-H">Youtube list</a>. And all of these are implemented in Owl.</p>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-gatys2015neural">
<p>Gatys, Leon A, Alexander S Ecker, and Matthias Bethge. 2015. “A Neural Algorithm of Artistic Style.” <em>arXiv Preprint arXiv:1508.06576</em>.</p>
</div>
<div id="ref-Johnson2016Perceptual">
<p>Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. 2016. “Perceptual Losses for Real-Time Style Transfer and Super-Resolution.” In <em>European Conference on Computer Vision</em>.</p>
</div>
<div id="ref-luan2017deep">
<p>Luan, Fujun, Sylvain Paris, Eli Shechtman, and Kavita Bala. 2017. “Deep Photo Style Transfer.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 4990–8.</p>
</div>
<div id="ref-zhu2017unpaired">
<p>Zhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 2223–32.</p>
</div>
</div>
</section>
</section>
</section>
</article></div><a href="case-gpu.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 32</small>Case - Using Accelerators</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>