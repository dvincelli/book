<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Case - Neural Style Transfer - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="case---neural-style-transfer">
<h1>Case - Neural Style Transfer</h1>
<p>What is Neural Style Transfer (NST)? It is a pretty cool application of Deep Neural Networks (DNN), “the process of using DNN to migrate the semantic content of one image to different styles”.</p>
<p>The process is actually very simple, as the title image shows, this application takes two images A and B as input. Let’s say A is “Mona Lisa” of Da Vinci, and B is “The Starry Night” of Vincent van Gogh.</p>
<p>We then specify A as the content image and B as the style image, then what a NST application can produce? Boom! A new Mona Lisa, but with the style of Van Gogh (see the middle of title image)! If you want another style, just replace image B and run the application again. Impressionism, abstractionism, classical art, you name it.</p>
<p>The figure below illustrate this point. You can apply different art styles to the same street view, or apply the same “Starry Sky” style to any pictures. Isn’t it amazing?</p>
<figure>
<img alt="" id="fig:case-nst:example_01" src="images/case-nst/nst_example.png"><figcaption>Figure 1: Example of applying neural style transfer on a street view picture</figcaption>
</figure>
<section class="level2" id="content-and-style-reconstruction">
<h2>Content and Style Reconstruction</h2>
<p><span data-cites="gatys2015neural" class="citation">(Gatys, Ecker, and Bethge 2015)</span> first propose to use DNN to let programmes to create artistic images of high perceptual quality. The examples above may look like magic, but surely its not. In this section, we will first introduce the intuition about how the neural style transfer algorithm works. For more formal and detailed introduction, please visit the <a href="https://arxiv.org/abs/1508.06576">original paper</a>.</p>
<p>The basic idea is plain: we want to get an image whose content is similar to one image and its artistic style close to the other image. Of course, to make the algorithm work, first we need to express this sentence in mathematical form so that computers can understand it. Let’s assume for a moment we have already know that, then style transfer can be formalised as an optimisation problem. Given a content image <code>c</code> and a style image <code>s</code>, our target is to get an output image <code>x</code> so that it minimises:</p>
<p><span class="math display">\[f(x) = \verb|content_distance|(x, c) + \verb|style_distance|(x, s)\]</span></p>
<p>Here the “distance” between two feature map is calculated by the euclidean distance between the two ndarrays.</p>
<p>You may remember from the regression or neural network chapters that training process is also an optimisation process. However, do not mistake the optimisation in NST as regression or DNN training. For the latter one, there is the function <span class="math inline">\(f_w\)</span> that contains parameter <span class="math inline">\(w\)</span> and the training process optimise <span class="math inline">\(w\)</span> to minimise <span class="math inline">\(f_w(x)\)</span>. The optimisation in NST is more like the traditional optimisation problem: we have a function <span class="math inline">\(f\)</span>, and we start we an initial input <span class="math inline">\(x_0\)</span> and update it iteratively until we have satisfying <span class="math inline">\(x\)</span> that minimise the target function.</p>
<p>Now we can comeback to the key problem. While we human beings can kind of feel the style of a paint and visually recognise the contents in a picture, how can we mathematically express the “content” and the “style” of an image? That’s where the convolution network comes to help. DNNs, especially the ones that are used for computer vision tasks, are found to be an convenient tool to capture the characteristics of an image. We have demonstrate in the previous chapter how CNNs are good at spotting the “features” in an image layer by layer. Therefore, in the next two sub-sections, we will explain how it can be used to express the content and style feature of an image.</p>
<p>We have introduced several CNN architectures to perform image detection task in the previous chapter. We choose to use VGG19 since its follows a simple linear stack structure and is proved to have good performance. We have built the VGG19 network structure in <a href="https://gist.github.com/jzstark/da5cc7f771bc8d9699cedc491b23f856">this gist</a>. It contains 38 layers in total, and we prepared pre-trained weights for it.</p>
<section class="level3" id="content-reconstruction">
<h3>Content Reconstruction</h3>
<p>From the image detection case, we know that the CNN extract features layer by layer until the features are so abstract that it can give an answer such as “this is a car” “this is an apple” etc. Therefore, we can use the feature map to reconstruct content of an image.</p>
<p>But which layer’s output should we use as a suitable indication of the image content? Let’s perform a simplified version of NST: we only care about re-constructing the content of the input image, so our target is to minimise:</p>
<p><span class="math display">\[f(x) = \verb|content_distance|(x, c)\]</span></p>
<p>As an example, we use fig.&nbsp;2 as the target content. (This image <a href="https://ccsearch.creativecommons.org/photos/f4024dc8-ce39-4e86-acfd-47532fef824d">“Tourists in Nanzen-Ji Hojo”</a> by blieusong is licensed under CC BY-SA 2.0.)</p>
<figure>
<img alt="" style="width:40.0%" id="fig:case-nst:content-example" title="hojo tourists" src="images/case-nst/hojo.png"><figcaption>Figure 2: Example content image in neural style transfer</figcaption>
</figure>
<p>Suppose we choose the output of <code>idx</code> layer as the chosen feature map to represent the content. First, we need to compute the target feature map:</p>
<div class="highlight">
<pre><code class="language-clike">let fill_content_targets x net = 
  let selected_topo = Array.sub nn.topo 0 (idx + 1) in 
  run' selected_topo x</code></pre>
</div>
<p>The function <code>fill_content_targets</code> takes the content image and the VGG network as input, and returns the target feature map as output. We only need to compute the feature map of the target content image once.</p>
<p>Here the <code>run'</code> function is implemented by accumulating the inference result along the selected part of the network, from the network input until the chosen layer, instead of processing the whole network:</p>
<div class="highlight">
<pre><code class="language-clike">let run' topo x =
  let last_node_output = ref (F 0.) in
  Array.iteri (fun i n -&gt;
    let input  = if i = 0 then x else !last_node_output in 
    let output = run [|input|] n.neuron in
    last_node_output := output;
  ) topo;
  !last_node_output</code></pre>
</div>
<p>Then we can start optimising the input image <code>x</code>. Let’s set the initial <code>x</code> to be a “white noise” image that only contains random pixels. This image has the same shape as content image.</p>
<div class="highlight">
<pre><code class="language-text">let input_shape = Dense.Ndarray.S.shape content_img in
Dense.Ndarray.S.(gaussian input_shape |&gt; scalar_mul 0.256)</code></pre>
</div>
<p>The feature map of the input image <code>x</code> is still calculated using the same process show in function <code>fill_content_targets</code>. We call the resulting feature map <code>response</code>, then the loss value can be calculated with the L2Norm of the difference between two feature maps, and then normalised with the feature map size.</p>
<div class="highlight">
<pre><code class="language-clike">let c_loss response target = 
  let loss = Maths.((pow (response - target) (F 2.)) |&gt; sum') in 
  let _, h, w, feature = get_shape target in 
  let c = float_of_int ( feature * h * w ) in
  Maths.(loss / (F c))</code></pre>
</div>
<p>Once the loss value is calculated, we can apply optimisers. Here we use the <code>minimise_fun</code> from <code>Optimise</code> module. The target function can be described as:</p>
<div class="highlight">
<pre><code class="language-clike">let g x = 
  fill_losses x;
  c_loss response target</code></pre>
</div>
<p>All it performs is what we just described: first calculating the feature map <code>response</code> of input image at a certain layer, and then compute the distance between it and the target content feature map as loss value.</p>
<p>Finally, we can perform the optimisation iterations:</p>
<div class="highlight">
<pre><code class="language-clike">let state, img = Optimise.minimise_fun params g (Arr input_img) in
let x' = ref img in
while Checkpoint.(state.current_batch &lt; state.batches) do
  Checkpoint.(state.stop &lt;- false);
  let a, img = Optimise.minimise_fun ~state params g !x' in 
  x' := img
done;</code></pre>
</div>
<p>We keep updating the image <code>x</code> for fixed number of iterations. Particularly, we use the Adam adaptive learning rate method, for it proves to be quite effective in style transfer optimisation:</p>
<div class="highlight">
<pre><code class="language-clike">let params = Params.config
    ~learning_rate:(Learning_Rate.Adam (learning_rate, 0.9, 0.999))
    ~checkpoint:(Checkpoint.Custom chkpt)
    iter</code></pre>
</div>
<p>Using the process above, we return to the problem of choosing a suitable layer as the indication of the image content. In this 38-layer VGG network, we choose these layers: 2, 7, 12, 21, 30. Then we can compare the optimisation result to see the effect of image reconstruction. Each one is generated using 100 iterations.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:case-nst:contents-rec" title="contents-reconstruction" src="images/case-nst/contents-reconstruction.png"><figcaption>Figure 3: Contents reconstruction</figcaption>
</figure>
<p>It is shown that, the content information is kept accurate at the lower level. Along the processing hierarchy of the network, feature map produced by the lower layer cares more about the small features that at the pixel level, while the higher layer gives more abstract information but less details to help with content reconstruction.</p>
</section>
<section class="level3" id="style-recreation">
<h3>Style Recreation</h3>
<p>Then similarly, we explore the other end of this problem. Now we only care about recreating an image with only the style of an input image. That is to say, we optimise the input image with this target to minimise:</p>
<p><span class="math display">\[f(x) = \verb|style_distance|(x, s)\]</span></p>
<p>As an example, we will use the famous “The Great Wave of Kanagawa” by Hokusai as our target style image:</p>
<figure>
<img alt="" style="width:40.0%" id="fig:case-nst:style-example" title="hokusai" src="images/case-nst/hokusai.png"><figcaption>Figure 4: Example style image in neural style transfer</figcaption>
</figure>
<p>The basic approach is the same as before: first compute the style representation of target image using the output from one or more layers, and then compute the style representation of the input image following the same method. The normalised distance between these two ndarrays are used as the optimisation target.</p>
<p>However, the difference is that, unlike the content representation, we cannot directly take one filter map from certain layer as the style representation. Instead, we need to t computes the correlations between different filters from the output of a layer. This correlation can be represented by the Gram matrix, which intuitively captures the “distribution of features” of feature maps from a certain layer. The <span class="math inline">\((i,j)\)</span>-th element of an Gram matrix is computed by element-wisely multiplying the <span class="math inline">\(i\)</span>-th and <span class="math inline">\(j\)</span>-th channels in the feature maps and summing across both width and height. This process can be simplified as a matrix multiplication. The result is normalised with the size of the feature map. The code is shown below.</p>
<div class="highlight">
<pre><code class="language-clike">let gram x = 
  let _, h, w, feature = get_shape x in 
  let new_shape = [|h * w; feature|] in 
  let ff = Maths.(reshape x new_shape) in 
  let size = F (float_of_int (feature * h * w)) in
  Maths.((transpose ff) *@ ff / size)</code></pre>
</div>
<p>Now that we have a method to represent the “style” of an image, we can proceed to calculate the loss value during optimisation. It is very similar to that of content recreation, and the only difference is that we use the distance between Gram matrices instead of the feature map from a certain layer as loss value.</p>
<div class="highlight">
<pre><code class="language-clike">let s_loss response_gram target_gram = 
  let loss = Maths.((pow (response_gram - target_gram) (F 2.)) |&gt; sum') in 
  let s = Algodiff.shape target_gram in
  let c = float_of_int (s.(0) * s.(1)) in
  Maths.(loss / (F c))</code></pre>
</div>
<p>However, note that for the optimisation, instead of using output from one layer, we usually utilises the loss value from multiple layers and the optimisation target for style reconstruction:</p>
<div class="highlight">
<pre><code class="language-clike">let g x = 
  fill_losses x;
  Array.fold_left Maths.(+) (F 0.) style_losses</code></pre>
</div>
<p>Here the <code>fill_losses</code> function compute style losses at different layers, and store them into the <code>style_losses</code> array. Then they are added up as the optimisation target. The rest process is the as in the content reconstruction.</p>
<p>As example, we choose the similar five layers from the VGG19 network: layer 2, 7, 12, 21, and 30. Then we computing the aggregated loss value fo the first layer, the first two layers, the first three layers, the first four layers, and all layers, as five different optimisation target. The results are shown in fig.&nbsp;5.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:case-nst:style-rec" title="style-reconstruction" src="images/case-nst/style-reconstruction.png"><figcaption>Figure 5: Style reconstruction</figcaption>
</figure>
<p>As the result shows, features from the beginning tends to contain low level information such as pixels, so reconstructing styles according to them results in a fragmented white-noise-like representation, which really does not show any obvious style. Only by adding more deep layer features can the style be gradually be reconstructed.</p>
</section>
<section class="level3" id="combining-content-and-style">
<h3>Combining Content and Style</h3>
<p>Now that we have seen these two extremes: only recreating content and only recreating style, it’s straightforward to understand the theory of style transfer: to synthesised an image that has similar content with one image and style close to the other. The code would be mostly similar to what we have seen, and the only difference now is simply adding the loss value of content and styles as the final optimisation target.</p>
<p>One thing we need to note during combining contents and style is the proportion of each part, and the choice of layers as representation. This problem is actually more artistic than technique, so here we only follow the current practice about parameter configuration. Please refer to the original paper about the effect of parameter tuning.</p>
<p>As suggested by previous experiment results, we use the feature maps from 23rd layer for content recreation, and combines the output of layer 2, 7, 12, 21, and 30 in VGG19 to represent the style feature of an image. When combining the loss values, we multiply the style loss with a weight number, and then add it to the content loss. Practice shows that a weigh number of 20 shows good performance.</p>
<p>You might also be wondering: why not choose the 2nd layer if it show the best content reconstruction result? The intuition is that we don’t want the synthesised image to be too close to the content image in content, because that would mean less style. Therefore we use a layer from the middle of CNN which shows to keep most of the information for content reconstruction.</p>
<figure>
<img alt="" style="width:90.0%" id="fig:case-nst:nst_example_01" title="nst_example_01" src="images/case-nst/nst_example_01.png"><figcaption>Figure 6: Combining content and style reconstruction</figcaption>
</figure>
<p>Combining all these factors together, fig.&nbsp;6 shows the result about running our code and creating an artistic view based on the original image.</p>
<p>All the code (about 180 lines) is included in <a href="https://gist.github.com/jzstark/6f28d54e69d1a19c1819f52c5b16c1a1">this Gist</a>. The pre-trained weight file for VGG19 is also included. As with the image detection applications, it also relies on <code>ImageMagick</code> to manipulate image format conversion and resizing. We only list part of it above, and there are many implementation details such as garbage collection are omitted to focus on the theory of the application itself. We therefore suggest you to play with the code itself with images or parameters of your choice.</p>
</section>
<section class="level3" id="running-nst">
<h3>Running NST</h3>
<p>To make the code above more suitable to use, this NST application provides a simple interfaces to use. Here is an example showing how to use it with two lines of code:</p>
<div class="highlight">
<pre><code class="language-clike">#zoo "6f28d54e69d1a19c1819f52c5b16c1a1"

Neural_transfer.run 
  ~ckpt:50 
  ~src:"path/to/content_img.jpg" 
  ~style:"path/to/style_img.jpg" 
  ~dst:"path/to/output_img.png" 250.;;</code></pre>
</div>
<p>Similar to the image detection application, the command can be simplified using the Zoo system in owl. The first line downloads gist files and imported this gist as an OCaml module, and the second line uses the <code>run</code> function to produce an output image to your designated path. Its syntax is quite straightforward, and you may only need to note the final parameter. It specifies how many iterations the optimisation algorithm runs. Normally 100 ~ 500 iterations is good enough.</p>
<p>This module also supports saving the intermediate images to the same directory as output image every N iterations (e.g.&nbsp;<code>path/to/output_img_N.png</code>). <code>N</code> is specified by the <code>ckpt</code> parameter, and its default value is 50 iterations. If users are already happy with the intermediate results, they can terminate the program without waiting for the final output image.</p>
<p>That’s all. Now you can try the code easily. If you don’t have suitable input images at hand, the gist already contains exemplar content and style images to get you started. More examples can be seen on our online <a href="http://demo.ocaml.xyz/neuraltrans.html">demo</a> page.</p>
</section>
<section class="level3" id="extending-nst">
<h3>Extending NST</h3>
<p>The neural style transfer has since attracts a lot of attentions. It is the core technology to many successful industrial applications, most notably photo rendering applications. For example, the <a href="https://play.google.com/store/apps/details?id=com.neuralprisma&amp;hl=en_GB">Prisma Photo Editor</a> features transforming your photos into paintings of hundreds of styles.</p>
<p>There are also many research work that aim to extend this work. One of these work is the <em>Deep Photo Style Transfer</em> proposed in <span data-cites="luan2017deep" class="citation">(Luan et al. 2017)</span>. The idea is simple: instead of using an art image, can I use another normal image as style reference? For example, we have a normal daylight street view in New York as content image, then we want to use the night view of London as reference, to synthesise an image of the night view of New York.</p>
<p>The authors identify two key challenges in this problem. The first is that, unlike in NST, we hope to only change to colours of the style image, and keep the content un-distorted, so as to create a “real” image as much as possible. For this challenge, the authors propose to add an regularisation item to our existing optimisation target “content distance + style distance”. This item, depending on only input and outpu images, penalises image distortion and seeks an image transform that is locally affine in color space. The second challenge is that, we don’t want the styles to be applied globally. For example, we only want to apply the style of an sunset sky to a blue sky, not a building. For this problem, the authors propose to coarsely segment input images into several parts before apply style transfer separately. If you are interested to check the original paper, the resulting photos are indeed beautifully and realistically rendered.</p>
<p>TODO: Image-to-Image Translation <span data-cites="zhu2017unpaired" class="citation">(Zhu et al. 2017)</span></p>
<p>Another variant is called the Fast Style Transfer. Instead of iteratively updating image, it proposes to use one pre-trained feed-forward network to do style transfer, and therefore improving the speed of rendering by orders of magnitude. That’s what we will be talking about in the rest of this chapter.</p>
</section>
</section>
<section class="level2" id="fast-style-transfer">
<h2>Fast Style Transfer</h2>
<p>Paper: <span data-cites="ulyanov2016texture" class="citation">(Ulyanov et al. 2016§)</span></p>
<p>One disadvantage of NST is that it could take a very long time to rendering an image, and if you want to change to another content or style image, then you have to wait a long time for the training again. If you want to render some of your best (or worst) selfies fast and send to your friends, NST is perhaps not a perfect choice.</p>
<p>This problem then leads to another application: Fast Neural Style Transfer (FST). FST sacrifice certain degrees of flexibility, which is that you cannot choose style images at will. But as a result, you only need to feed your content image to a DNN, finish an inference pass, and then the output will be the rendered styled image as you expected! The best part is that, one inference pass is much much faster that keep running a training phase.</p>
<section class="level3" id="theory">
<h3>Theory</h3>
</section>
<section class="level3" id="building-fst-network">
<h3>Building FST Network</h3>
<p>Based on the <a href="https://github.com/lengstrom/fast-style-transfer">TensorFlow implementation</a>, we have implemented a FST application in Owl, and it’s not complicated. Here is the network structure:</p>
<div class="highlight">
<pre><code class="language-ocaml">open Owl
open Neural.S
open Neural.S.Graph
open Neural.S.Algodiff
module N = Dense.Ndarray.S

(** Network Structure *)

let conv2d_layer ?(relu=true) kernel stride nn  =
  let result = 
    conv2d ~padding:SAME kernel stride nn
    |&gt; normalisation ~decay:0. ~training:true ~axis:3
  in
  match relu with
  | true -&gt; (result |&gt; activation Activation.Relu)
  | _    -&gt; result

let conv2d_trans_layer kernel stride nn = 
  transpose_conv2d ~padding:SAME kernel stride nn
  |&gt; normalisation ~decay:0. ~training:true ~axis:3
  |&gt; activation Activation.Relu

let residual_block wh nn = 
  let tmp = conv2d_layer [|wh; wh; 128; 128|] [|1;1|] nn
    |&gt; conv2d_layer ~relu:false [|wh; wh; 128; 128|] [|1;1|]
  in 
  add [|nn; tmp|]

let make_network h w = 
  input [|h;w;3|]
  |&gt; conv2d_layer [|9;9;3;32|] [|1;1|]
  |&gt; conv2d_layer [|3;3;32;64|] [|2;2|]
  |&gt; conv2d_layer [|3;3;64;128|] [|2;2|]
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; residual_block 3
  |&gt; conv2d_trans_layer [|3;3;128;64|] [|2;2|]
  |&gt; conv2d_trans_layer [|3;3;64;32|] [|2;2|]
  |&gt; conv2d_layer ~relu:false [|9;9;32;3|] [|1;1|]
  |&gt; lambda (fun x -&gt; Maths.((tanh x) * (F 150.) + (F 127.5)))
  |&gt; get_network</code></pre>
</div>
</section>
<section class="level3" id="running-fst">
<h3>Running FST</h3>
<p>That’s it. Given suitable weights, running an inference pass on this DNN is all it takes to get a styled image. Like NST, we have wrapped all things up in a <a href="https://gist.github.com/jzstark/f937ce439c8adcaea23d42753f487299">Gist</a>, and provide a simple user interface to users. Here is an example:</p>
<div class="highlight">
<pre><code class="language-clike">#zoo "f937ce439c8adcaea23d42753f487299"

FST.list_styles ();; (* show all supported styles *)
FST.run ~style:1 "path/to/content_img.png" "path/to/output_img.jpg" </code></pre>
</div>
<p>The <code>run</code> function mainly takes one content image and output to a new image file, the name of which is designated by the user. The image could be of any popular formats: jpeg, png, etc. This gist contains exemplar content images for you to use.</p>
<p>Note that we did say “given suitable weights”. A set of trained weight for the FST DNN represents a unique artistic style. We have already include six different weight files for use, and the users just need to pick one of them and load them into the DNN, without worrying about how to train these weights.</p>
<p>Current we support six art styles: “<a href="https://bit.ly/2nBW0ae">Udnie</a>” by Francis Picabia, “<a href="https://bit.ly/2nKk8Hl">The Great Wave off Kanagawa</a>” by Hokusai, “<a href="https://bit.ly/2KA7FAY">Rain Princess</a>” by Leonid Afremov, “<a href="https://bit.ly/2rS1fWQ">La Muse</a>” by Picasso, “<a href="https://bit.ly/1CvJz5d">The Scream</a>” by Edvard Munch, and “<a href="https://bit.ly/2wVfizH">The shipwreck of the Minotaur</a>” by J. M. W. Turner</p>
<p>Yes, maybe six styles are not enough for you, but think about it, you can now render any of your image to a nice art style fast, maybe about half a minute, or even faster if you are using GPU or other accelerators. Here is a teaser that renders one city view image to all these amazing art styles.</p>
<p><img id="fig:case-obj-detect:example_03" src="images/case-nst/example_fst00.png"></p>
<p>If you are still not persuaded, here is our ultimate solution for you: a [demo] website, where you can choose a style, upload an image, get yourself a cup of coffee, and then checkout the rendered image. To push things even further, we apply FST to some videos frame-by-frame, and put them together to get some artistic videos, as shown in this <a href="https://www.youtube.com/watch?v=cFOM-JnyJv4&amp;list=PLGt9zVony2zVSiHZb8kwwXfcmCuOH2W-H">Youtube list</a>. And all of these are implemented in Owl.</p>
</section>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-gatys2015neural">
<p>Gatys, Leon A, Alexander S Ecker, and Matthias Bethge. 2015. “A Neural Algorithm of Artistic Style.” <em>arXiv Preprint arXiv:1508.06576</em>.</p>
</div>
<div id="ref-luan2017deep">
<p>Luan, Fujun, Sylvain Paris, Eli Shechtman, and Kavita Bala. 2017. “Deep Photo Style Transfer.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 4990–8.</p>
</div>
<div id="ref-ulyanov2016texture">
<p>Ulyanov, Dmitry, Vadim Lebedev, Andrea Vedaldi, and Victor S Lempitsky. 2016. “Texture Networks: Feed-Forward Synthesis of Textures and Stylized Images.” In <em>ICML</em>, 1:4. 2.</p>
</div>
<div id="ref-zhu2017unpaired">
<p>Zhu, Jun-Yan, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. “Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 2223–32.</p>
</div>
</div>
</section>
</section>
</article></div><a href="case-gpu.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 32</small>Case - Using Accelerators</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>