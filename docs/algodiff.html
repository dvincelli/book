<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Algorithmic Differentiation - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="algorithmic-differentiation">
<h1>Algorithmic Differentiation</h1>
<p>TBD</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Introductory examples: speed and thermal field –&gt; definition/format of derivative and gradient. (Ref: Feynman lecture)</p>
<p>Computing derivatives (differentiation) is crucial in many scientific related fields: find maximum or minimum values using gradient descent (see later chapter); ODE (see later chapter); Non-linear optimisation such as KKT optimality conditions is still a prime application.</p>
<p>One new crucial application is in machine learning.</p>
<section class="level3" id="chain-rule">
<h3>Chain Rule</h3>
<p>Liang: <em>you need to talk about hessian, etc.</em></p>
<p>Before diving into how to do differentiation on computers, let’s recall how to do it manually from our Calculus 101. In calculus, the chain rule is a formula to compute the derivative of a composite function. Suppose we have two functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, then the Chain rule states that:</p>
<p><span id="eq:algodiff:chainrule01"><span class="math display">\[F'(x)=f'(g(x))g'(x).\qquad(1)\]</span></span></p>
<p>This seemingly simple rule is one of the basic rule in calculating derivatives. For example, let <span class="math inline">\(y = x^a\)</span>, where <span class="math inline">\(a\)</span> is a real number, and then we can get <span class="math inline">\(y'\)</span> using the chain rule. Specifically, let <span class="math inline">\(y=e^{ln~x^a} = e^{a~ln~x}\)</span>, and then we can set <span class="math inline">\(u= alnx\)</span> so that now <span class="math inline">\(y=e^u\)</span>. By applying the chain rule, we have:</p>
<p><span class="math display">\[y' = \frac{dy}{du}~\frac{du}{dx} = e^u~a~\frac{1}{x} = ax^{a-1}.\]</span></p>
<p>Also, it’s helpful to remember some rules:</p>
<div id="tbl:algodiff:chainrule02">
<table style="width:88%;">
<caption>Table 1: A Short Table of Basic Derivatives</caption>
<colgroup>
<col style="width: 33%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Derivatives</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\((u(x) + v(x))'\)</span></td>
<td style="text-align: left;"><span class="math inline">\(u'(x) + v'(x)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\((C\times~u(x))'\)</span></td>
<td style="text-align: left;"><span class="math inline">\(C\times~u'(x)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\((u(x)v(x))'\)</span></td>
<td style="text-align: left;"><span class="math inline">\(u'(x)v(x) + u(x)v'(x)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\((\frac{u(x)}{v(x)})'\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{u'(x)v(x) - u(x)v'(x)}{v^2(x)}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Using these basic rules, we can solve many derivative questions.</p>
</section>
<section class="level3" id="differentiation-methods">
<h3>Differentiation Methods</h3>
<p>As the models and algorithms become increasingly complex, sometimes the function being implicit, it is impractical to perform manual differentiation. Therefore, we turn to computer-based automated computation methods. There are three: numerical differentiation, symbolic differentiation, and algorithmic differentiation.</p>
<p><strong>Numerical Differentiation</strong></p>
<p>The numerical differentiation comes from the definition of derivative:</p>
<p><span class="math display">\[f'(x) = \lim_{\delta~\to~0}\frac{f(x+\delta) - f(x)}{\delta}.\]</span></p>
<p>We’ll discuss it in the Optimisation chapter, since optimisation using gradient is a very important application of differentiation. This method is easy in coding, but is subject to numerical errors. (EXPLAIN)</p>
<p><strong>Symbolic Differentiation</strong></p>
<p>This method gives exact solution, but in most cases, this solution gives a long list of symbols that will take much memory. Finding symbolic representation also cannot utilise common intermediate result, and is slow.</p>
<p><a href="https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/">Duplication</a></p>
<p><strong>Algorithmic Differentiation</strong></p>
<p>Algorithmic differentiation (AD) is a chain-rule based technique for calculating the derivatives with regards to input variables of functions defined in a computer programme. It is also known as automatic differentiation. It is a powerful tool in many fields.</p>
<p>It’s advantage compared with the other two.</p>
<p>Especially, it is not symbolic differentiation.</p>
<p>Now let’s talk about AD.</p>
</section>
</section>
<section class="level2" id="how-algorithmic-differentiation-works">
<h2>How Algorithmic Differentiation Works</h2>
<p>We have seen the chain rules being applied on simple functions such as <span class="math inline">\(y=x^a\)</span>. Now let’s check how this rule can be applied on more complex computations. Let’s look at the function below:</p>
<p><span id="eq:algodiff:example"><span class="math display">\[y(x_0, x_1) = (1 + e^{x_0~x_1 + sin(x_0)})^{-1}.\qquad(2)\]</span></span></p>
<p>This functions is based on a sigmoid function. Our goal is to compute the partial derivative <span class="math inline">\(\frac{\partial~y}{\partial~x_0}\)</span> and <span class="math inline">\(\frac{\partial~y}{\partial~x_1}\)</span>. To better illustrate this process, we express eq.&nbsp;2 as a graph, as shown in fig.&nbsp;1. At the right side of the figure, we have the final output <span class="math inline">\(y\)</span>, and at the roots of this graph are input variables. The nodes between them indicate constants or intermediate variables that are gotten via basic functions such as <code>sine</code>. All nodes are labelled by <span class="math inline">\(v_i\)</span>. An edge between two nodes represents an explicit dependency in the computation.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:algodiff:example_01" title="example_01" src="images/algodiff/example_01.png"><figcaption>Figure 1: Graph expression of function</figcaption>
</figure>
<p>Based on this graphic representation, there are two major ways to apply the chain rules: the forward differentiation mode, and the reverse differentiation mode (not “backward differentiation”, which is a method used for solving ordinary differential equations). Next, we introduce these two methods.</p>
<section class="level3" id="forward-mode">
<h3>Forward Mode</h3>
<p>Our target is to calculate <span class="math inline">\(\frac{\partial~y}{\partial~x_0}\)</span> (partial derivative regarding <span class="math inline">\(x_1\)</span> should be similar). But don’t be so hurry, let’s start with some earlier intermediate results that might be helpful. For example, what is <span class="math inline">\(\frac{\partial~x_0}{\partial~x_1}\)</span>? 1, obviously. Equally obvious is <span class="math inline">\(\frac{\partial~x_1}{\partial~x_1} = 0\)</span>. It’s just elementary. Now, things gets a bit trickier: what is <span class="math inline">\(\frac{\partial~v_3}{\partial~x_0}\)</span>? Not is a good time to use the chain rule:</p>
<p><span class="math display">\[\frac{\partial~v_3}{\partial~x_0} = \frac{\partial~(x_0~x_1)}{\partial~x_0} = x_1~\frac{\partial~(x_0)}{\partial~x_0} + x_0~\frac{\partial~(x_1)}{\partial~x_0} = x_1.\]</span></p>
<p>After calculating <span class="math inline">\(\frac{\partial~v_3}{\partial~x_0}\)</span>, we can then processed with derivatives of <span class="math inline">\(v_5\)</span>, <span class="math inline">\(v_6\)</span>, all the way to that of <span class="math inline">\(v_9\)</span> which is also the output <span class="math inline">\(y\)</span> we are looking for. This process starts with the input variables, and ends with output variables. Therefore, it is called <em>forward differentiation</em>. We can do simplify the math notations in this process by letting <span class="math inline">\(\dot{v_i}=\frac{\partial~(v_i)}{\partial~x_0}\)</span>. The <span class="math inline">\(\dot{v_i}\)</span> here is called <em>tangent</em> of function <span class="math inline">\(v_i(x_0, x_1, \ldots, x_n)\)</span> with regard to input variable <span class="math inline">\(x_0\)</span>. The forward differentiation mode is sometimes also called “tangent linear” mode.</p>
<p>Now we can present the full forward differentiation calculation process, as shown in tbl.&nbsp;2. Two simultaneous lines of computing happen: on the left hand side is the computation procedure specified by eq.&nbsp;2; on the right side shows computation of derivative for each intermediate variable with regard to <span class="math inline">\(x_0\)</span>. Let’s find out <span class="math inline">\(\dot{y}\)</span> when setting <span class="math inline">\(x_0 = 1\)</span>, and <span class="math inline">\(x_1 = 1\)</span>.</p>
<div id="tbl:algodiff:forward">
<table style="width:93%;">
<caption>Table 2: Computation process of forward differentiation</caption>
<colgroup>
<col style="width: 6%">
<col style="width: 38%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th style="text-align: left;">Intermediate computation</th>
<th style="text-align: left;">Derivative computation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td style="text-align: left;"><span class="math inline">\(v_0 = x_0 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_0}=1\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td style="text-align: left;"><span class="math inline">\(v_1 = x_1 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_1}=0\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td style="text-align: left;"><span class="math inline">\(v_2 = sin(v_0) = 0.84\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_2} = cos(v_2)*\dot{v_0} = 0.84 * 1 = 0.84\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td style="text-align: left;"><span class="math inline">\(v_3 = v_0~v_1 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_3} = v_0~\dot{v_1} + v_1~\dot{v_0} = 1 * 0 + 1 * 1 = 1\)</span></td>
</tr>
<tr class="odd">
<td>4</td>
<td style="text-align: left;"><span class="math inline">\(v_4 = v_2 + v3 = 1.84\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_4} = \dot{v_2} + \dot{v_3} = 1.84\)</span></td>
</tr>
<tr class="even">
<td>5</td>
<td style="text-align: left;"><span class="math inline">\(v_5 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_5} = 0\)</span></td>
</tr>
<tr class="odd">
<td>6</td>
<td style="text-align: left;"><span class="math inline">\(v_6 = \exp{(v_4)} = 6.30\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_6} = \exp{(v_4)} * \dot{v_4} = 6.30 * 1.84 = 11.59\)</span></td>
</tr>
<tr class="even">
<td>7</td>
<td style="text-align: left;"><span class="math inline">\(v_7 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_7} = 0\)</span></td>
</tr>
<tr class="odd">
<td>8</td>
<td style="text-align: left;"><span class="math inline">\(v_8 = v_5 + v_6 = 7.30\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_8} = \dot{v_5} + \dot{v_6} = 11.59\)</span></td>
</tr>
<tr class="even">
<td>9</td>
<td style="text-align: left;"><span class="math inline">\(y = v_9 = \frac{1}{v_8}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{y} = \frac{-1}{v_8^2} * \dot{v_8} = -0.22\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>We can validate this result with algorithmic differentiation module in Owl. If you don’t understand the code, don’t worry. We will cover the detail of this module in detail later.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">open Algodiff.D;;
let f x = 
  let x1 = Mat.get x 0 0 in 
  let x2 = Mat.get x 0 1 in 
  Maths.(div (F 1.) (F 1. + exp (x1 * x2 + (sin x1))))
;;
&gt;val f : t -&gt; t = &lt;fun&gt;
let x = Mat.zeros 1 2 ;;
&gt;val x : t = [Arr(1,2)]
let _ = grad f x |&gt; unpack_arr;;
&gt;- : A.arr =
&gt;      C0 C1
&gt;R0 -0.25  0
</code></pre>
</div>
</section>
<section class="level3" id="reverse-mode">
<h3>Reverse Mode</h3>
<p>Computation Steps and figure</p>
<p>Also called “adjoint” mode.</p>
</section>
<section class="level3" id="forward-or-reverse">
<h3>Forward or Reverse?</h3>
<p>Since both can be used to differentiate a function then the natural question is which mode we should choose in practice. The short answer is: it depends on your function.</p>
<p>In general, given a function that you want to differentiate, the rule of thumb is:</p>
<ul>
<li>if input variables &gt;&gt; output variables, then use reverse mode;</li>
<li>if input variables &lt;&lt; output variables, then use forward mode.</li>
</ul>
<p>Later we will show example of this point.</p>
<p><strong>Theoretical Basis:</strong> adjoint, dual number, first derivative, higher derivative, etc.</p>
</section>
</section>
<section class="level2" id="high-level-apis">
<h2>High-level APIs</h2>
<p>The design of AD in Owl.</p>
<p>Owl provides both numerical differentiation (in <a href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_numdiff_generic.mli">Numdiff.Generic</a> module) and algorithmic differentiation (in <a href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_algodiff_generic.mli">Algodiff.Generic</a> module).</p>
<p><code>Algodiff.Generic</code> is a functor which is able to support both <code>float32</code> and <code>float64</code> precision <code>AD</code>. However, you do not need to deal with <code>Algodiff.Generic.Make</code> directly since there are already two ready-made modules.</p>
<ul>
<li><code>Algodiff.S</code> supports <code>float32</code> precision;</li>
<li><code>Algodiff.D</code> supports <code>float64</code> precision;</li>
</ul>
<p><code>Algodiff</code> has implemented both forward and backward mode of AD. The complete list of APIs can be found in <a href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_algodiff_generic.mli">owl_algodiff_generic.mli</a>. The core APIs are listed below.</p>
<div class="highlight">
<pre><code class="language-text">
  val diff : (t -&gt; t) -&gt; t -&gt; t
  (* calculate derivative for f : scalar -&gt; scalar *)

  val grad : (t -&gt; t) -&gt; t -&gt; t
  (* calculate gradient for f : vector -&gt; scalar *)

  val jacobian : (t -&gt; t) -&gt; t -&gt; t
  (* calculate jacobian for f : vector -&gt; vector *)

  val hessian : (t -&gt; t) -&gt; t -&gt; t
  (* calculate hessian for f : scalar -&gt; scalar *)

  val laplacian : (t -&gt; t) -&gt; t -&gt; t
  (* calculate laplacian for f : scalar -&gt; scalar *)
</code></pre>
</div>
<p>Besides, there are also more helper functions such as <code>jacobianv</code> for calculating jacobian vector product; <code>diff'</code> for calculating both <code>f x</code> and <code>diff f x</code>, and etc.</p>
</section>
<section class="level2" id="examples">
<h2>Examples</h2>
<p>Mastering AD requires practice. Let’s see some examples.</p>
<section class="level3" id="higher-order-derivatives">
<h3>Higher-Order Derivatives</h3>
<p>The following code first defines a function <code>f0</code>, then calculates from the first to the fourth derivative by calling <code>Algodiff.AD.diff</code> function.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.D;;

let map f x = Owl.Mat.map (fun a -&gt; a |&gt; pack_flt |&gt; f |&gt; unpack_flt) x;;

(* calculate derivatives of f0 *)
let f0 x = Maths.(tanh x);;
let f1 = diff f0;;
let f2 = diff f1;;
let f3 = diff f2;;
let f4 = diff f3;;

let x = Owl.Mat.linspace (-4.) 4. 200;;
let y0 = map f0 x;;
let y1 = map f1 x;;
let y2 = map f2 x;;
let y3 = map f3 x;;
let y4 = map f4 x;;

(* plot the values of all functions *)
let h = Plot.create "plot_00.png" in
Plot.plot ~h x y0;
Plot.plot ~h x y1;
Plot.plot ~h x y2;
Plot.plot ~h x y3;
Plot.plot ~h x y4;
Plot.output h;;</code></pre>
</div>
<p>Start your <code>utop</code>, then load and open <code>Owl</code> library. Copy and past the code above, the generated figure will look like this.</p>
<figure>
<img alt="" style="width:90.0%" id="fig:algodiff:plot00" title="plot 00" src="images/algodiff/plot_00.png"><figcaption>Figure 2: Higher order derivatives</figcaption>
</figure>
<p>If you replace <code>f0</code> in the previous example with the following definition, then you will have another good-looking figure :)</p>
<div class="highlight">
<pre><code class="language-ocaml">let f0 x = Maths.(
  let y = exp (neg x) in
  (F 1. - y) / (F 1. + y)
);;</code></pre>
</div>
<p>As you see, you can just keep calling <code>diff</code> to get higher and higher-order derivatives. E.g.,</p>
<div class="highlight">
<pre><code class="language-ocaml">let f'''' f = f |&gt; diff |&gt; diff |&gt; diff |&gt; diff</code></pre>
</div>
<p>The code above will give you the fourth derivative of <code>f</code>, i.e.&nbsp;<code>f''''</code>.</p>
</section>
<section class="level3" id="choosing-forward-or-reverse-mode">
<h3>Choosing Forward or Reverse Mode</h3>
<p>Let’s look at the two simple functions <code>f</code> and <code>g</code> defined below. <code>f</code> falls into the first category we mentioned before, i.e., inputs is more than outputs; whilst <code>g</code> falls into the second category.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  open Algodiff.D;;

  (* f : vector -&gt; scalar *)
  let f x =
    let a = Mat.get x 0 0 in
    let b = Mat.get x 0 1 in
    let c = Mat.get x 0 2 in
    Maths.((sin a) + (cos b) + (sqr c))
  ;;

  (* g : scalar -&gt; vector *)
  let g x =
    let a = Maths.sin x in
    let b = Maths.cos x in
    let c = Maths.sqr x in

    let y = Mat.zeros 1 3 in
    let y = Mat.set y 0 0 a in
    let y = Mat.set y 0 1 b in
    let y = Mat.set y 0 2 c in
    y
  ;;
</code></pre>
</div>
<p>According to the rule of thumb, we need to use backward mode to differentiate <code>f</code>, i.e., calculate the gradient of <code>f</code>. How to do that then? Let’s look at the code snippet below.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let x = Mat.uniform 1 3;;           (* generate random input *)
  let x' = make_reverse x (tag ());;  (* init the backward mode *)
  let y = f x';;                      (* forward pass to build computation graph *)
  reverse_prop (F 1.) y;;             (* backward pass to propagate error *)
  let y' = adjval x';;                (* get the gradient value of f *)
</code></pre>
</div>
<p><code>make_reverse</code> function does two things for us: 1) wrap <code>x</code> into type <code>t</code> that Algodiff can process using type constructor <code>DF</code>; 2) generate a unique tag for the input so that input numbers can have nested structure. By calling <code>f x'</code>, we construct the computation graph of <code>f</code> and the graph structure is maintained in the returned result <code>y</code>. Finally, <code>reverse_prop</code> function propagates the error back to the inputs.</p>
<p>In the end, the gradient of <code>f</code> is stored in the adjacent value of <code>x'</code>, and we can retrieve that with <code>adjval</code> function.</p>
<p>How about function <code>g</code> then, the function represents those having a small amount of inputs but a large amount of outputs. According to the rule of thumb, we are suppose to use the forward pass to calculate the derivatives of the outputs w.r.t its inputs.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let x = make_forward (F 1.) (F 1.) (tag ());;  (* seed the input *)
  let y = g x;;                                  (* forward pass *)
  let y' = tangent y;;                           (* get all derivatives *)
</code></pre>
</div>
<p>Forward mode appears much simpler than the backward mode. <code>make_forward</code> function does almost the same thing as <code>make_reverse</code> does for us, the only exception is that <code>make_forward</code> uses <code>DF</code> type constructor to wrap up the input. All the derivatives are ready whenever the forward pass is finished, and they are stored as tangent values in <code>y</code>. We can retrieve the derivatives using <code>tangent</code> function, as we used <code>adjval</code> in the backward mode.</p>
<p>OK, how about we abandon the rule of thumb? In other words, let’s use forward mode to differentiate <code>f</code> rather than using backward mode. Please check the solution below.</p>
<div class="highlight">
<pre><code class="language-text">  Need to be fixed!

  let x0 = make_forward x (Arr Vec.(unit_basis 3 0)) (tag ());;  (* seed the first input variable *)
  let t0 = tangent (f x0);;                                      (* forward pass for the first variable *)

  let x1 = make_forward x (Arr Vec.(unit_basis 3 1)) (tag ());;  (* seed the second input variable *)
  let t1 = tangent (f x1);;                                      (* forward pass for the second variable *)

  let x2 = make_forward x (Arr Vec.(unit_basis 3 2)) (tag ());;  (* seed the third input variable *)
  let t2 = tangent (f x2);;                                      (* forward pass for the third variable *)
</code></pre>
</div>
<p>As we can see, for each input variable, we need to seed individual variable and perform one forward pass. The number of forward passes increase linearly as the number of inputs increases. However, for backward mode, no matter how many inputs there are, one backward pass can give us all the derivatives of the inputs. I guess now you understand why we need to use backward mode for <code>f</code>. One real-world example of <code>f</code> is machine learning and neural network algorithms, wherein there are many inputs but the output is often one scalar value from loss function.</p>
<p>Similarly, you can try to use backward mode to differentiate <code>g</code>. I will just this as an exercise for you. One last thing I want to mention is: backward mode needs to maintain a directed computation graph in the memory so that the errors can propagate back; whereas the forward mode does not have to do that due to the algebra of dual numbers.</p>
<p>In reality, you don’t really need to worry about forward or backward mode if you simply use high-level APIs such as <code>diff</code>, <code>grad</code>, <code>hessian</code>, and etc. However, there might be cases you do need to operate these low-level functions to write up your own applications (e.g., implementing a neural network), then knowing the mechanisms behind the scene is definitely a big plus.</p>
</section>
<section class="level3" id="simple-jacobian-and-gradient">
<h3>Simple Jacobian and Gradient</h3>
<p>REFER: Automatic Differentiation in MATLAB using ADMAT with Applications</p>
</section>
<section class="level3" id="gradient-descent-algorithm">
<h3>Gradient Descent Algorithm</h3>
<p>Gradient Descent (GD) is a popular numerical method for calculating the optimal value for a given function. Often you need to hand craft the derivative of your function <code>f</code> before plugging into gradient descendent algorithm. With <code>Algodiff</code>, derivation can be done easily. The following several lines of code define the skeleton of GD.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.D

let rec desc ?(eta=F 0.01) ?(eps=1e-6) f x =
  let g = (diff f) x in
  if (unpack_flt g) &lt; eps then x
  else desc ~eta ~eps f Maths.(x - eta * g);;</code></pre>
</div>
<p>Now let’s define a function we want to optimise, then plug it into <code>desc</code> function.</p>
<div class="highlight">
<pre><code class="language-ocaml">let f x = Maths.(sin x + cos x);;
let x_min = desc f (F 0.1);;</code></pre>
</div>
<p>Because we started searching from <code>0.</code>, the <code>desc</code> function successfully found the local minimum at <code>-2.35619175250552448</code>. You can visually verify that by plotting it out.</p>
<div class="highlight">
<pre><code class="language-ocaml">let g x = sin x +. cos x in
let h = Plot.create "plot_01.png" in
Plot.plot_fun ~h g (-5.) 5.;
Plot.output h;;</code></pre>
</div>
<figure>
<img alt="" style="width:90.0%" id="fig:algodiff:plot01" title="plot 01" src="images/algodiff/plot_01.png"><figcaption>Figure 3: Gradient descent</figcaption>
</figure>
</section>
<section class="level3" id="newtons-algorithm">
<h3>Newton’s Algorithm</h3>
<p>Newton’s method is a root-finding algorithm by successively searching for better approximation of the root. The Newton’s method converges faster than gradient descent. The following implementation calculates the exact hessian of <code>f</code> which in practice is very expensive operation.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.D

let rec newton ?(eta=F 0.01) ?(eps=1e-6) f x =
  let g, h = (gradhessian f) x in
  if (Maths.l2norm' g |&gt; unpack_flt) &lt; eps then x
  else newton ~eta ~eps f Maths.(x - eta * g *@ (inv h));;</code></pre>
</div>
<p>Now we can apply <code>newton</code> to find the extreme value of <code>Maths.(cos x |&gt; sum')</code>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let f x = Maths.(cos x |&gt; sum') in
newton f (Mat.uniform 1 2)
;;
&gt;- : t = [Arr(1,2)]
</code></pre>
</div>
</section>
</section>
<section class="level2" id="design-of-the-algorithmic-differentiation-module">
<h2>Design of the Algorithmic Differentiation Module</h2>
<section class="level3" id="lazy-evaluation">
<h3>Lazy Evaluation</h3>
</section>
<section class="level3" id="there-is-no-spoon-extend-ad-module">
<h3>“There Is No Spoon”: Extend AD Module</h3>
</section>
</section>
<section class="level2" id="algorithmic-differentiation-the-engine-of-neural-network">
<h2>Algorithmic Differentiation: The Engine of Neural Network</h2>
<p>TODO: Remove this part</p>
<p>In order to understand AD, you need to practice enough, especially if you are interested in the knowing the mechanisms under the hood. I provide some small but representative examples to help you start.</p>
<p>Reference: PyTorch</p>
<section class="level3" id="backpropagation-in-neural-network">
<h3>Backpropagation in Neural Network</h3>
<p>AD was proposed in 1970, and backpropagation was proposed in 1980s. They are different, but backprop is frequently implemented using the reverse mode AD.</p>
<p>Now let’s talk about the hyped neural network. Backpropagation is the core of all neural networks, actually it is just a special case of reverse mode AD. Therefore, we can write up the backpropagation algorithm from scratch easily with the help of <code>Algodiff</code> module.</p>
<div class="highlight">
<pre><code class="language-text">let backprop nn eta x y =
  let t = tag () in
  Array.iter (fun l -&gt;
    l.w &lt;- make_reverse l.w t;
    l.b &lt;- make_reverse l.b t;
  ) nn.layers;
  let loss = Maths.(cross_entropy y (run_network x nn) / (F (Mat.row_num y |&gt; float_of_int))) in
  reverse_prop (F 1.) loss;
  Array.iter (fun l -&gt;
    l.w &lt;- Maths.((primal l.w) - (eta * (adjval l.w))) |&gt; primal;
    l.b &lt;- Maths.((primal l.b) - (eta * (adjval l.b))) |&gt; primal;
  ) nn.layers;
  loss |&gt; unpack_flt</code></pre>
</div>
<p>Yes, we just used only 13 lines of code to implement the backpropagation. Actually, with some extra coding, we can make a smart application to recognise handwritten digits. E.g., running the application will give you the following prediction on handwritten digit <code>6</code>. The code has been included in Owl’s example and you can find the complete example in <a href="https://github.com/owlbarb/owl/blob/master/examples/backprop.ml">backprop.ml</a>.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:algodiff:plot34" title="plot 034" src="images/algodiff/plot_034.png"><figcaption>Figure 4: Mnist experiments on back propagation</figcaption>
</figure>
</section>
<section class="level3" id="example-computation-graph-of-simple-functions">
<h3>Example: Computation Graph of Simple Functions</h3>
<p>Backward mode generates and maintains a computation graph in order to back propagate the error. The computation graph is very helpful in both debugging and understanding the characteristic of your numerical functions. Owl provides two functions to facilitate you in generating computation graphs.</p>
<div class="highlight">
<pre><code class="language-text">  val to_trace: t list -&gt; string
  (* print out the trace in human-readable format *)

  val to_dot : tlist -&gt; string
  (* print out the computation graph in dot format *)</code></pre>
</div>
<p><code>to_trace</code> is useful when the graph is small and you can print it out on the terminal then observe it directly. <code>to_dot</code> is more useful when the graph grows bigger since you can use specialised visualisation tools to generate professional figures, such as Graphviz.</p>
<p>In the following, I will showcase several computation graphs. However, I will skip the details of how to generate these graphs since you can find out in the <a href="https://github.com/ryanrhymes/owl/blob/master/examples/computation_graph.ml">computation_graph.ml</a>.</p>
<p>Let’s start with a simple function as below.</p>
<div class="highlight">
<pre><code class="language-ocaml">let f x y = Maths.((x * sin (x + x) + ( F 1. * sqrt x) / F 7.) * (relu y) |&gt; sum)</code></pre>
</div>
<p>The generated computation graph looks like this.</p>
<figure>
<img alt="" style="width:60.0%" id="fig:algodiff:plot28" title="plot 028" src="images/algodiff/plot_028.png"><figcaption>Figure 5: Computation graph of a simple math function</figcaption>
</figure>
</section>
<section class="level3" id="example-computation-graph-of-vgg-like-neural-network">
<h3>Example: Computation Graph of VGG-like Neural Network</h3>
<p>Let’s define a VGG-like neural network as below.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Neural.S
open Neural.S.Graph

let make_network input_shape =
  input input_shape
  |&gt; normalisation ~decay:0.9
  |&gt; conv2d [|3;3;3;32|] [|1;1|] ~act_typ:Activation.Relu
  |&gt; conv2d [|3;3;32;32|] [|1;1|] ~act_typ:Activation.Relu ~padding:VALID
  |&gt; max_pool2d [|2;2|] [|2;2|] ~padding:VALID
  |&gt; dropout 0.1
  |&gt; conv2d [|3;3;32;64|] [|1;1|] ~act_typ:Activation.Relu
  |&gt; conv2d [|3;3;64;64|] [|1;1|] ~act_typ:Activation.Relu ~padding:VALID
  |&gt; max_pool2d [|2;2|] [|2;2|] ~padding:VALID
  |&gt; dropout 0.1
  |&gt; fully_connected 512 ~act_typ:Activation.Relu
  |&gt; linear 10 ~act_typ:Activation.(Softmax 1)
  |&gt; get_network</code></pre>
</div>
<p>The computation graph for this neural network become a bit more complicated now.</p>
<figure>
<img alt="" id="fig:algodiff:plot29" title="plot 029" src="images/algodiff/plot_029.png"><figcaption>Figure 6: Computation graph of the VGG neural network</figcaption>
</figure>
</section>
<section class="level3" id="example-computation-graph-of-lstm-network">
<h3>Example: Computation Graph of LSTM Network</h3>
<p>How about LSTM network? The following definition seems much lighter than convolutional neural network in the previous example.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Neural.S
open Neural.S.Graph

let make_network wndsz vocabsz =
  input [|wndsz|]
  |&gt; embedding vocabsz 40
  |&gt; lstm 128
  |&gt; linear 512 ~act_typ:Activation.Relu
  |&gt; linear vocabsz ~act_typ:Activation.(Softmax 1)
  |&gt; get_network</code></pre>
</div>
<p>However, the generated computation graph is way more complicated due to LSTM’s internal recurrent structure. You can download the <a href="https://raw.githubusercontent.com/wiki/ryanrhymes/owl/image/plot_030.pdf">PDF file 1</a> for better image quality.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:algodiff:plot30" title="plot 030" src="images/algodiff/plot_030.png"><figcaption>Figure 7: Computation graph of LSTM network</figcaption>
</figure>
</section>
<section class="level3" id="example-computation-graph-of-googles-inception">
<h3>Example: Computation Graph of Google’s Inception</h3>
<p>If the computation graph above hasn’t scared you yet, here is another one generated from Google’s Inception network for image classification. I will not paste the code here since the definition of the network per se is already quite complicated. You can use Owl’s zoo system <code>#zoo "6dfed11c521fb2cd286f2519fb88d3bf"</code>.</p>
<p>The image below is too small to check details, please download the <a href="https://raw.githubusercontent.com/wiki/ryanrhymes/owl/image/plot_031.pdf">PDF file 2</a>.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:algodiff:plot31" title="plot 031" src="images/algodiff/plot_031.png"><figcaption>Figure 8: Computation graph of the InceptionV3 neural network</figcaption>
</figure>
</section>
</section>
</section>
</article></div><a href="optimisation.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 12</small>Optimisation</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>