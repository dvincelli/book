<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Algorithmic Differentiation - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="algorithmic-differentiation">
<h1>Algorithmic Differentiation</h1>
<p>TBD</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>In science and engineering it is often necessary to study the relationship between two or more quantities, where change of one quantity leads to change of others. For example, in describing the motion an object, we describe velocity <span class="math inline">\(v\)</span> of an object with the change of the distance regarding time:</p>
<p><span id="eq:algodiff:def"><span class="math display">\[v = \lim_{\Delta~t}\frac{\Delta~s}{\Delta~t} = \frac{ds}{dt}.\qquad(1)\]</span></span></p>
<p>This relationship <span class="math inline">\(\frac{ds}{dt}\)</span> can be called “<em>derivative</em> of <span class="math inline">\(s\)</span> with respect to <span class="math inline">\(t\)</span>”. This process can be extended to higher dimensional space. For example, think about a solid block of material, placed in a cartesian axis system. You heat it at some part of it and cool it down at some other place, and you can imagine that the temperature <span class="math inline">\(T\)</span> at different position of this block: <span class="math inline">\(T(x, y, z)\)</span>. In this field, we can describe this change with partial derivatives along each axis:</p>
<p><span class="math display">\[\nabla~T = (\frac{\partial~T}{\partial~x}, \frac{\partial~T}{\partial~y}, \frac{\partial~T}{\partial~z}).\]</span></p>
<p>Here, the call the vector <span class="math inline">\(\nabla~T\)</span> <em>gradient</em> of <span class="math inline">\(T\)</span>. The procedure to calculating derivatives and gradients is called <em>differentiating</em>.</p>
<p>Differentiation is crucial in many scientific related fields: find maximum or minimum values using gradient descent (see later chapter); ODE (see later chapter); Non-linear optimisation such as KKT optimality conditions is still a prime application. One new crucial application is in machine learning.</p>
<p>TODO: detail description of application.</p>
<section class="level3" id="chain-rule">
<h3>Chain Rule</h3>
<p>Before diving into how to do differentiation on computers, let’s recall how to do it manually from our Calculus 101.</p>
<p>One of the most rule in performing differentiation is the <em>chain rule</em>. In calculus, the chain rule is a formula to compute the derivative of a composite function. Suppose we have two functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, then the Chain rule states that:</p>
<p><span id="eq:algodiff:chainrule01"><span class="math display">\[F'(x)=f'(g(x))g'(x).\qquad(2)\]</span></span></p>
<p>This seemingly simple rule is one of the basic rule in calculating derivatives. For example, let <span class="math inline">\(y = x^a\)</span>, where <span class="math inline">\(a\)</span> is a real number, and then we can get <span class="math inline">\(y'\)</span> using the chain rule. Specifically, let <span class="math inline">\(y=e^{ln~x^a} = e^{a~ln~x}\)</span>, and then we can set <span class="math inline">\(u= alnx\)</span> so that now <span class="math inline">\(y=e^u\)</span>. By applying the chain rule, we have:</p>
<p><span class="math display">\[y' = \frac{dy}{du}~\frac{du}{dx} = e^u~a~\frac{1}{x} = ax^{a-1}.\]</span></p>
<p>Besides the chain rule, it’s helpful to remember some basic differentiation equations, as shown in tbl.&nbsp;1. Here <span class="math inline">\(x\)</span> is variable and both <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are functions with regard to <span class="math inline">\(x\)</span>. <span class="math inline">\(C\)</span> is constant. Of course, this very short list is incomplete. Please refer to calculus textbooks for more information. Armed with chain rule and these basic equations, wen can begin to solve more differentiation problem than you can imagine.</p>
<div id="tbl:algodiff:chainrule02">
<table style="width:88%;">
<caption>Table 1: A Short Table of Basic Derivatives</caption>
<colgroup>
<col style="width: 33%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Derivatives</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\((u(x) + v(x))'\)</span></td>
<td style="text-align: left;"><span class="math inline">\(u'(x) + v'(x)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\((C\times~u(x))'\)</span></td>
<td style="text-align: left;"><span class="math inline">\(C\times~u'(x)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\((u(x)v(x))'\)</span></td>
<td style="text-align: left;"><span class="math inline">\(u'(x)v(x) + u(x)v'(x)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\((\frac{u(x)}{v(x)})'\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{u'(x)v(x) - u(x)v'(x)}{v^2(x)}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\sin(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\cos(x)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(e^x\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^x\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(log_a(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{x~\textrm{ln}~a}\)</span></td>
</tr>
</tbody>
</table>
</div>
</section>
<section class="level3" id="differentiation-methods">
<h3>Differentiation Methods</h3>
<p>As the models and algorithms become increasingly complex, sometimes the function being implicit, it is impractical to perform manual differentiation. Therefore, we turn to computer-based automated computation methods. There are three: numerical differentiation, symbolic differentiation, and algorithmic differentiation.</p>
<p><strong>Numerical Differentiation</strong></p>
<p>The numerical differentiation comes from the definition of derivative in eq.&nbsp;1. It uses a small step <span class="math inline">\(\delta\)</span> to approximate the limit in the definition:</p>
<p><span class="math display">\[f'(x) = \lim_{\delta~\to~0}\frac{f(x+\delta) - f(x)}{\delta}.\]</span></p>
<p>As long as you knows how to evaluate function <span class="math inline">\(f\)</span>, this method can be applied, and coding this method is also straightforward. However, the problem with this method is prone to truncation errors and round-off errors. The truncation errors is introduced by truncating an infinite sum and approximating it by a finite sum; the round-off error is then caused by representing numbers approximately in numerical computation during this process. Besides, this method is also slow due to requiring multiple evaluation of function <span class="math inline">\(f\)</span>. We’ll discuss it later in the optimisation chapter, since optimisation using gradient is a very important application of differentiation. Some discussion about numerically solving derivative related problems is also covered in the Ordinary Differentiation Equation chapter, where we focus on introducing solving these equations numerically, and how the impact of these errors can be reduced.</p>
<p><strong>Symbolic Differentiation</strong></p>
<p>Symbolic Differentiation is the opposite of numerical solution. It does not involve numerical computation, only math symbol manipulation. The rules we have introduced in tbl.&nbsp;1 are actually expressed in symbols. Think about this function: <span class="math inline">\(f(x_0, x_1, x_2) = x_0 * x_1 * x_2\)</span>. If we compute <span class="math inline">\(\nabla~f\)</span> symbolically, we end up with:</p>
<p><span class="math display">\[\nabla~f = (\frac{\partial~f}{\partial~x_0}, \frac{\partial~f}{\partial~x_1}, \frac{\partial~f}{\partial~x_2}) = (x_1 * x_2, x_0 * x_2, x_1 * x_2).\]</span></p>
<p>It is nice and accurate, leaving limited space for numerical errors. However, you can try to extend the number of variables from 3 to a large number <span class="math inline">\(n\)</span>, which means <span class="math inline">\(f(x) = \prod_{i=0}^{n-1}x_i\)</span>, and then try to perform the symbolic differentiation again.</p>
<p>The point is that, symbolic computations tends to give a very large result for even not very complex functions. It’s easy to have duplicated common sub computations, and produce exponentially large symbolic expressions. Therefore, as intuitive as it is, the symbolic differentiation method can easily takes a lot of memory in computer, and is slow.</p>
<p><strong>Algorithmic Differentiation</strong></p>
<p>Algorithmic differentiation (AD) is a chain-rule based technique for calculating the derivatives with regards to input variables of functions defined in a computer programme. It is also known as automatic differentiation, though strictly speaking AD does not fully automate differentiation and can lead to inefficient code.</p>
<p>It is important to realise that AD is not symbolic differentiation, as we will see in the next section. Even though AD also follows the chain rule, it directly applies numerical computation for intermediate results. Therefore, AD can generate exact results with acceptable speed and memory usage, and therefore highly applicable in various real world applications. Actually, according to <span data-cites="griewank1989automatic" class="citation">(Griewank and others 1989)</span>, the reverse mode of AD yields any gradient vector at no more than five times the cost of evaluating the function <span class="math inline">\(f\)</span> itself. AD has already been implemented in various popular languages, including the <a href="https://pythonhosted.org/ad/"><code>ad</code></a> in Python, <a href="https://www.juliadiff.org/"><code>JuliaDiff</code></a> in Julia, and <a href="http://www.cayugaresearch.com/admat.html"><code>ADMAT</code></a> in MATLAB, etc. In the rest of this chapter, we focus on introducing the AD module in Owl.</p>
</section>
</section>
<section class="level2" id="how-algorithmic-differentiation-works">
<h2>How Algorithmic Differentiation Works</h2>
<p>We have seen the chain rules being applied on simple functions such as <span class="math inline">\(y=x^a\)</span>. Now let’s check how this rule can be applied on more complex computations. Let’s look at the function below:</p>
<p><span id="eq:algodiff:example"><span class="math display">\[y(x_0, x_1) = (1 + e^{x_0~x_1 + sin(x_0)})^{-1}.\qquad(3)\]</span></span></p>
<p>This functions is based on a sigmoid function. Our goal is to compute the partial derivative <span class="math inline">\(\frac{\partial~y}{\partial~x_0}\)</span> and <span class="math inline">\(\frac{\partial~y}{\partial~x_1}\)</span>. To better illustrate this process, we express eq.&nbsp;3 as a graph, as shown in fig.&nbsp;1. At the right side of the figure, we have the final output <span class="math inline">\(y\)</span>, and at the roots of this graph are input variables. The nodes between them indicate constants or intermediate variables that are gotten via basic functions such as <code>sine</code>. All nodes are labelled by <span class="math inline">\(v_i\)</span>. An edge between two nodes represents an explicit dependency in the computation.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:algodiff:example_01" title="example_01" src="images/algodiff/example_01.png"><figcaption>Figure 1: Graph expression of function</figcaption>
</figure>
<p>Based on this graphic representation, there are two major ways to apply the chain rules: the forward differentiation mode, and the reverse differentiation mode (not “backward differentiation”, which is a method used for solving ordinary differential equations). Next, we introduce these two methods.</p>
<section class="level3" id="forward-mode">
<h3>Forward Mode</h3>
<p>Our target is to calculate <span class="math inline">\(\frac{\partial~y}{\partial~x_0}\)</span> (partial derivative regarding <span class="math inline">\(x_1\)</span> should be similar). But don’t be so hurry, let’s start with some earlier intermediate results that might be helpful. For example, what is <span class="math inline">\(\frac{\partial~x_0}{\partial~x_1}\)</span>? 1, obviously. Equally obvious is <span class="math inline">\(\frac{\partial~x_1}{\partial~x_1} = 0\)</span>. It’s just elementary. Now, things gets a bit trickier: what is <span class="math inline">\(\frac{\partial~v_3}{\partial~x_0}\)</span>? Not is a good time to use the chain rule:</p>
<p><span class="math display">\[\frac{\partial~v_3}{\partial~x_0} = \frac{\partial~(x_0~x_1)}{\partial~x_0} = x_1~\frac{\partial~(x_0)}{\partial~x_0} + x_0~\frac{\partial~(x_1)}{\partial~x_0} = x_1.\]</span></p>
<p>After calculating <span class="math inline">\(\frac{\partial~v_3}{\partial~x_0}\)</span>, we can then processed with derivatives of <span class="math inline">\(v_5\)</span>, <span class="math inline">\(v_6\)</span>, all the way to that of <span class="math inline">\(v_9\)</span> which is also the output <span class="math inline">\(y\)</span> we are looking for. This process starts with the input variables, and ends with output variables. Therefore, it is called <em>forward differentiation</em>. We can do simplify the math notations in this process by letting <span class="math inline">\(\dot{v_i}=\frac{\partial~(v_i)}{\partial~x_0}\)</span>. The <span class="math inline">\(\dot{v_i}\)</span> here is called <em>tangent</em> of function <span class="math inline">\(v_i(x_0, x_1, \ldots, x_n)\)</span> with regard to input variable <span class="math inline">\(x_0\)</span>. The forward differentiation mode is sometimes also called “tangent linear” mode.</p>
<p>Now we can present the full forward differentiation calculation process, as shown in tbl.&nbsp;2. Two simultaneous lines of computing happen: on the left hand side is the computation procedure specified by eq.&nbsp;3; on the right side shows computation of derivative for each intermediate variable with regard to <span class="math inline">\(x_0\)</span>. Let’s find out <span class="math inline">\(\dot{y}\)</span> when setting <span class="math inline">\(x_0 = 1\)</span>, and <span class="math inline">\(x_1 = 1\)</span>.</p>
<div id="tbl:algodiff:forward">
<table style="width:93%;">
<caption>Table 2: Computation process of forward differentiation</caption>
<colgroup>
<col style="width: 6%">
<col style="width: 38%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th style="text-align: left;">Intermediate computation</th>
<th style="text-align: left;">Tangent computation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td style="text-align: left;"><span class="math inline">\(v_0 = x_0 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_0}=1\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td style="text-align: left;"><span class="math inline">\(v_1 = x_1 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_1}=0\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td style="text-align: left;"><span class="math inline">\(v_2 = sin(v_0) = 0.84\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_2} = cos(v_0)*\dot{v_0} = 0.54 * 1 = 0.54\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td style="text-align: left;"><span class="math inline">\(v_3 = v_0~v_1 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_3} = v_0~\dot{v_1} + v_1~\dot{v_0} = 1 * 0 + 1 * 1 = 1\)</span></td>
</tr>
<tr class="odd">
<td>4</td>
<td style="text-align: left;"><span class="math inline">\(v_4 = v_2 + v3 = 1.84\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_4} = \dot{v_2} + \dot{v_3} = 1.54\)</span></td>
</tr>
<tr class="even">
<td>5</td>
<td style="text-align: left;"><span class="math inline">\(v_5 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_5} = 0\)</span></td>
</tr>
<tr class="odd">
<td>6</td>
<td style="text-align: left;"><span class="math inline">\(v_6 = \exp{(v_4)} = 6.30\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_6} = \exp{(v_4)} * \dot{v_4} = 6.30 * 1.54 = 9.70\)</span></td>
</tr>
<tr class="even">
<td>7</td>
<td style="text-align: left;"><span class="math inline">\(v_7 = 1\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_7} = 0\)</span></td>
</tr>
<tr class="odd">
<td>8</td>
<td style="text-align: left;"><span class="math inline">\(v_8 = v_5 + v_6 = 7.30\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{v_8} = \dot{v_5} + \dot{v_6} = 9.70\)</span></td>
</tr>
<tr class="even">
<td>9</td>
<td style="text-align: left;"><span class="math inline">\(y = v_9 = \frac{1}{v_8}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dot{y} = \frac{-1}{v_8^2} * \dot{v_8} = -0.18\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>This procedure show in this table can be illustrated in fig.&nbsp;2.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:algodiff:example_01_forward" title="example_01_forward" src="images/algodiff/example_01_forward.png"><figcaption>Figure 2: Example of forward accumulation with computational graph</figcaption>
</figure>
<p>Of course, all the numerical computation here are approximated with only two significant figures.<br>
We can validate this result with algorithmic differentiation module in Owl. If you don’t understand the code, don’t worry. We will cover the detail of this module in detail later.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">open Algodiff.D;;
let f x = 
  let x1 = Mat.get x 0 0 in 
  let x2 = Mat.get x 0 1 in 
  Maths.(div (F 1.) (F 1. + exp (x1 * x2 + (sin x1))))
;;
&gt;val f : t -&gt; t = &lt;fun&gt;
let x = Mat.ones 1 2 ;;
&gt;val x : t = [Arr(1,2)]
let _ = grad f x |&gt; unpack_arr;;
&gt;- : A.arr =
&gt;          C0        C1
&gt;R0 -0.181974 -0.118142
</code></pre>
</div>
<p><strong>TODO:</strong> introduce dual number</p>
</section>
<section class="level3" id="reverse-mode">
<h3>Reverse Mode</h3>
<p>Now let’s think this problem from the other direction, literally. The same questions: to calculate <span class="math inline">\(\frac{\partial~y}{\partial~x_0}\)</span>. We still follow the same “step by step” idea from the forward mode, but the difference is that, we think it backward. For example, here we reduce the problem in this way: since in this graph <span class="math inline">\(y = v_7 / v_8\)</span>, if only we can have <span class="math inline">\(\frac{\partial~y}{\partial~v_7}\)</span> and <span class="math inline">\(\frac{\partial~y}{\partial~v_8}\)</span>, then this problem should be one step closer towards my target problem.</p>
<p>First of course, we have <span class="math inline">\(\frac{\partial~y}{\partial~v_9} = 1\)</span>, since <span class="math inline">\(y\)</span> and <span class="math inline">\(v_9\)</span> are the same. Then how do we get <span class="math inline">\(\frac{\partial~y}{\partial~v_7}\)</span>? Again, time for chain rule:</p>
<p><span id="eq:algodiff:reverse_01"><span class="math display">\[\frac{\partial~y}{\partial~v_7} = \frac{\partial~y}{\partial~v_9} * \frac{\partial~v_9}{\partial~v_7} = 1 * \frac{\partial~v_9}{\partial~v_7} = \frac{\partial~(v_7 / v_8)}{\partial~v_7} = \frac{1}{v_8}.\qquad(4)\]</span></span></p>
<p>Hmm, let’s try to apply a notation to simplify this process. Let</p>
<p><span class="math display">\[\bar{v_i} = \frac{\partial~y}{\partial~v_i}\]</span></p>
<p>be the derivative of output variable <span class="math inline">\(y\)</span> with regard to intermediate node <span class="math inline">\(v_i\)</span>. It is called the <em>adjoint</em> of variable <span class="math inline">\(v_i\)</span> with respect to the output variable <span class="math inline">\(y\)</span>. Using this notation, eq.&nbsp;4 can be expressed as:</p>
<p><span class="math display">\[\bar{v_7} = \bar{v_9} * \frac{\partial~v_9}{\partial~v_7} = 1 * \frac{1}{v_8}.\]</span></p>
<p>Note the difference between tangent and adjoint. In the forward mode, we know <span class="math inline">\(\dot{v_0}\)</span> and <span class="math inline">\(\dot{v_1}\)</span>, then we calculate <span class="math inline">\(\dot{v_2}\)</span>, <span class="math inline">\(\dot{v3}\)</span>, …. and then finally we have <span class="math inline">\(\dot{v_9}\)</span>, which is the target. Here, we start with knowing <span class="math inline">\(\bar{v_9} = 1\)</span>, and then we calculate <span class="math inline">\(\bar{v_8}\)</span>, <span class="math inline">\(\bar{v_7}\)</span>, …. and then finally we have <span class="math inline">\(\bar{v_0} = \frac{\partial~y}{\partial~v_0} = \frac{\partial~y}{\partial~x_0}\)</span>, which is also exactly our target. Again, <span class="math inline">\(\dot{v_9} = \bar{v_0}\)</span> in this example, given that we are talking about derivative regarding <span class="math inline">\(x_0\)</span> when we use <span class="math inline">\(\dot{v_9}\)</span>. Following this line of calculation, the reverse differentiation mode is also called <em>adjoint mode</em>.</p>
<p>With that in mind, let’s see the full steps of performing reverse differentiation. First, we need to perform a forward pass to compute the required intermediate values, as shown in tbl.&nbsp;3.</p>
<div id="tbl:algodiff:reverse_01">
<table style="width:44%;">
<caption>Table 3: Forward pass in the reverse differentiation mode</caption>
<colgroup>
<col style="width: 6%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th style="text-align: left;">Intermediate computation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td style="text-align: left;"><span class="math inline">\(v_0 = x_0 = 1\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td style="text-align: left;"><span class="math inline">\(v_1 = x_1 = 1\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td style="text-align: left;"><span class="math inline">\(v_2 = sin(v_0) = 0.84\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td style="text-align: left;"><span class="math inline">\(v_3 = v_0~v_1 = 1\)</span></td>
</tr>
<tr class="odd">
<td>4</td>
<td style="text-align: left;"><span class="math inline">\(v_4 = v_2 + v3 = 1.84\)</span></td>
</tr>
<tr class="even">
<td>5</td>
<td style="text-align: left;"><span class="math inline">\(v_5 = 1\)</span></td>
</tr>
<tr class="odd">
<td>6</td>
<td style="text-align: left;"><span class="math inline">\(v_6 = \exp{(v_4)} = 6.30\)</span></td>
</tr>
<tr class="even">
<td>7</td>
<td style="text-align: left;"><span class="math inline">\(v_7 = 1\)</span></td>
</tr>
<tr class="odd">
<td>8</td>
<td style="text-align: left;"><span class="math inline">\(v_8 = v_5 + v_6 = 7.30\)</span></td>
</tr>
<tr class="even">
<td>9</td>
<td style="text-align: left;"><span class="math inline">\(y = v_9 = \frac{1}{v_8}\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>You might be wondering, this looks the same as the left side of tbl.&nbsp;2. You are right. These two are exactly the same, and we repeat it again to make the point that, this time you cannot perform the calculation with one pass. You must compute the required intermediate results first, and then perform the other “backward pass”, which is the key point in reverse mode.</p>
<div id="tbl:algodiff:reverse_02">
<table>
<caption>Table 4: Computation process of the backward pass in reverse differentiation</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 94%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th style="text-align: left;">Adjoint computation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_9} = 1\)</span></td>
</tr>
<tr class="even">
<td>11</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_8} = \bar{v_9}\frac{\partial~(v_7/v_8)}{\partial~v_8} = 1 * \frac{-v_7}{v_8^2} = \frac{-1}{7.30^2} = -0.019\)</span></td>
</tr>
<tr class="odd">
<td>12</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_7} = \bar{v_9}\frac{\partial~(v_7/v_8)}{\partial~v_7} = \frac{1}{v_8} = 0.137\)</span></td>
</tr>
<tr class="even">
<td>13</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_6} = \bar{v_8}\frac{\partial~v_8}{\partial~v_6} = \bar{v_8} * \frac{\partial~(v_6 + v5)}{\partial~v_6} = \bar{v_8}\)</span></td>
</tr>
<tr class="odd">
<td>14</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_5} = \bar{v_8}\frac{\partial~v_8}{\partial~v_5} = \bar{v_8} * \frac{\partial~(v_6 + v5)}{\partial~v_5} = \bar{v_8}\)</span></td>
</tr>
<tr class="even">
<td>15</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_4} = \bar{v_6}\frac{\partial~v_6}{\partial~v_4} = \bar{v_8} * \frac{\partial~\exp{(v_4)}}{\partial~v_4} = \bar{v_8} * e^{v_4}\)</span></td>
</tr>
<tr class="odd">
<td>16</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_3} = \bar{v_4}\frac{\partial~v_4}{\partial~v_3} = \bar{v_4} * \frac{\partial~(v_2 + v_3)}{\partial~v_3} = \bar{v_4}\)</span></td>
</tr>
<tr class="even">
<td>17</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_2} = \bar{v_4}\frac{\partial~v_4}{\partial~v_2} = \bar{v_4} * \frac{\partial~(v_2 + v_3)}{\partial~v_2} = \bar{v_4}\)</span></td>
</tr>
<tr class="odd">
<td>18</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_1} = \bar{v_3}\frac{\partial~v_3}{\partial~v_1} = \bar{v_3} * \frac{\partial~(v_0*v_1)}{\partial~v_1} = \bar{v_4} * v_0 = \bar{v_4}\)</span></td>
</tr>
<tr class="even">
<td>19</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_{02}} = \bar{v_2}\frac{\partial~v_2}{\partial~v_0} = \bar{v_2} * \frac{\partial~(sin(v_0))}{\partial~v_0} = \bar{v_4} * cos(v_0)\)</span></td>
</tr>
<tr class="odd">
<td>20</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_{03}} = \bar{v_3}\frac{\partial~v_3}{\partial~v_0} = \bar{v_3} * \frac{\partial~(v_0 * v_1)}{\partial~v_0} = \bar{v_4} * v_1\)</span></td>
</tr>
<tr class="even">
<td>21</td>
<td style="text-align: left;"><span class="math inline">\(\bar{v_0} = \bar{v_{02}} + \bar{v_{03}} = \bar{v_4}(cos(v_0) + v_1) = \bar{v_8} * e^{v_4}(0.54 + 1) = -0.019 * e^{1.84} * 1.54 = -0.18\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Note that things a bit different for <span class="math inline">\(x_0\)</span>. It is used in both intermediate variables <span class="math inline">\(v_2\)</span> and <span class="math inline">\(v_3\)</span>. Therefore, we compute the adjoint of <span class="math inline">\(v_0\)</span> with regard to <span class="math inline">\(v_2\)</span> (step 19) and <span class="math inline">\(v_3\)</span> (step 20), and accumulate them together (step 20). (TODO: Explain why adding these two adjoints.)</p>
<p>Similar to the forward mode, reverse differentiation process in [] can be clearly shown in figure fig.&nbsp;3.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:algodiff:example_01_reverse" title="example_01_reverse" src="images/algodiff/example_01_reverse.png"><figcaption>Figure 3: Example of reverse accumulation with computational graph</figcaption>
</figure>
<p>This result <span class="math inline">\(\bar{v_0} = -0.18\)</span> agrees what we have have gotten using the forward mode. However, if you still need another fold of insurance, we can use Owl to perform a numerical differentiation. The code would be similar to that of using algorithmic differentiation as shown before.</p>
<div class="highlight">
<pre><code class="language-ocaml">module D = Owl_numdiff_generic.Make (Dense.Ndarray.D);;

let x = Arr.ones [|2|] 

let f x = 
    let x1 = Arr.get x [|0|] in 
    let x2 = Arr.get x [|1|] in 
    Maths.(div 1. (1. +. exp (x1 *. x2 +. (sin x1))))</code></pre>
</div>
<p>And then we can get the differentiation result at the point <span class="math inline">\((x_0, x_1) = (0, 0)\)</span>, and it agrees with the previous results.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">D.grad f x;;
&gt;- : D.arr =
&gt;         C0        C1
&gt;R -0.181973 -0.118142
</code></pre>
</div>
</section>
<section class="level3" id="forward-or-reverse">
<h3>Forward or Reverse?</h3>
<p><strong>TODO</strong>: Jacobian: one column or one row at a time.</p>
<p>Since both can be used to differentiate a function then the natural question is which mode we should choose in practice. The short answer is: it depends on your function.</p>
<p>In general, given a function that you want to differentiate, the rule of thumb is:</p>
<ul>
<li>if input variables &gt;&gt; output variables, then use reverse mode;</li>
<li>if input variables &lt;&lt; output variables, then use forward mode.</li>
</ul>
<p>Later we will show example of this point.</p>
<p><strong>Theoretical Basis:</strong> first derivative, higher derivative, etc.</p>
</section>
</section>
<section class="level2" id="implementing-algorithmic-differentiation">
<h2>Implementing Algorithmic Differentiation</h2>
<section class="level3" id="native-implementation">
<h3>Native Implementation</h3>
<p>A most simple one, <code>toy_forward</code>, <code>toy_reverse</code>, support only small number of operators.</p>
</section>
<section class="level3" id="updated-implementations">
<h3>Updated Implementations</h3>
<p>2-3 times of updates</p>
</section>
<section class="level3" id="design-of-algorithmic-differentiation-in-owl">
<h3>Design of Algorithmic Differentiation in Owl</h3>
<p>The structure of main engine: recursive, node, module, etc.</p>
</section>
<section class="level3" id="advanced-feature-lazy-evaluation">
<h3>Advanced feature: Lazy Evaluation</h3>
</section>
<section class="level3" id="advanced-feature-extend-ad-module">
<h3>Advanced feature: Extend AD module</h3>
<p>“There is no spoon”</p>
</section>
</section>
<section class="level2" id="how-to-use-algorithmic-differentiation-in-owl">
<h2>How to use Algorithmic Differentiation in Owl</h2>
<section class="level3" id="high-level-apis">
<h3>High-level APIs</h3>
<p>The design of AD in Owl.</p>
<p>Owl provides both numerical differentiation (in <a href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_numdiff_generic.mli">Numdiff.Generic</a> module) and algorithmic differentiation (in <a href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_algodiff_generic.mli">Algodiff.Generic</a> module).</p>
<p><code>Algodiff.Generic</code> is a functor which is able to support both <code>float32</code> and <code>float64</code> precision <code>AD</code>. However, you do not need to deal with <code>Algodiff.Generic.Make</code> directly since there are already two ready-made modules.</p>
<ul>
<li><code>Algodiff.S</code> supports <code>float32</code> precision;</li>
<li><code>Algodiff.D</code> supports <code>float64</code> precision;</li>
</ul>
<p><code>Algodiff</code> has implemented both forward and backward mode of AD. The complete list of APIs can be found in <a href="https://github.com/ryanrhymes/owl/blob/ppl/src/base/optimise/owl_algodiff_generic.mli">owl_algodiff_generic.mli</a>. The core APIs are listed below.</p>
<div class="highlight">
<pre><code class="language-text">
  val diff : (t -&gt; t) -&gt; t -&gt; t
  (* calculate derivative for f : scalar -&gt; scalar *)

  val grad : (t -&gt; t) -&gt; t -&gt; t
  (* calculate gradient for f : vector -&gt; scalar *)

  val jacobian : (t -&gt; t) -&gt; t -&gt; t
  (* calculate jacobian for f : vector -&gt; vector *)

  val hessian : (t -&gt; t) -&gt; t -&gt; t
  (* calculate hessian for f : scalar -&gt; scalar *)

  val laplacian : (t -&gt; t) -&gt; t -&gt; t
  (* calculate laplacian for f : scalar -&gt; scalar *)
</code></pre>
</div>
<p>Besides, there are also more helper functions such as <code>jacobianv</code> for calculating jacobian vector product; <code>diff'</code> for calculating both <code>f x</code> and <code>diff f x</code>, and etc.</p>
<p>Mastering AD requires practice. Let’s see some examples.</p>
</section>
<section class="level3" id="example-higher-order-derivatives">
<h3>Example: Higher-Order Derivatives</h3>
<p>The following code first defines a function <code>f0</code>, then calculates from the first to the fourth derivative by calling <code>Algodiff.AD.diff</code> function.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Algodiff.D;;

let map f x = Owl.Mat.map (fun a -&gt; a |&gt; pack_flt |&gt; f |&gt; unpack_flt) x;;

(* calculate derivatives of f0 *)
let f0 x = Maths.(tanh x);;
let f1 = diff f0;;
let f2 = diff f1;;
let f3 = diff f2;;
let f4 = diff f3;;

let x = Owl.Mat.linspace (-4.) 4. 200;;
let y0 = map f0 x;;
let y1 = map f1 x;;
let y2 = map f2 x;;
let y3 = map f3 x;;
let y4 = map f4 x;;

(* plot the values of all functions *)
let h = Plot.create "plot_00.png" in
Plot.plot ~h x y0;
Plot.plot ~h x y1;
Plot.plot ~h x y2;
Plot.plot ~h x y3;
Plot.plot ~h x y4;
Plot.output h;;</code></pre>
</div>
<p>Start your <code>utop</code>, then load and open <code>Owl</code> library. Copy and past the code above, the generated figure will look like this.</p>
<figure>
<img alt="" style="width:90.0%" id="fig:algodiff:plot00" title="plot 00" src="images/algodiff/plot_00.png"><figcaption>Figure 4: Higher order derivatives</figcaption>
</figure>
<p>If you replace <code>f0</code> in the previous example with the following definition, then you will have another good-looking figure :)</p>
<div class="highlight">
<pre><code class="language-ocaml">let f0 x = Maths.(
  let y = exp (neg x) in
  (F 1. - y) / (F 1. + y)
);;</code></pre>
</div>
<p>As you see, you can just keep calling <code>diff</code> to get higher and higher-order derivatives. E.g.,</p>
<div class="highlight">
<pre><code class="language-ocaml">let f'''' f = f |&gt; diff |&gt; diff |&gt; diff |&gt; diff</code></pre>
</div>
<p>The code above will give you the fourth derivative of <code>f</code>, i.e.&nbsp;<code>f''''</code>.</p>
</section>
<section class="level3" id="example-choosing-forward-or-reverse-mode">
<h3>Example: Choosing Forward or Reverse Mode</h3>
<p>Let’s look at the two simple functions <code>f</code> and <code>g</code> defined below. <code>f</code> falls into the first category we mentioned before, i.e., inputs is more than outputs; whilst <code>g</code> falls into the second category.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  open Algodiff.D;;

  (* f : vector -&gt; scalar *)
  let f x =
    let a = Mat.get x 0 0 in
    let b = Mat.get x 0 1 in
    let c = Mat.get x 0 2 in
    Maths.((sin a) + (cos b) + (sqr c))
  ;;

  (* g : scalar -&gt; vector *)
  let g x =
    let a = Maths.sin x in
    let b = Maths.cos x in
    let c = Maths.sqr x in

    let y = Mat.zeros 1 3 in
    let y = Mat.set y 0 0 a in
    let y = Mat.set y 0 1 b in
    let y = Mat.set y 0 2 c in
    y
  ;;
</code></pre>
</div>
<p>According to the rule of thumb, we need to use backward mode to differentiate <code>f</code>, i.e., calculate the gradient of <code>f</code>. How to do that then? Let’s look at the code snippet below.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let x = Mat.uniform 1 3;;           (* generate random input *)
  let x' = make_reverse x (tag ());;  (* init the backward mode *)
  let y = f x';;                      (* forward pass to build computation graph *)
  reverse_prop (F 1.) y;;             (* backward pass to propagate error *)
  let y' = adjval x';;                (* get the gradient value of f *)
</code></pre>
</div>
<p><code>make_reverse</code> function does two things for us: 1) wrap <code>x</code> into type <code>t</code> that Algodiff can process using type constructor <code>DF</code>; 2) generate a unique tag for the input so that input numbers can have nested structure. By calling <code>f x'</code>, we construct the computation graph of <code>f</code> and the graph structure is maintained in the returned result <code>y</code>. Finally, <code>reverse_prop</code> function propagates the error back to the inputs.</p>
<p>In the end, the gradient of <code>f</code> is stored in the adjacent value of <code>x'</code>, and we can retrieve that with <code>adjval</code> function.</p>
<p>How about function <code>g</code> then, the function represents those having a small amount of inputs but a large amount of outputs. According to the rule of thumb, we are suppose to use the forward pass to calculate the derivatives of the outputs w.r.t its inputs.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let x = make_forward (F 1.) (F 1.) (tag ());;  (* seed the input *)
  let y = g x;;                                  (* forward pass *)
  let y' = tangent y;;                           (* get all derivatives *)
</code></pre>
</div>
<p>Forward mode appears much simpler than the backward mode. <code>make_forward</code> function does almost the same thing as <code>make_reverse</code> does for us, the only exception is that <code>make_forward</code> uses <code>DF</code> type constructor to wrap up the input. All the derivatives are ready whenever the forward pass is finished, and they are stored as tangent values in <code>y</code>. We can retrieve the derivatives using <code>tangent</code> function, as we used <code>adjval</code> in the backward mode.</p>
<p>OK, how about we abandon the rule of thumb? In other words, let’s use forward mode to differentiate <code>f</code> rather than using backward mode. Please check the solution below.</p>
<div class="highlight">
<pre><code class="language-text">  Need to be fixed!

  let x0 = make_forward x (Arr Vec.(unit_basis 3 0)) (tag ());;  (* seed the first input variable *)
  let t0 = tangent (f x0);;                                      (* forward pass for the first variable *)

  let x1 = make_forward x (Arr Vec.(unit_basis 3 1)) (tag ());;  (* seed the second input variable *)
  let t1 = tangent (f x1);;                                      (* forward pass for the second variable *)

  let x2 = make_forward x (Arr Vec.(unit_basis 3 2)) (tag ());;  (* seed the third input variable *)
  let t2 = tangent (f x2);;                                      (* forward pass for the third variable *)
</code></pre>
</div>
<p>As we can see, for each input variable, we need to seed individual variable and perform one forward pass. The number of forward passes increase linearly as the number of inputs increases. However, for backward mode, no matter how many inputs there are, one backward pass can give us all the derivatives of the inputs. I guess now you understand why we need to use backward mode for <code>f</code>. One real-world example of <code>f</code> is machine learning and neural network algorithms, wherein there are many inputs but the output is often one scalar value from loss function.</p>
<p>Similarly, you can try to use backward mode to differentiate <code>g</code>. I will just this as an exercise for you. One last thing I want to mention is: backward mode needs to maintain a directed computation graph in the memory so that the errors can propagate back; whereas the forward mode does not have to do that due to the algebra of dual numbers.</p>
<p>In reality, you don’t really need to worry about forward or backward mode if you simply use high-level APIs such as <code>diff</code>, <code>grad</code>, <code>hessian</code>, and etc. However, there might be cases you do need to operate these low-level functions to write up your own applications (e.g., implementing a neural network), then knowing the mechanisms behind the scene is definitely a big plus.</p>
</section>
<section class="level3" id="example-simple-jacobian-and-gradient">
<h3>Example: Simple Jacobian and Gradient</h3>
<p>The principle is to cover most of what we have listed in the API with examples.</p>
</section>
<section class="level3" id="more-examples-in-book">
<h3>More Examples in Book</h3>
<p>Differentiation is an important topic in scientific computing, and therefore is not limited to only this chapter in our book. We use AD in the newton method to find extreme values in optimisation problem in the Optimisation chapter. It is also used in the Regression chapter to solve the linear regression problem with gradient descent. More importantly, the algorithmic differentiation is core module in many modern deep neural libraries such as PyTorch. The neural network module in Owl benefit a lot from our solid AD module. We will elaborate these aspects in the following chapters. Stay tuned!</p>
</section>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-griewank1989automatic">
<p>Griewank, Andreas, and others. 1989. “On Automatic Differentiation.” <em>Mathematical Programming: Recent Developments and Applications</em> 6 (6): 83–107.</p>
</div>
</div>
</section>
</section>
</article></div><a href="optimisation.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 12</small>Optimisation</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>