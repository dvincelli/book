<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Core Optimisation - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="core-optimisation">
<h1>Core Optimisation</h1>
<p>TBD</p>
<section class="level2" id="background">
<h2>Background</h2>
<section class="level3" id="numerical-libraries">
<h3>Numerical Libraries</h3>
<p>There are two widely used specifications of low level linear algebra routines. Basic Linear Algebra Subprograms (BLAS) consists of three levels of routines, from vector to matrix-vector and then to matrix-matrix operations. The other one, Linear Algebra Package (LAPACK), specifies routines for advanced numerical linear algebra, including solving systems of linear equations, linear least squares, eigenvalue problems, SVD, etc.</p>
<p>The implementations of these specifications vary in different libraries, e.g.&nbsp;OpenBLAS~ and Math Kernel Library (MKL). OpenBLAS is a popular open source optimised BLAS library. MKL is a proprietary library, and provides highly optimised mathematical functions on Intel processors. It implements not only BLAS and LAPACK but also FFT and other computationally intensive mathematical functions. Another implementation is Eigen, a C++ template linear algebra library. The CPU implementation of many kernels in TensorFlow uses the Eigen Tensor class. The Automatically Tuned Linear Algebra Software (ATLAS) is another BLAS implementation, featuring automatically-tuned routines on specific hardware.</p>
<p>These basic libraries focus on optimising the performance of operations in different hardware and software environment, but they don’t provide APIs that are easy to use for end users. That requires libraries such as NumPy, Julia, Matlab, and Owl. NumPy is the fundamental package for scientific computing with Python. It contains a powerful N-dimensional array abstraction. Julia is a high-level, high-performance dynamic programming language for numerical computing. Both are widely used and considered state of the art in numerical computing. Both NumPy and Julia rely on OpenBLAS or MKL for linear algebra backends. Matlab, the numerical computing library that has millions of uses worldwide, also belongs to this category.</p>
<p>Deep learning libraries such as TensorFlow, PyTorch, and MxNet are popular. Keras is a user-friendly neural networks API that can run on top of TensorFlow. Instead of the wide range of numerical functionalities that NumPy etc. provide, these libraries focus on building machine learning applications for both research and production. Owl library provides its own neural network module.</p>
</section>
<section class="level3" id="optimisation-of-numerical-computation">
<h3>Optimisation of Numerical Computation</h3>
<p>To achieve optimal performance has always been the target of numerical libraries. However, the complexity of current computation platforms is growing fast, and the “free” performance boost that benefits from hardware upgrade also stagnates. These factors have made it difficult to achieve the optimal performance. Below list some of the techniques that I use to optimise operations in Owl.</p>
<p>One method to utilise the parallelism of a computation platform is to use the Single Instruction Multiple Data (SIMD) instruction sets. They exploit data level parallelism by executing the same instruction on a set of data simultaneously, instead of repeating it multiple times on a single scalar value. One easy way to use SIMD is to rely on the automatic vectorisation capabilities of modern compilers, but in many cases developers have to manually vectorise their code with SIMD intrinsic functions. The Intel Advanced Vector Extensions (AVX) instruction set is offered on Intel and AMD processors, and the ARM processors provide the Advanced SIMD (NEON) extension.</p>
<p>Another form of parallelism is to execute instructions on multiple cores. OpenMP is a C/C++/FORTRAN compiler extension that allows shared memory multiprocessing programming. It is widely supported on compilers such as GCC and Clang, on different hardware platforms. It is important for a numerical library to porting existing code to the OpenMP standard.</p>
<p>To achieve optimal performance often requires choosing the most suitable system parameters on different machines or for different inputs. Aiming at providing fast matrix multiplication routines, the ATLAS library runs a set of micro-benchmarks to decide hardware specifications, and then search for the most suitable parameters such as block size in a wide tuning space.</p>
<p>One general algorithm cannot always achieve optimal performance. One of the most important techniques the Julia uses is ``multiple dispatch’’, which means that the library provides different specific implementations according to the type of inputs.</p>
<p>Besides these techniques, the practical experience from others always worth learning during development. These principles still hold true in the development of modern numerical libraries. An optimised routine can perform orders of magnitude faster than a naive implementation.</p>
</section>
</section>
<section class="level2" id="interfacing-to-c-code">
<h2>Interfacing to C Code</h2>
<p>We interface to C. It’s common practice</p>
<p>How NumPy and Julia interfaces</p>
<p>The core operations are ndarray operation.</p>
<p>The other, such as FFT, Linear Algebra, are explained in their respective chapter.</p>
<section class="level3" id="ndarray-operations">
<h3>Ndarray Operations</h3>
<p>N-dimensional array (ndarray) is the core data type of a numerical library. As we have seen in the previous chapter</p>
<p>The internals about ndarray:</p>
<p>An ndarray is a container of items of the same type. It consists of a contiguous block of memory, combined with an indexing scheme that maps N integers into the location of an item in the block. A stride indexing scheme can then be applied on this block of memory to access elements. Two widely used types of indexing schemes are <em>column-major</em> that is used in FORTRAN and <em>row-major</em> of C.</p>
<p>List the categories of operations that are optimised with C.</p>
<ul>
<li>Math, including map, fold, cmp</li>
<li>Conv and pooling</li>
<li>Repeat</li>
<li>Slicing</li>
<li>Sort</li>
<li>Transpose</li>
<li>Slicing</li>
<li>matrix: swap, check (is_*)</li>
<li>Contract</li>
<li>Slide</li>
</ul>
<p>Why choose these operations</p>
</section>
<section class="level3" id="from-ocaml-to-c">
<h3>From OCaml to C</h3>
<p>Let’s use examples to see exactly how we implement core operations wih C and interface them to OCaml.</p>
<p>In Owl, ndarray is built on OCaml’s native <code>Bigarray.Genarray</code>. The Bigarray module implements multi-dimensional numerical arrays of integers and floating-point numbers, and <code>Genarray</code> is the type of <code>Bigarrays</code> with variable numbers of dimensions.</p>
<p>Genarray is of type <code>('a, 'b, 't) t</code>. It has three parameters: OCaml type for accessing array elements (<code>'a</code>), the actual type of array elements (<code>'b</code>), and indexing scheme (<code>'t</code>). The initial design of Owl supports both col-major and row-major indexing, but this choice leads to a lot of confusion, since the FORTRAN way of indexing starts from index 1, while the row-major starts from 0. Owl sticks with the row-major scheme now, and therefore in the core library the owl ndarray is define as:</p>
<div class="highlight">
<pre><code class="language-ocaml">open Bigarray
type ('a, 'b) owl_arr = ('a, 'b, c_layout) Genarray.t</code></pre>
</div>
<p>Now, let’s look at the <code>'a</code> and <code>'b</code>. In the GADT type <code>('a, 'b) kind</code>, an OCaml type <code>'a</code> is for values read or written in the Bigarray, such as <code>int</code> or <code>float</code>, and <code>'b</code> represents the actual contents of the Bigarray, such as the <code>float32_elt</code> that contains 32-bit single precision floats. Owl supports four basic types of element: float, double, float complex, and double complex number. And we use the definition of type <code>('a, 'b) kind</code> in the <code>BigArray</code> module.</p>
<div class="highlight">
<pre><code class="language-ocaml">open Bigarray

type ('a, 'b) kind = 
|   Float32 : (float, float32_elt) kind
|   Float64 : (float, float64_elt) kind
|   Complex32 : (Complex.t, complex32_elt) kind
|   Complex64 : (Complex.t, complex64_elt) kind</code></pre>
</div>
<p>Suppose we want to implement the sine math function, which maps the <code>sin</code> function on every elements in the . We we need to implement four different versions, each for one of these four number types. The basic code looks like this:</p>
<div class="highlight">
<pre><code class="language-text">let _owl_sin : type a b. (a, b) kind -&gt; int -&gt; ('a, 'b) owl_arr -&gt; ('a, 'b) owl_arr -&gt; unit =
  fun k l x y -&gt;
  match k with
  | Float32   -&gt; owl_float32_sin l x y
  | Float64   -&gt; owl_float64_sin l x y
  | Complex32 -&gt; owl_complex32_sin l x y
  | Complex64 -&gt; owl_complex64_sin l x y
  | _         -&gt; failwith "_owl_sin: unsupported operation"</code></pre>
</div>
<p>The <code>_owl_sin</code> implementation takes four input parameter, the first is the number type <code>kind</code>, the second is the total number of elements <code>l</code> to apply the <code>sin</code> function, the third one <code>x</code> is the source ndarray, and the final one <code>y</code> is the target ndarray. This function apply the <code>sin</code> function on the first <code>l</code> elements from <code>x</code> and then put the results in <code>y</code>. Therefore we can simply add a simple layer of wrapper around this function in the <code>Dense</code> module:</p>
<div class="highlight">
<pre><code class="language-text">let sin x =
  let y = copy x in
  _owl_sin (kind x) (numel y) x y;
  y</code></pre>
</div>
<p>But wait, what are the <code>owl_float32_sin</code> and <code>owl_float64_sin</code> etc. in <code>_owl_sin</code> function? How are they implemented? Let’s take a look:</p>
<div class="highlight">
<pre><code class="language-clike">external owl_float32_sin
  : int
  -&gt; ('a, 'b) owl_arr
  -&gt; ('a, 'b) owl_arr
  -&gt; unit
  = "float32_sin"</code></pre>
</div>
<p>OCaml provides mechanism for interfacing with C using the <code>external</code> keyword: <code>external ocaml-function-name : type = c-function-name</code>. This defines the value name <code>ocaml-function-name</code> as a function with type <code>type</code> that executes by calling the given C function <code>c-function-name</code>. Here we already have a C function that is called “float32_sin”, and <code>owl_float32_sin</code> calls that function.</p>
<p>Now, finally, we venture into the world of C. We first need to include the necessary header files provided by OCaml:</p>
<div class="highlight">
<pre><code class="language-c">#include &lt;caml/mlvalues.h&gt; // definition of the value type, and conversion macros
#include &lt;caml/alloc.h&gt; //allocation functions to create structured ocaml objects 
#include &lt;caml/memory.h&gt; // miscellaneous memory-related functions and macros
#include &lt;caml/fail.h&gt; //functions for raising exceptions
#include &lt;caml/callback.h&gt; // callback from C to ocaml
#include &lt;caml/threads.h&gt; //operations for interfacing in the presence of multiple threads</code></pre>
</div>
<p>In the C file, the outlines of function <code>float32_sin</code> is:</p>
<div class="highlight">
<pre><code class="language-c">CAMLprim value float32_sin(value vN, value vX, value vY) {
  ...
}</code></pre>
</div>
<p>To define a C primitive to interface with OCaml, we use the <code>CAMLprim</code> macro. Unlike normal C functions, all the input types and output type are defined as <code>value</code> instead of <code>int</code>, <code>void</code> etc. It is represents OCaml values and encodes objects of several base types such as integers, strings, floats, etc. as well as OCaml data structures. The specific type of input will be passed in when the functions is called at runtime.</p>
<p>Now let’s look at the content within this function. First, the input is of type <code>value</code> and we have to change them into the normal types for further processing.</p>
<div class="highlight">
<pre><code class="language-c">CAMLparam3(vN, vX, vY);
int N = Long_val(vN);
struct caml_ba_array *X = Caml_ba_array_val(vX);
float *X_data = (float*) X-&gt;data;
struct caml_ba_array *Y = Caml_ba_array_val(vY);
float *Y_data = (float *) Y-&gt;data;</code></pre>
</div>
<p>These “value” type parameters or local variables must be processed with one of the CAMLparam macros. Here we use the <code>CAMLparam3</code> macro since there are three parameters. There are six CAMLparam macros from CAMLparam0 to CAMLparam5, taking zero to five parameters. For more than five parameters, you can first call <code>CAMLparam5</code>, and then use one or more <code>CAMLxparam1</code> to <code>CAMLxparam5</code> functions after that.</p>
<p>The next step we convert the <code>value</code> type inputs into normal types. The <code>Long_val</code> macro convert the value into long int type. Similarly, there are also <code>Double_val</code>, <code>Int32_val</code> etc. We convert the Bigarray to a structure to the structure of type <code>caml_ba_array</code>. The function <code>Caml_ba_array_val</code> returns a pointer to this structure. Its member <code>data</code> is a pointer to the data part of the array. Besides, the information of ndarray dimension is also included. The member <code>num_dims</code> of <code>caml_ba_array</code> is the number of dimensions, and <code>dim[i]</code> is the i-th dimension.</p>
<p>One more thing to do before the “real” coding. If the computation is complex, we don’t want all the OCaml threads to be stuck. Therefore, we need to call the <code>caml_release_runtime_system</code> function to release the master lock and other OCaml resources, so as to allow other threads to run.</p>
<div class="highlight">
<pre><code class="language-c">caml_release_runtime_system(); </code></pre>
</div>
<p>Finally, we can do the real computation, and now that we have finished converting the input data to the familiar types, the code itself is straight forward;</p>
<div class="highlight">
<pre><code class="language-c">float *start_x, *stop_x;
float *start_y;

start_x = X_data;
stop_x = start_x + N;
start_y = Y_data;

while (start_x != stop_x) {
    float x = *start_x;
    *start_y = sinf(X);
    start_x += 1;
    start_y += 1;
};</code></pre>
</div>
<p>That’s all, we move the pointers forward and apply the <code>sinf</code> function from the C standard library one by one.</p>
<p>As you can expect, when all the computation is finished, we need to end the multiple threading.</p>
<div class="highlight">
<pre><code class="language-c">caml_acquire_runtime_system();</code></pre>
</div>
<p>And finally, we need to return the result with <code>CAMLreturn</code> macro – not normal type, but the <code>value</code> type. In this function we don’t need to return anything, so we use the <code>Val_unit</code> macro:</p>
<div class="highlight">
<pre><code class="language-c">CAMLreturn(Val_unit);</code></pre>
</div>
<p>That’s all for this function. But if we want to return a, say, long int, you can the use <code>Val_long</code> to wrap an int type into <code>value</code> type. In the Owl core C code, we normally finish the all the computation and copy the result in-place, and then returns <code>Val_unit</code>, as shown in this example.</p>
<p>Now that we finish <code>float32_sin</code>, we can copy basically all the code above and implement the rest three functions: <code>float64_sin</code>, <code>complex32_sin</code>, and <code>complex64_sin</code>. However, this kind of coding practice is apparently not ideal. Instead, in the core implementation, Owl utilises the macros and templates of C. In the above implementation, we abstract out the only three special part: function name, math function used, and data type. We assign macro <code>FUN</code> to the first one, <code>MAPFN</code> to the next, and <code>NUMBER</code> to the third. Then the function is written as a template:</p>
<div class="highlight">
<pre><code class="language-c">CAMLprim value FUN(value vN, value vX, value vY) {
  ...
  NUMBER *X_data = (NUMBER *) X-&gt;data;
  ...
  *start_y = (MAPFN(x));
  ...
}</code></pre>
</div>
<p>This template is defined in the file <code>owl_ndarray_maths_map.h</code> file. In anther stub C file, these macros are defined as:</p>
<div class="highlight">
<pre><code class="language-c">#define FUN float32_sin
#define NUMBER float
#define MAPFN(X) (sinf(X))
#include "owl_ndarray_maths_map.h"</code></pre>
</div>
<p>In this way, we can easily extend this template to other data types. To extend it to complex number, we can use the <code>_Complex float</code> and <code>_Complex double</code> as number type, and the <code>csinf</code> and <code>csin</code> for math function function on complex data type.</p>
<div class="highlight">
<pre><code class="language-c">#define FUN4 complex32_sin
#define NUMBER _Complex float
#define MAPFN(X) (csinf(X))
#include "owl_ndarray_maths_map.h"</code></pre>
</div>
<p>Once finished the template, we can find that, this template does not only apply to <code>sin</code>, but also the other triangular functions, and many more other similar unary math function that accept one input, such as <code>exp</code> and <code>log</code>, etc.</p>
<div class="highlight">
<pre><code class="language-c">#define FUN float32_log
#define NUMBER float
#define MAPFN(X) (logf(X))
#include "owl_ndarray_maths_map.h"</code></pre>
</div>
<p>Of course, the template can become quite complex for other types of function. But by utilising the template and macros, the core C code of Owl is much simplified. A brief recap: in the core module we are talking about three files. The first one is a ocaml file that contains functions like <code>_owl_sin</code> that interfaces to C code using <code>external</code> keyword. Then the C implementation is divided into the template file, normally as a <code>.h</code> header file, and is named as <code>*_impl.h</code>. The stub that finally utilises these templates to generate functions are put into <code>*_stub.c</code> files.</p>
<p>Note that if the input parameters are more than 5, then two primitives should be implemented. The first <code>bytecode</code> function takes two arguments: a pointer to a list of <code>value</code> type arguments, and an integer that indicating the number of arguments provided. The other <code>native</code> function takes its arguments directly. The syntax of using <code>external</code> should also be changed to include both functions.</p>
<div class="highlight">
<pre><code class="language-clike">external name : type = bytecode-C-function-name native-code-C-function-name</code></pre>
</div>
<p>For example, in our implementation of convolution we have a pair of functions:</p>
<div class="highlight">
<pre><code class="language-c">CAMLprim value FUN_NATIVE (spatial) (
  value vInput_ptr, value vKernel_ptr, value vOutput_ptr,
  value vBatches, value vInput_cols, value vInput_rows, value vIn_channel,
  value vKernel_cols, value vKernel_rows,
  value vOutput_cols, value vOutput_rows, value vOut_channel,
  value vRow_stride,  value vCol_stride,
  value vPadding, value vRow_in_stride, value vCol_in_stride
) {
  ....
}

CAMLprim value FUN_BYTE (spatial) (value * argv, int argn) {
  return FUN_NATIVE (spatial) (
    argv[0], argv[1], argv[2], argv[3], argv[4], argv[5], argv[6], argv[7],
    argv[8], argv[9], argv[10], argv[11], argv[12], argv[13], argv[14],
    argv[15], argv[16]
  );
}</code></pre>
</div>
<p>In the stub we define the function name macros:</p>
<div class="highlight">
<pre><code class="language-clike">#define FUN_NATIVE(dim) stub_float32_ndarray_conv ## _ ## dim  ## _ ## native
#define FUN_BYTE(dim) stub_float32_ndarray_conv ## _ ## dim  ## _ ## bytecode</code></pre>
</div>
<p>And therefore in the OCaml interfacing code we interface to C code with:</p>
<div class="highlight">
<pre><code class="language-clike">external owl_float32_ndarray_conv_spatial 
  :  ('a, 'b) owl_arr -&gt; ('a, 'b) owl_arr -&gt; ('a, 'b) owl_arr -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; int -&gt; unit
  = "stub_float32_ndarray_conv_spatial_bytecode" "stub_float32_ndarray_conv_spatial_native"</code></pre>
</div>
<p>More details of interfacing to C code OCaml can be found in the OCaml <a href="https://caml.inria.fr/pub/docs/manual-ocaml/intfc.html">documentation</a>. Another approach is to use the Foreign Function Interface, as explained <a href="https://dev.realworldocaml.org/foreign-function-interface.html">here</a>.</p>
</section>
</section>
<section class="level2" id="optimisation-techniques">
<h2>Optimisation Techniques</h2>
<p>We try to apply multiple techniques if possible.</p>
<p>Show some optimisation techniques at the C level, and demonstrate their effect. It does not have to perform better than every one.</p>
<p>In this section, I choose representative operations. I compare their performance in different numerical libraries: Owl, NumPy, and Julia. The purpose is two-fold: first, to bring insight into the low-level structure design; second, to demonstrate the possible optimisations in implementing these operations.</p>
<p>In the performance measurements, I use multiple input sizes, and observe the execution time and memory usage. The experiments are conducted on both a laptop (Thinkpad T460s, Core i5 CPU) and a Raspberry Pi (rPi) 3B model. They represent different CPU architectures and computation power.</p>
<section class="level3" id="map-operations">
<h3>Map Operations</h3>
<p>The <code>map</code> operations are a family of operations that accept ndarray as input, and apply a function on all the elements in the ndarray. I use the trigonometric <code>sin</code> operation as a representative map arithmetic operation in this section. It requires heavy computation. In the implementation, it directly calls the low-level C functions via a single template. It calls function <code>MAPFN</code> on one array element-wise, and the result is put in the other array. In the case of sine function, it calls <code>sinf</code> or <code>sin</code> function from the C standard library <code>libc</code>.</p>
<div class="highlight">
<pre><code class="language-clike">for (int i = 0; i &lt; N; i++) {
    NUMBER x = *(start_x + i);
    *(start_y + i) = (MAPFN(x));
}</code></pre>
</div>
<p>Therefore, the performance is mainly decided by the linked low level library, and may be affected by the cost of wrapper around these libraries. Both vectorisation and parallelisation techniques can be utilised to improve its performance.</p>
<p>Applying computation-intensive operations such as sine in a for-loop can be vectorised using SIMD instructions. The computation performance can be boosted by executing single instruction on multiple data in the input ndarray. There are SIMD implementations of certain mathematical functions , and compilers such as GNU GCC also provide options to automatically generating vectorised loop.</p>
<p>Map function can also benefit from parallel execution on the multi-core CPU, such as using OpenMP. To parallelise a loop in C program, I only need to add a single line of OpenMP compiler directive, as shown in code below.</p>
<div class="highlight">
<pre><code class="language-clike">for (int i = 0; i &lt; N; i++) {
    NUMBER x = *(start_x + i);
    *(start_y + i) = (MAPFN(x));
}</code></pre>
</div>
<p>Now that I have different versions of implementation: normal, SIMD version, and OpenMP version. The mechanism for switching between these different implementations in Owl is to provide another set of operations and switch depending on configuration flags. For example, the OpenMP implementation is switched as below.</p>
<div class="highlight">
<pre><code class="language-clike">#ifdef _OPENMP
#define OWL_NDARRAY_MATHS_MAP  "owl_ndarray_maths_map_omp.h"
#else
#define OWL_NDARRAY_MATHS_MAP  "owl_ndarray_maths_map.h"
#endif</code></pre>
</div>
<figure>
<img alt="" style="width:50.0%" id="fig:core-opt:op_eval_sin_tp" src="images/core-opt/opeval_tp_sin_00.png"><figcaption>Figure 1: Sin operation on laptop</figcaption>
</figure>
<figure>
<img alt="" style="width:50.0%" id="fig:core-opt:op_eval_sin_rpi" src="images/core-opt/opeval_rpi_sin_00.png"><figcaption>Figure 2: Sin operation on rPi</figcaption>
</figure>
<p>To measure performance, I compare the sine operation in Owl, NumPy, Julia, and C. The compiling flags in C and Owl are set to the same level 3 optimisation. The input is a vector of single-precision float numbers. I increase the input size from 100,000 to 5,000,000 gradually. The comparison results are shown in fig.&nbsp;1 and fig.&nbsp;2.</p>
<p>It can be seen that the execution time in Owl grows linearly with input size, and very similar to that of C library. Julia has large deviation, but it performs fastest on rPi, even faster than C. It is because of Julia utilises NEON, the SIMD architecture extension on ARM. In some cases, NumPy can be compiled with MKL library. The MKL Vector Math functions provide highly optimised routines for trigonometric operations. In this evaluation I use NumPy library that is not compiled with MKL, and it performs close to Owl and C, with slightly larger deviation.</p>
<p>Note the trade-off in code design. My current approach is a common template for all map functions, and relies on the C library for implementation. A specific implementation using SIMD for each operation would perform better, but that would require more complex logic to decide the execution hardware and software environment, and the code structure would be less generic.</p>
<p>Another optimisation is to remove the memory copy phase by applying mutable operations. A mutable operation does not create new memory space before calculation, but instead utilise existing memory space of input ndarray.</p>
</section>
<section class="level3" id="reduction-operations">
<h3>Reduction Operations</h3>
<p>Reduction operations such as <code>sum</code> and <code>max</code> accumulate values in an ndarray along certain axes by certain functions. For example, <code>sum</code> is used for implementing BatchNormalisation neuron, which is a frequently used neuron in DNN.</p>
<p>A naive implementation of multi-axes <code>sum</code> operation is to repeat sum operation along one axis for each axis specified. However, it creates extra temporary intermediate results. In applications such as DNN, the inefficiency of reduction operation becomes a memory and performance bottleneck. The implementation is shown below.</p>
<div class="highlight">
<pre><code class="language-clike">int iy = 0;
int cnt = 0;

for (int ix = 0; ix &lt; N;) {
  for (int k = 0; k &lt; innersize; k++) {
    ACCFN(y[iy+k], x[ix+k]);
  }

  ix += innersize;
  cnt++;

  if (cnt == loopsize) {
    cnt = 0;
    int residual;
    int iterindex = ix;
    int pre_iteridx = ix;

    for (int i = ndim - 1; i &gt;= 0; i--) {
      iterindex /= x_shape[i];
      residual = pre_iteridx - iterindex * x_shape[i];
      iy += residual * strides[i];
      pre_iteridx = iterindex;
    }
  }
}</code></pre>
</div>
<p>The <code>ACCFN(y, x)</code> function in this template is the accumulation function that is unique to each operation. For <code>sum</code>, it adds <code>y</code> and <code>x</code> and accumulate the value to <code>y</code>. In this algorithm, the elements in original ndarray <code>x</code> and the target reduced ndarray <code>y</code> are iterated one-by-one, but at different steps, indicating by <code>iy</code> and <code>ix</code>.</p>
<p>One optimisation step before this algorithm is to combine adjacent axes. For example, if an ndarray of shape <span class="math inline">\([2,3,4,5]\)</span> is to be reduced along the second and third axis, then it can be simplified to reducing an ndarray of shape <span class="math inline">\([2,12,5]\)</span>.</p>
<figure>
<img alt="" style="width:60.0%" id="fig:core-opt:opeval_sumreduce" src="images/core-opt/opeval_tp_sum_reduce_mem_00.png"><figcaption>Figure 3: Sin operation on laptop</figcaption>
</figure>
<p>Since it involves multiple axes, to evaluate the reduction operation, I use a four-dimensional ndarray of float numbers as input. All four dimensions are of the same length. I measure the peak memory usage with increasing length, each for axis equals to 0, 1, and both 0 and 2 dimension. The evaluation result compared with NumPy and Julia is shown in fig.&nbsp;3.</p>
</section>
<section class="level3" id="repeat-operations">
<h3>Repeat Operations</h3>
<p><code>Repeat</code> is another operation that is frequently used in DNN. It is used for implementing the Upsampling and BatchNormalisation neurons. The <code>repeat</code> operation repeats elements of an ndarray along each axis for specified times. It consists of inner repeat and outer repeat (or <code>tile</code>). The former repeats elements of an input ndarray, while the later constructs an ndarray by repeating the whole input ndarray by specified number of times along each axis.</p>
<p>Similar to the reduction functions, a multi-axes repeat function can hardly achieve ideal performance by simply using existing operation for multiple times. I implement multi-axes repeat in Owl and it outperforms NumPy and Julia.</p>
<p>The optimisation I use in the algorithm follows two patterns. The first is to provide multiple implementations for different inputs. For example, if only one axis is used, then a specific implementation for that case would be much faster than a general solution. The second is to reduce intermediate results. Similar to the reduction operations, a multiple-axes <code>repeat</code> could be implemented by multiple single axis operation, but it would lead to extra memory usage and much slower execution speed.</p>
<p>The core code of my proposed repeat algorithm is shown below. Here I define <code>HD</code> to be the highest non-one-repeat dimension, copy the HD dimension from source ndarray to target ndarray, and then copy the lower dimensions within target ndarray.</p>
<div class="highlight">
<pre><code class="language-clike">int block_num[HD];
for (int i = 0; i &lt; HD; ++i) {
  block_num[i] = slice_x[i] / slice_x[HD];
}
int counter[HD];
memset(counter, 0, sizeof(counter));

int ofsx = 0;
int ofsy = 0;
int block_sz = reps[HD];
int num_hd = block_num[0];

/* Copy the last-dim block */
for (int i = 0; i &lt; num_hd; ++i) {
  int ofsy_sub = ofsy;
  if (block_sz == 1) {
    COPYFUN(slice_x[HD], x, ofsx, 1, y, ofsy, 1);
  } else {
    for (int j = 0; j &lt; slice_x[HD]; ++j) {
      COPYFUN(block_sz, x, ofsx + j, 0, y, ofsy_sub, 1);
      ofsy_sub += block_sz;
    }
  }

  /* Increase index */
  ofsx += slice_x[HD];
  ofsy += stride_y[HD - 1] * reps[HD - 1];
  for (int j = HD - 1; j &gt; 0; --j) {
    int c = counter[j];
    if (c + 1 == block_num[j]) {
      ofsy += stride_y[j - 1] * (reps[j - 1] - 1);
    }
    counter[j] = (c + 1 == block_num[j] ? 0 : c + 1);
  }
}</code></pre>
</div>
<p>The evaluation of <code>repeat</code> is similar to that of reduction operations. I use a four-dimensional ndarray of float numbers as input. All four dimensions are of the same length. I measure the speed for increasing length, the repetition times is set to 2 on all dimensions.</p>
<p>The evaluation results compared with NumPy and Julia are shown in figures below. I also measure the peak memory usage. As can be seen, my repeat operation achieves about half of that in NumPy with regard to both execution speed and memory usage. The outer repeat operation in NumPy is implemented using the single axis version, and thus is less efficient. The repeat operation in Julia is much slower. One reason is that <code>repeat</code> is not a computation-intensive operation, so the optimisation techniques such as static compilation and vectorisation are of less importance than algorithm design.</p>
<p><img alt="Repeat operation speed on Laptop" style="width:50.0%" id="fig:core-opt:opeval_tp_repeat_00" src="images/core-opt/opeval_tp_repeat_01.png"> <img alt="Repeat operation speed on rPi" style="width:50.0%" id="fig:core-opt:opeval_rpi_repeat_00" src="images/core-opt/opeval_rpi_repeat_00.png"> <img alt="Repeat operation memory usage comparison" style="width:90.0%" id="fig:core-opt:opeval_tp_repeat_mem_00" src="images/core-opt/opeval_tp_repeat_mem_00.png"></p>
</section>
<section class="level3" id="convolution-operations">
<h3>Convolution Operations</h3>
<p>The convolution operations take up the majority of computation involved in deep neural network. A convolution operation takes two ndarrays as input: image (<span class="math inline">\(I\)</span>) and kernel (<span class="math inline">\(F\)</span>). In a 2-dimensional convolution, both ndarrays are of four dimensions. The image ndarray has <span class="math inline">\(B\)</span> batches, each image has size <span class="math inline">\(H\times W\)</span>, and has <span class="math inline">\(IC\)</span> channels. The kernel ndarray has <span class="math inline">\(R\)</span> rows, <span class="math inline">\(C\)</span> columns, the same input channel <span class="math inline">\(IC\)</span>, and output channel <span class="math inline">\(K\)</span>. The convolution can then be expressed as:</p>
<p><span id="eq:core-opt:conv"><span class="math display">\[CONV_{b,h,w,k} = \sum_{ic=1}^{IC}\sum_{r=1}^{R}\sum_{c=1}^{C}I_{b,h+r,w+c,ic}F_{r,c,ic,k}.\qquad(1)\]</span></span></p>
<p>The convolution operation is first implemented in Owl by interfacing to Eigen library, which is also used in TensorFlow for CPU convolution implementation. However, interfacing to this C++ library proves to be problematic and leads to a lot of installation issues. Therefore I decide to use C to implement convolutions, which consists of three types: <code>Conv</code>, <code>ConvBackwardKernel</code>, <code>ConvBackwardInput</code>.</p>
<p>The <code>Conv</code> operation calculates the output given input image and kernel. Similarly, <code>ConvBackwardKernel</code> calculates the kernel given the input and output ndarrays, and <code>ConvBackwardInput</code> gets input ndarray from kernel and output. The last two are mainly used in the backpropagation phase in training a DNN, but all three operations share a similar calculation algorithm.</p>
<p>A naive convolution algorithm is to implement eq.&nbsp;1 with nested for-loops. It is easy to see that this approach does not benefit from any parallelisation, and thus not suitable for production code.</p>
<p>The next version of implementation uses the <code>im2col</code> algorithm. A <code>im2col</code>-based convolution transforms the input ndarray into a matrix with redundancy. Convolution then can be performed as one matrix multiplication, which can benefit from highly optimised linear algebra packages such as OpenBLAS.</p>
<p>However, this algorithm requires generating a large temporary intermediate matrix. Depending on input image size, this matrix can take Gigabytes of memory in applications such as FST. Algorithms such as Memory-efficient Convolution&nbsp;aims to reduce the size of this intermediate matrix, but still fail with large input or kernel sizes.</p>
<p>To reduce the memory usage, I apply the method proposed in&nbsp;<span data-cites="goto2008anatomy" class="citation">(Goto and Geijn 2008)</span>, which is to cut matrices into small blocks so as to fit into the L1/L2 cache of CPU to do high-performance computation while reducing the memory usage, regardless of input size. Multiplication of two matrices can be divided into multiplication of small blocks. I implement the method proposed in&nbsp;<span data-cites="goto2008anatomy" class="citation">(Goto and Geijn 2008)</span> to calculated suitable block size based on the cache size of CPU.</p>
<p>To further improve the performance, I use the SIMD intrinsics in filling the temporary matrix from input ndarray. For one thing, depending on the input channel is divisible by the supported data length of SIMD (e.g.&nbsp;8 float numbers for AVX), I provide two set of implementations for filling the temporary blocks. During loading data from input ndarrays to these matrix blocks, I also use AVX intrinsics such as <code>_mm256_load_ps</code> to improve performance. Finally, the matrix multiplication between two small matrix blocks is implemented by the routines in OpenBLAS.</p>
<p>This is the current method I use in implementing various convolutions in Owl, on all 1-3 dimensions, and for different types such as dilated and transpose convolution operations. Above this C implementation level, mutable convolution operations are also provided, so as to further improve performance by utilising existing memory space.</p>
<figure>
<img alt="" style="width:90.0%" id="fig:core-opt:op_eval_eigen_conv_tp" src="images/core-opt/eigen_tp_conv2d.png"><figcaption>Figure 4: Measure the performance of Conv2D operation on Owl and Eigen on laptop</figcaption>
</figure>
<figure>
<img alt="" style="width:90.0%" id="fig:core-opt:op_eval_eigen_conv_rpi" src="images/core-opt/eigen_rpi_conv2d_bk.png"><figcaption>Figure 5: Measure the performance of Conv2D Backward kernel operation on Owl and Eigen on rPi</figcaption>
</figure>
<figure>
<img alt="" style="width:90.0%" id="fig:core-opt:op_eval_eigen_conv_mem" src="images/core-opt/eigen_rpi_conv2d_bi.png"><figcaption>Figure 6: Measure the memory usage of Conv2D Backward Input operation on Owl and Eigen</figcaption>
</figure>
<p>To measure the performance of my convolution implementation, I compare the three convolution operations on both the labtop and rPi as described before. I use two settings: fixed input size with varying kernel size; and fixed kernel size with varying input size. The Owl code is interfaced to existing implementation and Eigen library. The results in fig.&nbsp;4 and fig.&nbsp;5show that, our <code>Conv2D</code> implementation is as efficient as that in Eigen, and the <code>Conv2DBackwardKernel</code> operation is faster on the rPi. In fig.&nbsp;6 it is shown that our proposed implementation of <code>Conv2DBackwardInput</code> operation uses less memory than Eigen.</p>
</section>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-goto2008anatomy">
<p>Goto, Kazushige, and Robert A Geijn. 2008. “Anatomy of High-Performance Matrix Multiplication.” <em>ACM Transactions on Mathematical Software (TOMS)</em> 34 (3): 12.</p>
</div>
</div>
</section>
</section>
</article></div><a href="aeos.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 21</small>Automatic Empirical Tuning</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>